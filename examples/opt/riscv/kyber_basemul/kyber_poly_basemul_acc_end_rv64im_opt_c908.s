.macro load_coeffs poly, len, wordLen
  lh s0,  \len*\wordLen*0(\poly)
  lh s1,  \len*\wordLen*1(\poly)
  lh s2,  \len*\wordLen*2(\poly)
  lh s3,  \len*\wordLen*3(\poly)
  lh s4,  \len*\wordLen*4(\poly)
  lh s5,  \len*\wordLen*5(\poly)
  lh s6,  \len*\wordLen*6(\poly)
  lh s7,  \len*\wordLen*7(\poly)
  lh s8,  \len*\wordLen*8(\poly)
  lh s9,  \len*\wordLen*9(\poly)
  lh s10, \len*\wordLen*10(\poly)
  lh s11, \len*\wordLen*11(\poly)
  lh a2,  \len*\wordLen*12(\poly)
  lh a3,  \len*\wordLen*13(\poly)
  lh a4,  \len*\wordLen*14(\poly)
  lh a5,  \len*\wordLen*15(\poly)
.endm

.macro store_coeffs poly, len, wordLen
  sh s0,  \len*\wordLen*0(\poly)
  sh s1,  \len*\wordLen*1(\poly)
  sh s2,  \len*\wordLen*2(\poly)
  sh s3,  \len*\wordLen*3(\poly)
  sh s4,  \len*\wordLen*4(\poly)
  sh s5,  \len*\wordLen*5(\poly)
  sh s6,  \len*\wordLen*6(\poly)
  sh s7,  \len*\wordLen*7(\poly)
  sh s8,  \len*\wordLen*8(\poly)
  sh s9,  \len*\wordLen*9(\poly)
  sh s10, \len*\wordLen*10(\poly)
  sh s11, \len*\wordLen*11(\poly)
  sh a2,  \len*\wordLen*12(\poly)
  sh a3,  \len*\wordLen*13(\poly)
  sh a4,  \len*\wordLen*14(\poly)
  sh a5,  \len*\wordLen*15(\poly)
.endm

.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

// a <- a*b*(-2^{-64}) mod+- q
// q32: q<<32; bqinv: b*qinv
.macro plant_mul_const_inplace q32, bqinv, a
  mul  \a, \a, \bqinv
  srai \a, \a, 32
  addi \a, \a, 8
  mulh \a, \a, \q32
.endm

// r <- a*b*(-2^{-64}) mod+- q
// q32: q<<32; bqinv: b*qinv
.macro plant_mul_const q32, bqinv, a, r
    mul  \r, \a, \bqinv
    srai \r, \r, 32
    addi \r, \r, 8
    mulh \r, \r, \q32
.endm

// each layer increases coefficients by 0.5q; In ct_butterfly, twiddle and tmp can be reused because each twiddle is only used once. The gs_butterfly cannot.
.macro ct_butterfly coeff0, coeff1, twiddle, q, tmp
  plant_mul_const \q, \twiddle, \coeff1, \tmp
  sub \coeff1, \coeff0, \tmp
  add \coeff0, \coeff0, \tmp
.endm

.macro gs_butterfly coeff0, coeff1, twiddle, q, tmp
  sub \tmp, \coeff0, \coeff1
  add \coeff0, \coeff0, \coeff1
  plant_mul_const \q, \twiddle, \tmp, \coeff1
.endm

// in-place plantard reduction to a
// output \in (-0.5q, 0.5q); q32: q<<32
.macro plant_red q32, qinv, a
  mul  \a, \a, \qinv
  srai \a, \a, 32
  addi \a, \a, 8
  mulh \a, \a, \q32
.endm

.equ q,    3329
.equ q32,  0xd0100000000                // q << 32
.equ qinv, 0x3c0f12886ba8f301           // q^-1 mod 2^64
.equ plantconst, 0x13afb7680bb055       // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0x1a390f4d9791e139    // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

// void poly_basemul_acc_end_rv64im(int16_t *r, const int16_t *a, const int16_t *b, uint64_t *zetas, int32_t *r_double)
// compute basemul, accumulate the 32-bit results into r_double, and reduce r_double to r
// a0: r, a1: a, a2: b, a3: zetas, a4: r_double
// a5: q<<32, a6: loop control
// t0-t3: a[2i,2i+1],b[2i,2i+1]
// t4: zeta, t5-t6: temp
.global poly_basemul_acc_end_rv64im_opt_c908
.align 2
poly_basemul_acc_end_rv64im_opt_c908:
    addi sp, sp, -8*2
    sd   s0, 0*8(sp)
    sd   s1, 1*8(sp)
    li s0, q32
    li s1, qinv
    li a6, 64
                                // Instructions:    1
                                // Expected cycles: 1
                                // Expected IPC:    1.00
                                //
                                // Cycle bound:     1.0
                                // IPC bound:       1.00
                                //
                                // Wall time:     0.02s
                                // User time:     0.02s
                                //
                                // ----- cycle (expected) ------>
                                // 0                        25
                                // |------------------------|----
        lh x25, 2*1(x12)        // *.............................

                                 // ------ cycle (expected) ------>
                                 // 0                        25
                                 // |------------------------|-----
        // lh x25, 2*1(x12)      // *..............................

        addi a6, a6, -1
poly_basemul_acc_end_rv64im_loop:
                                  // Instructions:    63
                                  // Expected cycles: 32
                                  // Expected IPC:    1.97
                                  //
                                  // Cycle bound:     33.0
                                  // IPC bound:       1.91
                                  //
                                  // Wall time:     21.86s
                                  // User time:     21.86s
                                  //
                                  // ------ cycle (expected) ------->
                                  // 0                        25
                                  // |------------------------|------
        ld x20, 8*0(x13)          // *...............................
        addi x13, x13, 8*1        // *...............................
        lh x19, 2*3(x12)          // .*..............................
        lh x21, 2*2(x11)          // ..*.............................
        mul x7, x25, x20          // ..*.............................
        lh x22, 2*2(x12)          // ...*............................
        neg x1, x20               // ...*............................
        mul x4, x19, x1           // ....*...........................
        lh x26, 2*3(x11)          // ....*...........................
        lh x23, 2*1(x11)          // .....*..........................
        mul x6, x21, x19          // .....*..........................
        lh x15, 2*0(x12)          // ......*.........................
        srai x7, x7, 32           // ......*.........................
        addi x5, x7, 8            // .......*........................
        mul x17, x26, x22         // .......*........................
        srai x20, x4, 32          // ........*.......................
        mulh x7, x5, x8           // ........*.......................
        lh x5, 2*0(x11)           // .........*......................
        addi x18, x20, 8          // .........*......................
        mul x24, x21, x22         // ..........*.....................
        mulh x28, x18, x8         // ..........*.....................
        add x6, x6, x17           // ...........*....................
        lw x29, 4*3(x14)          // ...........*....................
        mul x16, x5, x15          // ............*...................
        mul x7, x23, x7           // ............*...................
        add x20, x6, x29          // .............*..................
        lw x21, 4*0(x14)          // .............*..................
        mul x15, x23, x15         // ..............*.................
        mul x27, x26, x28         // ..............*.................
        lw x23, 4*2(x14)          // ...............*................
        mul x29, x5, x25          // ...............*................
        lw x31, 4*1(x14)          // ................*...............
        add x1, x7, x16           // ................*...............
        add x18, x1, x21          // .................*..............
        mul x25, x20, x9          // .................*..............
        mul x1, x18, x9           // ..................*.............
        add x16, x27, x24         // ..................*.............
        add x21, x29, x15         // ...................*............
        add x20, x16, x23         // ...................*............
        add x30, x21, x31         // ....................*...........
        mul x29, x20, x9          // ....................*...........
        srai x25, x25, 32         // .....................*..........
        mul x4, x30, x9           // .....................*..........
        srai x21, x1, 32          // ......................*.........
        addi x25, x25, 8          // ......................*.........
        mulh x23, x25, x8         // .......................*........
        addi x6, x21, 8           // .......................*........
        mulh x16, x6, x8          // ........................*.......
        srai x5, x29, 32          // ........................*.......
        addi x5, x5, 8            // .........................*......
        srai x20, x4, 32          // .........................*......
        addi x28, x20, 8          // ..........................*.....
        mulh x17, x5, x8          // ..........................*.....
        mulh x18, x28, x8         // ...........................*....
        sh x23, 2*3(x10)          // ...........................*....
        addi x12, x12, 2*4        // ............................*...
        sh x16, 2*0(x10)          // ............................*...
        addi x14, x14, 4*4        // .............................*..
        lh x25, 2*1(x12)          // .............................e..
        addi x11, x11, 2*4        // ..............................*.
        sh x17, 2*2(x10)          // ..............................*.
        sh x18, 2*1(x10)          // ...............................*
        addi x10, x10, 2*4        // ...............................*

                                    // -------- cycle (expected) -------->
                                    // 0                        25
                                    // |------------------------|---------
        // lh x7, 2*0(x12)          // ...'.....*.........................
        // lh x28, 2*1(x12)         // e..'............................~..
        // ld x29, 8*0(x13)         // ...*...............................
        // lh x5, 2*0(x11)          // ...'........*......................
        // lh x6, 2*1(x11)          // ...'....*..........................
        // mul  x30, x28, x29       // ...'.*.............................
        // srai x30, x30, 32        // ...'.....*.........................
        // addi x30, x30, 8         // ...'......*........................
        // mulh x30, x30, x8        // ...'.......*.......................
        // lw  x17, 4*0(x14)        // ...'............*..................
        // mul x30, x6, x30         // ...'...........*...................
        // mul x31, x5, x7          // ...'...........*...................
        // add x30, x30, x31        // ...'...............*...............
        // add x30, x30, x17        // ...'................*..............
        // mul  x30, x30, x9        // ...'.................*.............
        // srai x30, x30, 32        // ...'.....................*.........
        // addi x30, x30, 8         // ...'......................*........
        // mulh x30, x30, x8        // ...'.......................*.......
        // sh  x30, 2*0(x10)        // ...'...........................*...
        // lw  x17, 4*1(x14)        // ...'...............*...............
        // mul x30, x5, x28         // ...'..............*................
        // mul x31, x6, x7          // ...'.............*.................
        // add x30, x30, x31        // ...'..................*............
        // add x30, x30, x17        // ...'...................*...........
        // mul  x30, x30, x9        // ...'....................*..........
        // srai x30, x30, 32        // ...'........................*......
        // addi x30, x30, 8         // ...'.........................*.....
        // mulh x30, x30, x8        // ...'..........................*....
        // sh  x30, 2*1(x10)        // ..~'..............................*
        // neg x29, x29             // ...'..*............................
        // lh x7, 2*2(x12)          // ...'..*............................
        // lh x28, 2*3(x12)         // ...'*..............................
        // lh x5, 2*2(x11)          // ...'.*.............................
        // lh x6, 2*3(x11)          // ...'...*...........................
        // mul  x30, x28, x29       // ...'...*...........................
        // srai x30, x30, 32        // ...'.......*.......................
        // addi x30, x30, 8         // ...'........*......................
        // mulh x30, x30, x8        // ...'.........*.....................
        // lw  x17, 4*2(x14)        // ...'..............*................
        // mul x30, x6, x30         // ...'.............*.................
        // mul x31, x5, x7          // ...'.........*.....................
        // add x30, x30, x31        // ...'.................*.............
        // add x30, x30, x17        // ...'..................*............
        // mul  x30, x30, x9        // ...'...................*...........
        // srai x30, x30, 32        // ...'.......................*.......
        // addi x30, x30, 8         // ...'........................*......
        // mulh x30, x30, x8        // ...'.........................*.....
        // sh  x30, 2*2(x10)        // .~.'.............................*.
        // lw  x17, 4*3(x14)        // ...'..........*....................
        // mul x30, x5, x28         // ...'....*..........................
        // mul x31, x6, x7          // ...'......*........................
        // add x30, x30, x31        // ...'..........*....................
        // add x30, x30, x17        // ...'............*..................
        // mul  x30, x30, x9        // ...'................*..............
        // srai x30, x30, 32        // ...'....................*..........
        // addi x30, x30, 8         // ...'.....................*.........
        // mulh x30, x30, x8        // ...'......................*........
        // sh  x30, 2*3(x10)        // ...'..........................*....
        // addi x10, x10, 2*4       // ..~'..............................*
        // addi x11, x11, 2*4       // .~.'.............................*.
        // addi x12, x12, 2*4       // ...'...........................*...
        // addi x13, x13, 8*1       // ...*...............................
        // addi x14, x14, 4*4       // ~..'............................*..

        addi a6, a6, -1
        bne a6, zero, poly_basemul_acc_end_rv64im_loop
                                  // Instructions:    62
                                  // Expected cycles: 32
                                  // Expected IPC:    1.94
                                  //
                                  // Cycle bound:     32.0
                                  // IPC bound:       1.94
                                  //
                                  // Wall time:     2.23s
                                  // User time:     2.23s
                                  //
                                  // ------ cycle (expected) ------->
                                  // 0                        25
                                  // |------------------------|------
        ld x20, 8*0(x13)          // *...............................
        addi x13, x13, 8*1        // *...............................
        lh x31, 2*3(x12)          // .*..............................
        lh x21, 2*0(x11)          // ..*.............................
        mul x7, x25, x20          // ..*.............................
        lw x28, 4*2(x14)          // ...*............................
        neg x5, x20               // ...*............................
        lh x15, 2*2(x12)          // ....*...........................
        mul x20, x31, x5          // ....*...........................
        mul x6, x21, x25          // .....*..........................
        lh x1, 2*0(x12)           // .....*..........................
        lw x24, 4*3(x14)          // ......*.........................
        srai x19, x7, 32          // ......*.........................
        lh x26, 2*1(x11)          // .......*........................
        addi x5, x19, 8           // .......*........................
        srai x25, x20, 32         // ........*.......................
        mulh x20, x5, x8          // ........*.......................
        lh x7, 2*2(x11)           // .........*......................
        addi x5, x25, 8           // .........*......................
        mul x22, x26, x1          // ..........*.....................
        mulh x25, x5, x8          // ..........*.....................
        mul x23, x21, x1          // ...........*....................
        lh x16, 2*3(x11)          // ...........*....................
        mul x1, x7, x15           // ............*...................
        mul x21, x26, x20         // ............*...................
        lw x26, 4*1(x14)          // .............*..................
        mul x18, x7, x31          // .............*..................
        mul x5, x16, x25          // ..............*.................
        add x20, x6, x22          // ..............*.................
        lw x7, 4*0(x14)           // ...............*................
        mul x17, x16, x15         // ...............*................
        add x30, x21, x23         // ................*...............
        add x25, x20, x26         // ................*...............
        mul x23, x25, x9          // .................*..............
        add x25, x30, x7          // .................*..............
        add x6, x5, x1            // ..................*.............
        mul x25, x25, x9          // ..................*.............
        add x21, x18, x17         // ...................*............
        add x20, x6, x28          // ...................*............
        mul x5, x20, x9           // ....................*...........
        add x4, x21, x24          // ....................*...........
        mul x27, x4, x9           // .....................*..........
        srai x20, x23, 32         // .....................*..........
        srai x25, x25, 32         // ......................*.........
        addi x20, x20, 8          // ......................*.........
        mulh x7, x20, x8          // .......................*........
        addi x25, x25, 8          // .......................*........
        mulh x25, x25, x8         // ........................*.......
        srai x20, x5, 32          // ........................*.......
        addi x20, x20, 8          // .........................*......
        srai x23, x27, 32         // .........................*......
        addi x23, x23, 8          // ..........................*.....
        mulh x20, x20, x8         // ..........................*.....
        addi x11, x11, 2*4        // ...........................*....
        mulh x5, x23, x8          // ...........................*....
        sh x7, 2*1(x10)           // ............................*...
        sh x25, 2*0(x10)          // .............................*..
        addi x14, x14, 4*4        // .............................*..
        sh x20, 2*2(x10)          // ..............................*.
        addi x12, x12, 2*4        // ..............................*.
        sh x5, 2*3(x10)           // ...............................*
        addi x10, x10, 2*4        // ...............................*

                                   // ------ cycle (expected) ------->
                                   // 0                        25
                                   // |------------------------|------
        // ld x20, 8*0(x13)        // *...............................
        // addi x13, x13, 8*1      // *...............................
        // lh x19, 2*3(x12)        // .*..............................
        // lh x21, 2*2(x11)        // .........*......................
        // mul x7, x25, x20        // ..*.............................
        // lh x22, 2*2(x12)        // ....*...........................
        // neg x1, x20             // ...*............................
        // mul x4, x19, x1         // ....*...........................
        // lh x26, 2*3(x11)        // ...........*....................
        // lh x23, 2*1(x11)        // .......*........................
        // mul x6, x21, x19        // .............*..................
        // lh x15, 2*0(x12)        // .....*..........................
        // srai x7, x7, 32         // ......*.........................
        // addi x5, x7, 8          // .......*........................
        // mul x17, x26, x22       // ...............*................
        // srai x20, x4, 32        // ........*.......................
        // mulh x7, x5, x8         // ........*.......................
        // lh x5, 2*0(x11)         // ..*.............................
        // addi x18, x20, 8        // .........*......................
        // mul x24, x21, x22       // ............*...................
        // mulh x28, x18, x8       // ..........*.....................
        // add x6, x6, x17         // ...................*............
        // lw x29, 4*3(x14)        // ......*.........................
        // mul x16, x5, x15        // ...........*....................
        // mul x7, x23, x7         // ............*...................
        // add x20, x6, x29        // ....................*...........
        // lw x21, 4*0(x14)        // ...............*................
        // mul x15, x23, x15       // ..........*.....................
        // mul x27, x26, x28       // ..............*.................
        // lw x23, 4*2(x14)        // ...*............................
        // mul x29, x5, x25        // .....*..........................
        // lw x31, 4*1(x14)        // .............*..................
        // add x1, x7, x16         // ................*...............
        // add x18, x1, x21        // .................*..............
        // mul x25, x20, x9        // .....................*..........
        // mul x1, x18, x9         // ..................*.............
        // add x16, x27, x24       // ..................*.............
        // add x21, x29, x15       // ..............*.................
        // add x20, x16, x23       // ...................*............
        // add x30, x21, x31       // ................*...............
        // mul x29, x20, x9        // ....................*...........
        // srai x25, x25, 32       // .........................*......
        // mul x4, x30, x9         // .................*..............
        // srai x21, x1, 32        // ......................*.........
        // addi x25, x25, 8        // ..........................*.....
        // mulh x23, x25, x8       // ...........................*....
        // addi x6, x21, 8         // .......................*........
        // mulh x16, x6, x8        // ........................*.......
        // srai x5, x29, 32        // ........................*.......
        // addi x5, x5, 8          // .........................*......
        // srai x20, x4, 32        // .....................*..........
        // addi x28, x20, 8        // ......................*.........
        // mulh x17, x5, x8        // ..........................*.....
        // mulh x18, x28, x8       // .......................*........
        // sh x23, 2*3(x10)        // ...............................*
        // addi x12, x12, 2*4      // ..............................*.
        // sh x16, 2*0(x10)        // .............................*..
        // addi x14, x14, 4*4      // .............................*..
        // addi x11, x11, 2*4      // ...........................*....
        // sh x17, 2*2(x10)        // ..............................*.
        // sh x18, 2*1(x10)        // ............................*...
        // addi x10, x10, 2*4      // ...............................*

    ld   s0, 0*8(sp)
    ld   s1, 1*8(sp)
    addi sp, sp, 8*2
ret