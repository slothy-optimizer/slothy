.macro load_coeffs poly, len, wordLen
  lh s0,  \len*\wordLen*0(\poly)
  lh s1,  \len*\wordLen*1(\poly)
  lh s2,  \len*\wordLen*2(\poly)
  lh s3,  \len*\wordLen*3(\poly)
  lh s4,  \len*\wordLen*4(\poly)
  lh s5,  \len*\wordLen*5(\poly)
  lh s6,  \len*\wordLen*6(\poly)
  lh s7,  \len*\wordLen*7(\poly)
  lh s8,  \len*\wordLen*8(\poly)
  lh s9,  \len*\wordLen*9(\poly)
  lh s10, \len*\wordLen*10(\poly)
  lh s11, \len*\wordLen*11(\poly)
  lh a2,  \len*\wordLen*12(\poly)
  lh a3,  \len*\wordLen*13(\poly)
  lh a4,  \len*\wordLen*14(\poly)
  lh a5,  \len*\wordLen*15(\poly)
.endm

.macro store_coeffs poly, len, wordLen
  sh s0,  \len*\wordLen*0(\poly)
  sh s1,  \len*\wordLen*1(\poly)
  sh s2,  \len*\wordLen*2(\poly)
  sh s3,  \len*\wordLen*3(\poly)
  sh s4,  \len*\wordLen*4(\poly)
  sh s5,  \len*\wordLen*5(\poly)
  sh s6,  \len*\wordLen*6(\poly)
  sh s7,  \len*\wordLen*7(\poly)
  sh s8,  \len*\wordLen*8(\poly)
  sh s9,  \len*\wordLen*9(\poly)
  sh s10, \len*\wordLen*10(\poly)
  sh s11, \len*\wordLen*11(\poly)
  sh a2,  \len*\wordLen*12(\poly)
  sh a3,  \len*\wordLen*13(\poly)
  sh a4,  \len*\wordLen*14(\poly)
  sh a5,  \len*\wordLen*15(\poly)
.endm

.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

// a <- a*b*(-2^{-64}) mod+- q
// q32: q<<32; bqinv: b*qinv
.macro plant_mul_const_inplace q32, bqinv, a
  mul  \a, \a, \bqinv
  srai \a, \a, 32
  addi \a, \a, 8
  mulh \a, \a, \q32
.endm

// r <- a*b*(-2^{-64}) mod+- q
// q32: q<<32; bqinv: b*qinv
.macro plant_mul_const q32, bqinv, a, r
    mul  \r, \a, \bqinv
    srai \r, \r, 32
    addi \r, \r, 8
    mulh \r, \r, \q32
.endm

// each layer increases coefficients by 0.5q; In ct_butterfly, twiddle and tmp can be reused because each twiddle is only used once. The gs_butterfly cannot.
.macro ct_butterfly coeff0, coeff1, twiddle, q, tmp
  plant_mul_const \q, \twiddle, \coeff1, \tmp
  sub \coeff1, \coeff0, \tmp
  add \coeff0, \coeff0, \tmp
.endm

.macro gs_butterfly coeff0, coeff1, twiddle, q, tmp
  sub \tmp, \coeff0, \coeff1
  add \coeff0, \coeff0, \coeff1
  plant_mul_const \q, \twiddle, \tmp, \coeff1
.endm

// in-place plantard reduction to a
// output \in (-0.5q, 0.5q); q32: q<<32
.macro plant_red q32, qinv, a
  mul  \a, \a, \qinv
  srai \a, \a, 32
  addi \a, \a, 8
  mulh \a, \a, \q32
.endm

.equ q,    3329
.equ q32,  0xd0100000000                // q << 32
.equ qinv, 0x3c0f12886ba8f301           // q^-1 mod 2^64
.equ plantconst, 0x13afb7680bb055       // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0x1a390f4d9791e139    // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

// void poly_basemul_acc_cache_init_rv64im(int32_t *r, const int16_t *a, const int16_t *b, int16_t *b_cache, uint64_t *zetas)
// compute basemul, cache bzeta into b_cache, and accumulate the 32-bit results into r
// a0: r, a1: a, a2: b, a3: b_cache, a4: zetas
// a5: q<<32, a6: loop control, a7: accumulated value
// t0-t3: a[2i,2i+1],b[2i,2i+1]
// t4: zeta, t5-t6: temp
.global poly_basemul_acc_cache_init_rv64im_opt_c908
.align 2
poly_basemul_acc_cache_init_rv64im_opt_c908:
    addi sp, sp, -8*15
    save_regs
    li a5, q32
    li a6, 64
                               // Instructions:    1
                               // Expected cycles: 1
                               // Expected IPC:    1.00
                               //
                               // Cycle bound:     1.0
                               // IPC bound:       1.00
                               //
                               // Wall time:     0.01s
                               // User time:     0.01s
                               //
                               // ----- cycle (expected) ------>
                               // 0                        25
                               // |------------------------|----
        lh x1, 2*1(x12)        // *.............................

                                // ------ cycle (expected) ------>
                                // 0                        25
                                // |------------------------|-----
        // lh x1, 2*1(x12)      // *..............................

        addi a6, a6, -1
poly_basemul_acc_cache_init_rv64im_loop:
                                  // Instructions:    49
                                  // Expected cycles: 25
                                  // Expected IPC:    1.96
                                  //
                                  // Cycle bound:     25.0
                                  // IPC bound:       1.96
                                  //
                                  // Wall time:     116.99s
                                  // User time:     116.99s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        ld x4, 8*0(x14)           // *.............................
        addi x14, x14, 8*1        // *.............................
        lh x5, 2*0(x12)           // .*............................
        mul x30, x1, x4           // ..*...........................
        lh x7, 2*3(x12)           // ..*...........................
        lh x21, 2*2(x12)          // ...*..........................
        addi x12, x12, 2*4        // ...*..........................
        lh x18, 2*2(x11)          // ....*.........................
        neg x23, x4               // ....*.........................
        lh x9, 2*3(x11)           // .....*........................
        mul x20, x7, x23          // .....*........................
        lh x4, 2*1(x11)           // ......*.......................
        srai x22, x30, 32         // ......*.......................
        mul x8, x18, x7           // .......*......................
        addi x22, x22, 8          // .......*......................
        mul x31, x9, x21          // ........*.....................
        mulh x27, x22, x15        // ........*.....................
        srai x26, x20, 32         // .........*....................
        lh x20, 2*0(x11)          // .........*....................
        addi x25, x26, 8          // ..........*...................
        lw x23, 4*1(x10)          // ..........*...................
        mulh x26, x25, x15        // ...........*..................
        lw x19, 4*0(x10)          // ...........*..................
        mul x30, x4, x27          // ............*.................
        mul x28, x20, x5          // ............*.................
        mul x29, x20, x1          // .............*................
        lh x1, 2*1(x12)           // .............e................
        mul x3, x18, x21          // ..............*...............
        mul x6, x4, x5            // ..............*...............
        add x31, x8, x31          // ...............*..............
        sh x26, 2*1(x13)          // ...............*..............
        add x5, x30, x28          // ................*.............
        mul x9, x9, x26           // ................*.............
        add x5, x5, x19           // .................*............
        lw x22, 4*3(x10)          // .................*............
        sw x5, 4*0(x10)           // ..................*...........
        add x19, x29, x6          // ..................*...........
        add x24, x19, x23         // ...................*..........
        lw x17, 4*2(x10)          // ...................*..........
        sw x24, 4*1(x10)          // ....................*.........
        add x6, x31, x22          // ....................*.........
        sw x6, 4*3(x10)           // .....................*........
        add x21, x9, x3           // .....................*........
        sh x27, 2*0(x13)          // ......................*.......
        addi x13, x13, 2*2        // ......................*.......
        addi x11, x11, 2*4        // .......................*......
        add x31, x21, x17         // .......................*......
        sw x31, 4*2(x10)          // ........................*.....
        addi x10, x10, 4*4        // ........................*.....

                                    // --------- cycle (expected) --------->
                                    // 0                        25
                                    // |------------------------|-----------
        // lh x7, 2*0(x12)          // ............'*.......................
        // lh x28, 2*1(x12)         // e...........'............~...........
        // ld x29, 8*0(x14)         // ............*........................
        // lh x5, 2*0(x11)          // ............'........*...............
        // lh x6, 2*1(x11)          // ............'.....*..................
        // mul  x30, x28, x29       // ............'.*......................
        // srai x30, x30, 32        // ............'.....*..................
        // addi x30, x30, 8         // ............'......*.................
        // mulh x30, x30, x15       // ............'.......*................
        // sh  x30, 2*0(x13)        // .........~..'.....................*..
        // lw  x17, 4*0(x10)        // ............'..........*.............
        // mul x30, x6, x30         // ............'...........*............
        // mul x31, x5, x7          // ............'...........*............
        // add x30, x30, x31        // ...~........'...............*........
        // add x30, x30, x17        // ....~.......'................*.......
        // sw  x30, 4*0(x10)        // .....~......'.................*......
        // lw  x17, 4*1(x10)        // ............'.........*..............
        // mul x30, x5, x28         // ~...........'............*...........
        // mul x31, x6, x7          // .~..........'.............*..........
        // add x30, x30, x31        // .....~......'.................*......
        // add x30, x30, x17        // ......~.....'..................*.....
        // sw  x30, 4*1(x10)        // .......~....'...................*....
        // neg x29, x29             // ............'...*....................
        // lh x7, 2*2(x12)          // ............'..*.....................
        // lh x28, 2*3(x12)         // ............'.*......................
        // lh x5, 2*2(x11)          // ............'...*....................
        // lh x6, 2*3(x11)          // ............'....*...................
        // mul  x30, x28, x29       // ............'....*...................
        // srai x30, x30, 32        // ............'........*...............
        // addi x30, x30, 8         // ............'.........*..............
        // mulh x30, x30, x15       // ............'..........*.............
        // sh  x30, 2*1(x13)        // ..~.........'..............*.........
        // lw  x17, 4*2(x10)        // ......~.....'..................*.....
        // mul x30, x6, x30         // ...~........'...............*........
        // mul x31, x5, x7          // .~..........'.............*..........
        // add x30, x30, x31        // ........~...'....................*...
        // add x30, x30, x17        // ..........~.'......................*.
        // sw  x30, 4*2(x10)        // ...........~'.......................*
        // lw  x17, 4*3(x10)        // ....~.......'................*.......
        // mul x30, x5, x28         // ............'......*.................
        // mul x31, x6, x7          // ............'.......*................
        // add x30, x30, x31        // ..~.........'..............*.........
        // add x30, x30, x17        // .......~....'...................*....
        // sw  x30, 4*3(x10)        // ........~...'....................*...
        // addi x10, x10, 4*4       // ...........~'.......................*
        // addi x11, x11, 2*4       // ..........~.'......................*.
        // addi x12, x12, 2*4       // ............'..*.....................
        // addi x13, x13, 2*2       // .........~..'.....................*..
        // addi x14, x14, 8*1       // ............*........................

        addi a6, a6, -1
        bne a6, zero, poly_basemul_acc_cache_init_rv64im_loop
                                  // Instructions:    48
                                  // Expected cycles: 25
                                  // Expected IPC:    1.92
                                  //
                                  // Cycle bound:     25.0
                                  // IPC bound:       1.92
                                  //
                                  // Wall time:     1.36s
                                  // User time:     1.36s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        ld x5, 8*0(x14)           // *.............................
        addi x14, x14, 8*1        // *.............................
        lh x8, 2*3(x12)           // .*............................
        lh x19, 2*0(x12)          // ..*...........................
        neg x9, x5                // ..*...........................
        mul x31, x1, x5           // ...*..........................
        lh x18, 2*2(x12)          // ...*..........................
        mul x3, x8, x9            // ....*.........................
        lh x27, 2*0(x11)          // ....*.........................
        lh x4, 2*2(x11)           // .....*........................
        addi x12, x12, 2*4        // .....*........................
        lh x24, 2*1(x11)          // ......*.......................
        mul x22, x27, x19         // .......*......................
        srai x26, x31, 32         // .......*......................
        addi x28, x26, 8          // ........*.....................
        srai x17, x3, 32          // ........*.....................
        mulh x25, x28, x15        // .........*....................
        addi x17, x17, 8          // .........*....................
        lh x29, 2*3(x11)          // ..........*...................
        mulh x6, x17, x15         // ..........*...................
        mul x28, x24, x19         // ...........*..................
        mul x31, x27, x1          // ...........*..................
        mul x17, x4, x18          // ............*.................
        lw x21, 4*1(x10)          // ............*.................
        mul x19, x29, x18         // .............*................
        mul x27, x24, x25         // .............*................
        sh x6, 2*1(x13)           // ..............*...............
        mul x1, x29, x6           // ..............*...............
        add x20, x31, x28         // ...............*..............
        mul x29, x4, x8           // ...............*..............
        add x4, x20, x21          // ................*.............
        lw x23, 4*0(x10)          // ................*.............
        add x22, x27, x22         // .................*............
        lw x6, 4*2(x10)           // .................*............
        add x20, x22, x23         // ..................*...........
        add x1, x1, x17           // ..................*...........
        add x1, x1, x6            // ...................*..........
        lw x22, 4*3(x10)          // ...................*..........
        sw x1, 4*2(x10)           // ....................*.........
        add x17, x29, x19         // ....................*.........
        sw x20, 4*0(x10)          // .....................*........
        add x22, x17, x22         // .....................*........
        sw x22, 4*3(x10)          // ......................*.......
        addi x11, x11, 2*4        // ......................*.......
        sw x4, 4*1(x10)           // .......................*......
        addi x10, x10, 4*4        // .......................*......
        sh x25, 2*0(x13)          // ........................*.....
        addi x13, x13, 2*2        // ........................*.....

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // ld x4, 8*0(x14)         // *..............................
        // addi x14, x14, 8*1      // *..............................
        // lh x5, 2*0(x12)         // ..*............................
        // mul x30, x1, x4         // ...*...........................
        // lh x7, 2*3(x12)         // .*.............................
        // lh x21, 2*2(x12)        // ...*...........................
        // addi x12, x12, 2*4      // .....*.........................
        // lh x18, 2*2(x11)        // .....*.........................
        // neg x23, x4             // ..*............................
        // lh x9, 2*3(x11)         // ..........*....................
        // mul x20, x7, x23        // ....*..........................
        // lh x4, 2*1(x11)         // ......*........................
        // srai x22, x30, 32       // .......*.......................
        // mul x8, x18, x7         // ...............*...............
        // addi x22, x22, 8        // ........*......................
        // mul x31, x9, x21        // .............*.................
        // mulh x27, x22, x15      // .........*.....................
        // srai x26, x20, 32       // ........*......................
        // lh x20, 2*0(x11)        // ....*..........................
        // addi x25, x26, 8        // .........*.....................
        // lw x23, 4*1(x10)        // ............*..................
        // mulh x26, x25, x15      // ..........*....................
        // lw x19, 4*0(x10)        // ................*..............
        // mul x30, x4, x27        // .............*.................
        // mul x28, x20, x5        // .......*.......................
        // mul x29, x20, x1        // ...........*...................
        // mul x3, x18, x21        // ............*..................
        // mul x6, x4, x5          // ...........*...................
        // add x31, x8, x31        // ....................*..........
        // sh x26, 2*1(x13)        // ..............*................
        // add x5, x30, x28        // .................*.............
        // mul x9, x9, x26         // ..............*................
        // add x5, x5, x19         // ..................*............
        // lw x22, 4*3(x10)        // ...................*...........
        // sw x5, 4*0(x10)         // .....................*.........
        // add x19, x29, x6        // ...............*...............
        // add x24, x19, x23       // ................*..............
        // lw x17, 4*2(x10)        // .................*.............
        // sw x24, 4*1(x10)        // .......................*.......
        // add x6, x31, x22        // .....................*.........
        // sw x6, 4*3(x10)         // ......................*........
        // add x21, x9, x3         // ..................*............
        // sh x27, 2*0(x13)        // ........................*......
        // addi x13, x13, 2*2      // ........................*......
        // addi x11, x11, 2*4      // ......................*........
        // add x31, x21, x17       // ...................*...........
        // sw x31, 4*2(x10)        // ....................*..........
        // addi x10, x10, 4*4      // .......................*.......

    restore_regs
    addi sp, sp, 8*15
ret