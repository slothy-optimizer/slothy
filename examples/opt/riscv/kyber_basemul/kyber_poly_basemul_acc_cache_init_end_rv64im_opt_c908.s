.macro load_coeffs poly, len, wordLen
  lh s0,  \len*\wordLen*0(\poly)
  lh s1,  \len*\wordLen*1(\poly)
  lh s2,  \len*\wordLen*2(\poly)
  lh s3,  \len*\wordLen*3(\poly)
  lh s4,  \len*\wordLen*4(\poly)
  lh s5,  \len*\wordLen*5(\poly)
  lh s6,  \len*\wordLen*6(\poly)
  lh s7,  \len*\wordLen*7(\poly)
  lh s8,  \len*\wordLen*8(\poly)
  lh s9,  \len*\wordLen*9(\poly)
  lh s10, \len*\wordLen*10(\poly)
  lh s11, \len*\wordLen*11(\poly)
  lh a2,  \len*\wordLen*12(\poly)
  lh a3,  \len*\wordLen*13(\poly)
  lh a4,  \len*\wordLen*14(\poly)
  lh a5,  \len*\wordLen*15(\poly)
.endm

.macro store_coeffs poly, len, wordLen
  sh s0,  \len*\wordLen*0(\poly)
  sh s1,  \len*\wordLen*1(\poly)
  sh s2,  \len*\wordLen*2(\poly)
  sh s3,  \len*\wordLen*3(\poly)
  sh s4,  \len*\wordLen*4(\poly)
  sh s5,  \len*\wordLen*5(\poly)
  sh s6,  \len*\wordLen*6(\poly)
  sh s7,  \len*\wordLen*7(\poly)
  sh s8,  \len*\wordLen*8(\poly)
  sh s9,  \len*\wordLen*9(\poly)
  sh s10, \len*\wordLen*10(\poly)
  sh s11, \len*\wordLen*11(\poly)
  sh a2,  \len*\wordLen*12(\poly)
  sh a3,  \len*\wordLen*13(\poly)
  sh a4,  \len*\wordLen*14(\poly)
  sh a5,  \len*\wordLen*15(\poly)
.endm

.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

// a <- a*b*(-2^{-64}) mod+- q
// q32: q<<32; bqinv: b*qinv
.macro plant_mul_const_inplace q32, bqinv, a
  mul  \a, \a, \bqinv
  srai \a, \a, 32
  addi \a, \a, 8
  mulh \a, \a, \q32
.endm

// r <- a*b*(-2^{-64}) mod+- q
// q32: q<<32; bqinv: b*qinv
.macro plant_mul_const q32, bqinv, a, r
    mul  \r, \a, \bqinv
    srai \r, \r, 32
    addi \r, \r, 8
    mulh \r, \r, \q32
.endm

// each layer increases coefficients by 0.5q; In ct_butterfly, twiddle and tmp can be reused because each twiddle is only used once. The gs_butterfly cannot.
.macro ct_butterfly coeff0, coeff1, twiddle, q, tmp
  plant_mul_const \q, \twiddle, \coeff1, \tmp
  sub \coeff1, \coeff0, \tmp
  add \coeff0, \coeff0, \tmp
.endm

.macro gs_butterfly coeff0, coeff1, twiddle, q, tmp
  sub \tmp, \coeff0, \coeff1
  add \coeff0, \coeff0, \coeff1
  plant_mul_const \q, \twiddle, \tmp, \coeff1
.endm

// in-place plantard reduction to a
// output \in (-0.5q, 0.5q); q32: q<<32
.macro plant_red q32, qinv, a
  mul  \a, \a, \qinv
  srai \a, \a, 32
  addi \a, \a, 8
  mulh \a, \a, \q32
.endm

.equ q,    3329
.equ q32,  0xd0100000000                // q << 32
.equ qinv, 0x3c0f12886ba8f301           // q^-1 mod 2^64
.equ plantconst, 0x13afb7680bb055       // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0x1a390f4d9791e139    // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

// void poly_basemul_acc_cache_init_end_rv64im(int16_t *r, const int16_t *a, const int16_t *b, int16_t *b_cache, uint64_t *zetas, int32_t *r_double)
// compute basemul, cache bzeta into b_cache, accumulate the 32-bit results into r_double, and reduce r_double to r
// a0: r, a1: a, a2: b, a3: b_cache, a4: zetas, a5: r_double
// a6: loop control, a7: accumulated value
// t0-t3: a[2i,2i+1],b[2i,2i+1]
// t4: zeta, t5-t6: temp
// s0: q<<32, s1: qinv
.global poly_basemul_acc_cache_init_end_rv64im_opt_c908
.align 2
poly_basemul_acc_cache_init_end_rv64im_opt_c908:
    addi sp, sp, -8*2
    sd   s0, 0*8(sp)
    sd   s1, 1*8(sp)
    li s0, q32
    li s1, qinv
    li a6, 64
poly_basemul_acc_cache_init_end_rv64im_loop:
                                  // Instructions:    66
                                  // Expected cycles: 34
                                  // Expected IPC:    1.94
                                  //
                                  // Cycle bound:     34.0
                                  // IPC bound:       1.94
                                  //
                                  // Wall time:     14.42s
                                  // User time:     14.42s
                                  //
                                  // ------- cycle (expected) -------->
                                  // 0                        25
                                  // |------------------------|--------
        ld x28, 8*0(x14)          // *.................................
        addi x14, x14, 8*1        // *.................................
        lh x19, 2*3(x12)          // .*................................
        lh x31, 2*1(x12)          // ..*...............................
        neg x26, x28              // ..*...............................
        lh x17, 2*0(x12)          // ...*..............................
        lh x16, 2*1(x11)          // ....*.............................
        mul x25, x19, x26         // ....*.............................
        mul x27, x31, x28         // .....*............................
        lh x28, 2*0(x11)          // .....*............................
        lh x21, 2*2(x12)          // ......*...........................
        addi x12, x12, 2*4        // ......*...........................
        lh x6, 2*2(x11)           // .......*..........................
        mul x1, x16, x17          // .......*..........................
        srai x26, x25, 32         // ........*.........................
        mul x23, x28, x31         // ........*.........................
        srai x27, x27, 32         // .........*........................
        addi x20, x26, 8          // .........*........................
        lw x22, 4*1(x15)          // ..........*.......................
        mulh x7, x20, x8          // ..........*.......................
        mul x5, x6, x21           // ...........*......................
        lh x24, 2*3(x11)          // ...........*......................
        add x1, x23, x1           // ............*.....................
        addi x27, x27, 8          // ............*.....................
        mulh x25, x27, x8         // .............*....................
        add x26, x1, x22          // .............*....................
        mul x20, x24, x7          // ..............*...................
        sh x7, 2*1(x13)           // ..............*...................
        mul x23, x26, x9          // ...............*..................
        mul x30, x6, x19          // ...............*..................
        lw x29, 4*2(x15)          // ................*.................
        mul x4, x24, x21          // ................*.................
        mul x31, x16, x25         // .................*................
        mul x18, x28, x17         // .................*................
        lw x19, 4*3(x15)          // ..................*...............
        add x5, x20, x5           // ..................*...............
        add x28, x5, x29          // ...................*..............
        lw x7, 4*0(x15)           // ...................*..............
        mul x16, x28, x9          // ....................*.............
        add x26, x30, x4          // ....................*.............
        add x5, x31, x18          // .....................*............
        add x6, x26, x19          // .....................*............
        add x5, x5, x7            // ......................*...........
        mul x27, x6, x9           // ......................*...........
        srai x17, x23, 32         // .......................*..........
        mul x5, x5, x9            // .......................*..........
        addi x29, x17, 8          // ........................*.........
        srai x31, x16, 32         // ........................*.........
        addi x31, x31, 8          // .........................*........
        mulh x7, x29, x8          // .........................*........
        mulh x31, x31, x8         // ..........................*.......
        srai x28, x27, 32         // ..........................*.......
        addi x30, x28, 8          // ...........................*......
        srai x17, x5, 32          // ...........................*......
        addi x16, x17, 8          // ............................*.....
        mulh x30, x30, x8         // ............................*.....
        sh x25, 2*0(x13)          // .............................*....
        mulh x4, x16, x8          // .............................*....
        sh x31, 2*2(x10)          // ..............................*...
        addi x11, x11, 2*4        // ..............................*...
        addi x15, x15, 4*4        // ...............................*..
        sh x7, 2*1(x10)           // ...............................*..
        sh x30, 2*3(x10)          // ................................*.
        addi x13, x13, 2*2        // ................................*.
        sh x4, 2*0(x10)           // .................................*
        addi x10, x10, 2*4        // .................................*

                                    // ------- cycle (expected) -------->
                                    // 0                        25
                                    // |------------------------|--------
        // lh x7, 2*0(x12)          // ...*..............................
        // lh x28, 2*1(x12)         // ..*...............................
        // ld x29, 8*0(x14)         // *.................................
        // lh x5, 2*0(x11)          // .....*............................
        // lh x6, 2*1(x11)          // ....*.............................
        // mul  x30, x28, x29       // .....*............................
        // srai x30, x30, 32        // .........*........................
        // addi x30, x30, 8         // ............*.....................
        // mulh x30, x30, x8        // .............*....................
        // sh  x30, 2*0(x13)        // .............................*....
        // lw x17, 4*0(x15)         // ...................*..............
        // mul x30, x6, x30         // .................*................
        // mul x31, x5, x7          // .................*................
        // add x30, x30, x31        // .....................*............
        // add x30, x30, x17        // ......................*...........
        // mul  x30, x30, x9        // .......................*..........
        // srai x30, x30, 32        // ...........................*......
        // addi x30, x30, 8         // ............................*.....
        // mulh x30, x30, x8        // .............................*....
        // sh  x30, 2*0(x10)        // .................................*
        // lw  x17, 4*1(x15)        // ..........*.......................
        // mul x30, x5, x28         // ........*.........................
        // mul x31, x6, x7          // .......*..........................
        // add x30, x30, x31        // ............*.....................
        // add x30, x30, x17        // .............*....................
        // mul  x30, x30, x9        // ...............*..................
        // srai x30, x30, 32        // .......................*..........
        // addi x30, x30, 8         // ........................*.........
        // mulh x30, x30, x8        // .........................*........
        // sh  x30, 2*1(x10)        // ...............................*..
        // neg x29, x29             // ..*...............................
        // lh x7, 2*2(x12)          // ......*...........................
        // lh x28, 2*3(x12)         // .*................................
        // lh x5, 2*2(x11)          // .......*..........................
        // lh x6, 2*3(x11)          // ...........*......................
        // mul  x30, x28, x29       // ....*.............................
        // srai x30, x30, 32        // ........*.........................
        // addi x30, x30, 8         // .........*........................
        // mulh x30, x30, x8        // ..........*.......................
        // sh  x30, 2*1(x13)        // ..............*...................
        // lw  x17, 4*2(x15)        // ................*.................
        // mul x30, x6, x30         // ..............*...................
        // mul x31, x5, x7          // ...........*......................
        // add x30, x30, x31        // ..................*...............
        // add x30, x30, x17        // ...................*..............
        // mul  x30, x30, x9        // ....................*.............
        // srai x30, x30, 32        // ........................*.........
        // addi x30, x30, 8         // .........................*........
        // mulh x30, x30, x8        // ..........................*.......
        // sh  x30, 2*2(x10)        // ..............................*...
        // lw  x17, 4*3(x15)        // ..................*...............
        // mul x30, x5, x28         // ...............*..................
        // mul x31, x6, x7          // ................*.................
        // add x30, x30, x31        // ....................*.............
        // add x30, x30, x17        // .....................*............
        // mul  x30, x30, x9        // ......................*...........
        // srai x30, x30, 32        // ..........................*.......
        // addi x30, x30, 8         // ...........................*......
        // mulh x30, x30, x8        // ............................*.....
        // sh  x30, 2*3(x10)        // ................................*.
        // addi x10, x10, 2*4       // .................................*
        // addi x11, x11, 2*4       // ..............................*...
        // addi x12, x12, 2*4       // ......*...........................
        // addi x13, x13, 2*2       // ................................*.
        // addi x14, x14, 8*1       // *.................................
        // addi x15, x15, 4*4       // ...............................*..

        addi a6, a6, -1
        bne a6, zero,, poly_basemul_acc_cache_init_end_rv64im_loop
    ld   s0, 0*8(sp)
    ld   s1, 1*8(sp)
    addi sp, sp, 8*2
ret