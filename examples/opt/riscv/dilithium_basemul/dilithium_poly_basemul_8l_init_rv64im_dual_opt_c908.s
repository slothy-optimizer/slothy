.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

# void poly_basemul_8l_init_rv64im(int64_t r[256], const int32_t a[256], const int32_t b[256])
.globl poly_basemul_8l_init_rv64im_opt_c908_dual
.align 2
poly_basemul_8l_init_rv64im_opt_c908_dual:
    addi sp, sp, -8*15
    save_regs
    // loop control
    li gp, 32*8*8
    add gp, gp, a0
                // Instructions:    0
                // Expected cycles: 0
                // Expected IPC:    0.00
                //
                // Wall time:     0.01s
                // User time:     0.01s
                //
poly_basemul_8l_init_rv64im_looper:
                                  // Instructions:    34
                                  // Expected cycles: 24
                                  // Expected IPC:    1.42
                                  //
                                  // Cycle bound:     17.0
                                  // IPC bound:       2.00
                                  //
                                  // Wall time:     6.89s
                                  // User time:     6.89s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        lw x26, 1*4(x11)          // *.............................
        lw x21, 0*4(x11)          // .*............................
        lw x31, 0*4(x12)          // ..*...........................
        lw x5, 1*4(x12)           // ...*..........................
        mul x27, x21, x31         // ....*.........................
        lw x21, 3*4(x11)          // ....*.........................
        mul x4, x26, x5           // .....*........................
        lw x15, 3*4(x12)          // .....*........................
        lw x20, 2*4(x11)          // ......*.......................
        mul x24, x21, x15         // .......*......................
        lw x18, 4*4(x11)          // .......*......................
        lw x5, 5*4(x11)           // ........*.....................
        lw x21, 4*4(x12)          // .........*....................
        lw x15, 2*4(x12)          // ..........*...................
        mul x29, x18, x21         // ...........*..................
        lw x6, 5*4(x12)           // ...........*..................
        lw x31, 6*4(x12)          // ............*.................
        mul x18, x20, x15         // .............*................
        lw x1, 7*4(x12)           // .............*................
        mul x22, x5, x6           // ..............*...............
        sd x27, 0*8(x10)          // ..............*...............
        lw x19, 6*4(x11)          // ...............*..............
        lw x8, 7*4(x11)           // ................*.............
        addi x11, x11, 4*8        // ................*.............
        mul x7, x19, x31          // .................*............
        sd x4, 1*8(x10)           // .................*............
        sd x18, 2*8(x10)          // ..................*...........
        mul x5, x8, x1            // ..................*...........
        sd x24, 3*8(x10)          // ...................*..........
        addi x12, x12, 4*8        // ...................*..........
        sd x29, 4*8(x10)          // ....................*.........
        sd x22, 5*8(x10)          // .....................*........
        sd x7, 6*8(x10)           // ......................*.......
        sd x5, 7*8(x10)           // .......................*......

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // lw x5,  0*4(x11)        // .*......................'~.....
        // lw x6,  1*4(x11)        // *.......................~......
        // lw x8,  0*4(x12)        // ..*.....................'.~....
        // lw x9,  1*4(x12)        // ...*....................'..~...
        // lw x7,  2*4(x11)        // ......*.................'......
        // lw x28,  3*4(x11)       // ....*...................'...~..
        // lw x18,  2*4(x12)       // ..........*.............'......
        // lw x19,  3*4(x12)       // .....*..................'....~.
        // mul x24, x5, x8         // ....*...................'...~..
        // mul x25, x6, x9         // .....*..................'....~.
        // lw x29,  4*4(x11)       // .......*................'......
        // lw x30,  5*4(x11)       // ........*...............'......
        // mul x26, x7, x18        // .............*..........'......
        // mul x27, x28, x19       // .......*................'......
        // lw x20,  4*4(x12)       // .........*..............'......
        // lw x21,  5*4(x12)       // ...........*............'......
        // lw x31,  6*4(x11)       // ...............*........'......
        // lw x4,  7*4(x11)        // ................*.......'......
        // lw x22,  6*4(x12)       // ............*...........'......
        // lw x23,  7*4(x12)       // .............*..........'......
        // mul x13, x29, x20       // ...........*............'......
        // mul x14, x30, x21       // ..............*.........'......
        // sd x24,  0*8(x10)       // ..............*.........'......
        // sd x25,  1*8(x10)       // .................*......'......
        // mul x15, x31, x22       // .................*......'......
        // mul x16, x4, x23        // ..................*.....'......
        // sd x26, 2*8(x10)        // ..................*.....'......
        // sd x27, 3*8(x10)        // ...................*....'......
        // sd x13,  4*8(x10)       // ....................*...'......
        // sd x14,  5*8(x10)       // .....................*..'......
        // addi x11, x11, 4*8      // ................*.......'......
        // addi x12, x12, 4*8      // ...................*....'......
        // sd x15,  6*8(x10)       // ......................*.'......
        // sd x16,  7*8(x10)       // .......................*'......

        addi gp, gp, 64
        bne gp, a0, poly_basemul_8l_init_rv64im_looper
                // Instructions:    0
                // Expected cycles: 0
                // Expected IPC:    0.00
                //
                // Wall time:     0.01s
                // User time:     0.01s
                //
    restore_regs
    addi sp, sp, 8*15
    ret