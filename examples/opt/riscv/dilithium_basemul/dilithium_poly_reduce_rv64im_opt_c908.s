/// Copyright (c) 2024 Jipeng Zhang (jp-zhang@outlook.com) (Original Code)
/// Copyright (c) 2026 Amin Abdulrahman (amin@abdulrahman.de) (Modifications)
/// Copyright (c) 2026 Justus Bergermann (mail@justus-bergermann.de) (Modifications)
///
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.

.equ q,    8380417
.equ q32,  0x7fe00100000000               // q << 32
.equ qinv, 0x180a406003802001             // q^-1 mod 2^64
.equ plantconst, 0x200801c0602            // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0xb7b9f10ccf939804      // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

# void poly_reduce_rv64im(int32_t in[256]);
.globl poly_reduce_rv64im_opt_c908
.align 2
poly_reduce_rv64im_opt_c908:
    li a1, 4194304  # 1<<22
    li a2, q
    addi a3, a0, 64*4*4
                                 // Instructions:    4
                                 // Expected cycles: 4
                                 // Expected IPC:    1.00
                                 //
                                 // Cycle bound:     4.0
                                 // IPC bound:       1.00
                                 //
                                 // Wall time:     0.02s
                                 // User time:     0.02s
                                 //
                                 // ----- cycle (expected) ------>
                                 // 0                        25
                                 // |------------------------|----
        lw x25, 1*4(x10)         // *.............................
        lw x4, 3*4(x10)          // .*............................
        add x31, x25, x11        // ..*...........................
        add x8, x4, x11          // ...*..........................

                                  // ------ cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|-----
        // lw x25, 1*4(x10)       // *..............................
        // lw x4, 3*4(x10)        // .*.............................
        // add x31, x25, x11      // ..*............................
        // add x8, x4, x11        // ...*...........................

        addi a3, a3, -16
poly_reduce_rv64im_loop:
                                 // Instructions:    24
                                 // Expected cycles: 12
                                 // Expected IPC:    2.00
                                 //
                                 // Cycle bound:     16.0
                                 // IPC bound:       1.50
                                 //
                                 // Wall time:     2.64s
                                 // User time:     2.64s
                                 //
                                 // ----- cycle (expected) ------>
                                 // 0                        25
                                 // |------------------------|----
        lw x26, 0*4(x10)         // *.............................
        srai x13, x31, 23        // *.............................
        mul x23, x13, x12        // .*............................
        lw x19, 2*4(x10)         // .*............................
        add x31, x26, x11        // ..*...........................
        srai x13, x8, 23         // ..*...........................
        srai x31, x31, 23        // ...*..........................
        mul x8, x13, x12         // ...*..........................
        mul x30, x31, x12        // ....*.........................
        add x31, x19, x11        // ....*.........................
        sub x21, x25, x23        // .....*........................
        srai x31, x31, 23        // .....*........................
        lw x25, 1*4(x10)         // ......e.......................
        mul x22, x31, x12        // ......*.......................
        sub x8, x4, x8           // .......*......................
        lw x4, 3*4(x10)          // .......e......................
        add x31, x25, x11        // ........e.....................
        sw x8, 3*4(x10)          // ........*.....................
        sw x21, 1*4(x10)         // .........*....................
        sub x5, x26, x30         // .........*....................
        sw x5, 0*4(x10)          // ..........*...................
        sub x27, x19, x22        // ..........*...................
        add x8, x4, x11          // ...........e..................
        sw x27, 2*4(x10)         // ...........*..................

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // lw x14, 0*4(x10)        // ......*...........~............
        // lw x15, 1*4(x10)        // e.....'.....~.....'.....~......
        // lw x16, 2*4(x10)        // ......'*..........'~...........
        // lw x17, 3*4(x10)        // .e....'......~....'......~.....
        // add  x5, x14, x11       // ......'.*.........'.~..........
        // add  x6, x15, x11       // ..e...'.......~...'.......~....
        // add  x7, x16, x11       // ......'...*.......'...~........
        // add  x28, x17, x11      // .....e'..........~'..........~.
        // srai x5, x5, 23         // ......'..*........'..~.........
        // srai x6, x6, 23         // ......*...........~............
        // srai x7, x7, 23         // ......'....*......'....~.......
        // srai x28, x28, 23       // ......'.*.........'.~..........
        // mul  x5, x5, x12        // ......'...*.......'...~........
        // mul  x6, x6, x12        // ......'*..........'~...........
        // mul  x7, x7, x12        // ~.....'.....*.....'.....~......
        // mul  x28, x28, x12      // ......'..*........'..~.........
        // sub  x14, x14, x5       // ...~..'........*..'........~...
        // sub  x15, x15, x6       // ......'....*......'....~.......
        // sub  x16, x16, x7       // ....~.'.........*.'.........~..
        // sub  x17, x17, x28      // .~....'......*....'......~.....
        // sw x14, 0*4(x10)        // ....~.'.........*.'.........~..
        // sw x15, 1*4(x10)        // ...~..'........*..'........~...
        // sw x16, 2*4(x10)        // .....~'..........*'..........~.
        // sw x17, 3*4(x10)        // ..~...'.......*...'.......~....

        addi a0, a0, 16
        bne a0, a3, poly_reduce_rv64im_loop
                                 // Instructions:    20
                                 // Expected cycles: 12
                                 // Expected IPC:    1.67
                                 //
                                 // Cycle bound:     12.0
                                 // IPC bound:       1.67
                                 //
                                 // Wall time:     0.17s
                                 // User time:     0.17s
                                 //
                                 // ----- cycle (expected) ------>
                                 // 0                        25
                                 // |------------------------|----
        srai x19, x31, 23        // *.............................
        srai x31, x8, 23         // *.............................
        mul x19, x19, x12        // .*............................
        lw x23, 0*4(x10)         // .*............................
        mul x31, x31, x12        // ..*...........................
        lw x8, 2*4(x10)          // ..*...........................
        add x13, x23, x11        // ...*..........................
        srai x13, x13, 23        // ....*.........................
        add x18, x8, x11         // ....*.........................
        mul x13, x13, x12        // .....*........................
        srai x18, x18, 23        // .....*........................
        sub x25, x25, x19        // ......*.......................
        mul x19, x18, x12        // ......*.......................
        sub x4, x4, x31          // .......*......................
        sw x25, 1*4(x10)         // .......*......................
        sw x4, 3*4(x10)          // ........*.....................
        sub x25, x23, x13        // .........*....................
        sub x19, x8, x19         // ..........*...................
        sw x25, 0*4(x10)         // ..........*...................
        sw x19, 2*4(x10)         // ...........*..................

                                  // ------ cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|-----
        // lw x26, 0*4(x10)       // .*.............................
        // srai x13, x31, 23      // *..............................
        // mul x23, x13, x12      // .*.............................
        // lw x19, 2*4(x10)       // ..*............................
        // add x31, x26, x11      // ...*...........................
        // srai x13, x8, 23       // *..............................
        // srai x31, x31, 23      // ....*..........................
        // mul x8, x13, x12       // ..*............................
        // mul x30, x31, x12      // .....*.........................
        // add x31, x19, x11      // ....*..........................
        // sub x21, x25, x23      // ......*........................
        // srai x31, x31, 23      // .....*.........................
        // mul x22, x31, x12      // ......*........................
        // sub x8, x4, x8         // .......*.......................
        // sw x8, 3*4(x10)        // ........*......................
        // sw x21, 1*4(x10)       // .......*.......................
        // sub x5, x26, x30       // .........*.....................
        // sw x5, 0*4(x10)        // ..........*....................
        // sub x27, x19, x22      // ..........*....................
        // sw x27, 2*4(x10)       // ...........*...................

    ret