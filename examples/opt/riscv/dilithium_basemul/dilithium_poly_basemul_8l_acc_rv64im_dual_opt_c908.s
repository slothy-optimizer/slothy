.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

// void poly_basemul_8l_acc_rv64im_dual(int64_t r[256], const int32_t a[256], const int32_t b[256])
.globl poly_basemul_8l_acc_rv64im_dual_opt_c908
.align 2
poly_basemul_8l_acc_rv64im_dual_opt_c908:
    addi sp, sp, -8*15
    save_regs
    // loop control
    li  gp, 32*8*8
    add gp, gp, a0
                // Instructions:    0
                // Expected cycles: 0
                // Expected IPC:    0.00
                //
                // Wall time:     0.01s
                // User time:     0.01s
                //
poly_basemul_8l_acc_rv64im_looper:
                                  // Instructions:    50
                                  // Expected cycles: 32
                                  // Expected IPC:    1.56
                                  //
                                  // Cycle bound:     25.0
                                  // IPC bound:       2.00
                                  //
                                  // Wall time:     15.76s
                                  // User time:     15.76s
                                  //
                                  // ------ cycle (expected) ------->
                                  // 0                        25
                                  // |------------------------|------
        lw x16, 0*4(x11)          // *...............................
        ld x24, 2*8(x10)          // .*..............................
        ld x29, 0*8(x10)          // ..*.............................
        lw x23, 3*4(x12)          // ...*............................
        lw x25, 0*4(x12)          // ....*...........................
        lw x1, 3*4(x11)           // .....*..........................
        mul x28, x16, x25         // ......*.........................
        lw x25, 7*4(x11)          // ......*.........................
        lw x5, 2*4(x12)           // .......*........................
        mul x19, x1, x23          // .......*........................
        lw x13, 6*4(x12)          // ........*.......................
        ld x8, 3*8(x10)           // .........*......................
        ld x23, 4*8(x10)          // ..........*.....................
        lw x7, 5*4(x12)           // ...........*....................
        add x21, x19, x8          // ...........*....................
        lw x14, 4*4(x12)          // ............*...................
        ld x8, 1*8(x10)           // .............*..................
        lw x16, 1*4(x12)          // ..............*.................
        lw x19, 1*4(x11)          // ...............*................
        add x29, x28, x29         // ................*...............
        lw x1, 2*4(x11)           // ................*...............
        lw x28, 5*4(x11)          // .................*..............
        mul x30, x19, x16         // .................*..............
        lw x19, 7*4(x12)          // ..................*.............
        addi x12, x12, 4*8        // ..................*.............
        ld x4, 7*8(x10)           // ...................*............
        mul x1, x1, x5            // ...................*............
        lw x15, 4*4(x11)          // ....................*...........
        mul x31, x28, x7          // ....................*...........
        mul x20, x25, x19         // .....................*..........
        lw x18, 6*4(x11)          // .....................*..........
        sd x29, 0*8(x10)          // ......................*.........
        mul x7, x15, x14          // ......................*.........
        ld x28, 5*8(x10)          // .......................*........
        add x9, x1, x24           // .......................*........
        sd x9, 2*8(x10)           // ........................*.......
        add x19, x30, x8          // ........................*.......
        sd x21, 3*8(x10)          // .........................*......
        mul x13, x18, x13         // .........................*......
        sd x19, 1*8(x10)          // ..........................*.....
        add x9, x31, x28          // ..........................*.....
        addi x11, x11, 4*8        // ...........................*....
        sd x9, 5*8(x10)           // ...........................*....
        add x22, x20, x4          // ............................*...
        ld x20, 6*8(x10)          // ............................*...
        add x27, x7, x23          // .............................*..
        sd x22, 7*8(x10)          // .............................*..
        add x6, x13, x20          // ..............................*.
        sd x27, 4*8(x10)          // ..............................*.
        sd x6, 6*8(x10)           // ...............................*

                                   // ------ cycle (expected) ------->
                                   // 0                        25
                                   // |------------------------|------
        // lw x5, 0*4(x11)         // *...............................
        // lw x6, 1*4(x11)         // ...............*................
        // lw x8, 0*4(x12)         // ....*...........................
        // lw x9, 1*4(x12)         // ..............*.................
        // ld x13, 0*8(x10)        // ..*.............................
        // ld x14, 1*8(x10)        // .............*..................
        // lw x7, 2*4(x11)         // ................*...............
        // lw x28, 3*4(x11)        // .....*..........................
        // lw x18, 2*4(x12)        // .......*........................
        // lw x19, 3*4(x12)        // ...*............................
        // ld x15, 2*8(x10)        // .*..............................
        // ld x16, 3*8(x10)        // .........*......................
        // mul x24, x5, x8         // ......*.........................
        // mul x25, x6, x9         // .................*..............
        // lw x29, 4*4(x11)        // ....................*...........
        // lw x30, 5*4(x11)        // .................*..............
        // mul x26, x7, x18        // ...................*............
        // mul x27, x28, x19       // .......*........................
        // lw x20, 4*4(x12)        // ............*...................
        // lw x21, 5*4(x12)        // ...........*....................
        // add x24, x24, x13       // ................*...............
        // add x25, x25, x14       // ........................*.......
        // lw x31, 6*4(x11)        // .....................*..........
        // lw x4, 7*4(x11)         // ......*.........................
        // add x26, x26, x15       // .......................*........
        // add x27, x27, x16       // ...........*....................
        // sd x24, 0*8(x10)        // ......................*.........
        // sd x25, 1*8(x10)        // ..........................*.....
        // lw x22, 6*4(x12)        // ........*.......................
        // lw x23, 7*4(x12)        // ..................*.............
        // ld x13, 4*8(x10)        // ..........*.....................
        // ld x14, 5*8(x10)        // .......................*........
        // sd x26, 2*8(x10)        // ........................*.......
        // sd x27, 3*8(x10)        // .........................*......
        // mul x24, x29, x20       // ......................*.........
        // mul x25, x30, x21       // ....................*...........
        // ld x15, 6*8(x10)        // ............................*...
        // ld x16, 7*8(x10)        // ...................*............
        // mul x26, x31, x22       // .........................*......
        // mul x27, x4, x23        // .....................*..........
        // add x24, x24, x13       // .............................*..
        // add x25, x25, x14       // ..........................*.....
        // sd x24, 4*8(x10)        // ..............................*.
        // sd x25, 5*8(x10)        // ...........................*....
        // add x26, x26, x15       // ..............................*.
        // add x27, x27, x16       // ............................*...
        // sd x26, 6*8(x10)        // ...............................*
        // sd x27, 7*8(x10)        // .............................*..
        // addi x11, x11, 4*8      // ...........................*....
        // addi x12, x12, 4*8      // ..................*.............

        addi a0, a0, 64
        bne a0, gp, poly_basemul_8l_acc_rv64im_looper
                // Instructions:    0
                // Expected cycles: 0
                // Expected IPC:    0.00
                //
                // Wall time:     0.01s
                // User time:     0.01s
                //
    restore_regs
    addi sp, sp, 8*15
    ret