/// Copyright (c) 2024 Jipeng Zhang (jp-zhang@outlook.com) (Original Code)
/// Copyright (c) 2026 Amin Abdulrahman (amin@abdulrahman.de) (Modifications)
/// Copyright (c) 2026 Justus Bergermann (mail@justus-bergermann.de) (Modifications)
///
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.

.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

.macro plant_red_x4 q32, qinv, a_0, a_1, a_2, a_3
  mul  \a_0, \a_0, \qinv
  mul  \a_1, \a_1, \qinv
  mul  \a_2, \a_2, \qinv
  mul  \a_3, \a_3, \qinv
  srai \a_0, \a_0, 32
  srai \a_1, \a_1, 32
  srai \a_2, \a_2, 32
  srai \a_3, \a_3, 32
  addi \a_0, \a_0, 256
  addi \a_1, \a_1, 256
  addi \a_2, \a_2, 256
  addi \a_3, \a_3, 256
  mulh \a_0, \a_0, \q32
  mulh \a_1, \a_1, \q32
  mulh \a_2, \a_2, \q32
  mulh \a_3, \a_3, \q32
.endm
.equ q,    8380417
.equ q32,  0x7fe00100000000               // q << 32
.equ qinv, 0x180a406003802001             // q^-1 mod 2^64
.equ plantconst, 0x200801c0602            // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0xb7b9f10ccf939804      // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

# void poly_basemul_8l_acc_end_rv64im(int32_t r[256], const int32_t a[256], const int32_t b[256], int64_t r_double[256])
.globl poly_basemul_8l_acc_end_rv64im_opt_c908
.align 2
poly_basemul_8l_acc_end_rv64im_opt_c908:
    addi sp, sp, -8*15
    save_regs
    li a4, q32
    li a5, qinv
    // loop control
    li gp, 64*4*4
    add gp, gp, a0
                                // Instructions:    7
                                // Expected cycles: 6
                                // Expected IPC:    1.17
                                //
                                // Cycle bound:     6.0
                                // IPC bound:       1.17
                                //
                                // Wall time:     0.03s
                                // User time:     0.03s
                                //
                                // ----- cycle (expected) ------>
                                // 0                        25
                                // |------------------------|----
        lw x31, 0*4(x12)        // *.............................
        lw x19, 1*4(x12)        // .*............................
        lw x4, 2*4(x12)         // ..*...........................
        lw x23, 2*4(x11)        // ...*..........................
        lw x25, 3*4(x12)        // ....*.........................
        mul x5, x23, x4         // .....*........................
        lw x4, 3*4(x11)         // .....*........................

                                 // ------ cycle (expected) ------>
                                 // 0                        25
                                 // |------------------------|-----
        // lw x31, 0*4(x12)      // *..............................
        // lw x19, 1*4(x12)      // .*.............................
        // lw x25, 3*4(x12)      // ....*..........................
        // lw x8, 2*4(x12)       // ..*............................
        // lw x26, 2*4(x11)      // ...*...........................
        // lw x4, 3*4(x11)       // .....*.........................
        // mul x5, x26, x8       // .....*.........................

        addi gp, gp, -16
poly_basemul_8l_acc_end_rv64im_looper:
                                  // Instructions:    44
                                  // Expected cycles: 22
                                  // Expected IPC:    2.00
                                  //
                                  // Cycle bound:     28.0
                                  // IPC bound:       1.57
                                  //
                                  // Wall time:     33.12s
                                  // User time:     33.12s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        lw x26, 0*4(x11)          // *.............................
        mul x21, x4, x25          // *.............................
        lw x25, 1*4(x11)          // .*............................
        addi x12, x12, 4*4        // .*............................
        ld x23, 2*8(x13)          // ..*...........................
        mul x26, x26, x31         // ..*...........................
        ld x18, 3*8(x13)          // ...*..........................
        mul x25, x25, x19         // ...*..........................
        ld x8, 0*8(x13)           // ....*.........................
        add x20, x23, x5          // ....*.........................
        mul x19, x20, x15         // .....*........................
        add x9, x18, x21          // .....*........................
        ld x22, 1*8(x13)          // ......*.......................
        mul x24, x9, x15          // ......*.......................
        lw x31, 0*4(x12)          // .......e......................
        add x8, x8, x26           // .......*......................
        add x18, x22, x25         // ........*.....................
        mul x27, x8, x15          // ........*.....................
        srai x29, x19, 32         // .........*....................
        mul x17, x18, x15         // .........*....................
        addi x25, x29, 256        // ..........*...................
        srai x6, x24, 32          // ..........*...................
        mulh x29, x25, x14        // ...........*..................
        addi x24, x6, 256         // ...........*..................
        srai x25, x27, 32         // ............*.................
        mulh x18, x24, x14        // ............*.................
        lw x19, 1*4(x12)          // .............e................
        addi x23, x25, 256        // .............*................
        lw x25, 3*4(x12)          // ..............e...............
        mulh x23, x23, x14        // ..............*...............
        srai x20, x17, 32         // ...............*..............
        lw x8, 2*4(x12)           // ...............e..............
        sw x18, 3*4(x10)          // ................*.............
        addi x30, x20, 256        // ................*.............
        sw x29, 2*4(x10)          // .................*............
        mulh x30, x30, x14        // .................*............
        sw x23, 0*4(x10)          // ..................*...........
        addi x11, x11, 4*4        // ..................*...........
        lw x26, 2*4(x11)          // ...................e..........
        addi x9, x13, 8*4         // ...................*..........
        lw x4, 3*4(x11)           // ....................e.........
        addi x13, x9, 0           // ....................*.........
        mul x5, x26, x8           // .....................e........
        sw x30, 1*4(x10)          // .....................*........

                                   // --------- cycle (expected) --------->
                                   // 0                        25
                                   // |------------------------|-----------
        // lw x8, 0*4(x11)         // ...............*.....................
        // lw x9, 1*4(x11)         // ...............'*....................
        // lw x18, 2*4(x11)        // ............e..'..................~..
        // lw x19, 3*4(x11)        // .............e.'...................~.
        // lw x5, 0*4(x12)         // e..............'......~..............
        // lw x6, 1*4(x12)         // ......e........'............~........
        // lw x7, 2*4(x12)         // ........e......'..............~......
        // lw x28, 3*4(x12)        // .......e.......'.............~.......
        // ld x20, 0*8(x13)        // ...............'...*.................
        // ld x22, 1*8(x13)        // ...............'.....*...............
        // ld x24, 2*8(x13)        // ...............'.*...................
        // ld x26, 3*8(x13)        // ...............'..*..................
        // mul x29, x8, x5         // ...............'.*...................
        // mul x16, x9, x6         // ...............'..*..................
        // mul x31, x18, x7        // ..............e'.....................
        // mul x17, x19, x28       // ...............*.....................
        // add x20, x20, x29       // ~..............'......*..............
        // add x22, x22, x16       // .~.............'.......*.............
        // add x24, x24, x31       // ...............'...*.................
        // add x26, x26, x17       // ...............'....*................
        // mul  x20, x20, x15      // .~.............'.......*.............
        // mul  x22, x22, x15      // ..~............'........*............
        // mul  x24, x24, x15      // ...............'....*................
        // mul  x26, x26, x15      // ...............'.....*...............
        // srai x20, x20, 32       // .....~.........'...........*.........
        // srai x22, x22, 32       // ........~......'..............*......
        // srai x24, x24, 32       // ..~............'........*............
        // srai x26, x26, 32       // ...~...........'.........*...........
        // addi x20, x20, 256      // ......~........'............*........
        // addi x22, x22, 256      // .........~.....'...............*.....
        // addi x24, x24, 256      // ...~...........'.........*...........
        // addi x26, x26, 256      // ....~..........'..........*..........
        // mulh x20, x20, x14      // .......~.......'.............*.......
        // mulh x22, x22, x14      // ..........~....'................*....
        // mulh x24, x24, x14      // ....~..........'..........*..........
        // mulh x26, x26, x14      // .....~.........'...........*.........
        // sw x20, 0*4(x10)        // ...........~...'.................*...
        // sw x22, 1*4(x10)        // ..............~'....................*
        // sw x24, 2*4(x10)        // ..........~....'................*....
        // sw x26, 3*4(x10)        // .........~.....'...............*.....
        // addi x11, x11, 4*4      // ...........~...'.................*...
        // addi x12, x12, 4*4      // ...............'*....................
        // addi x13, x13, 8*4      // ............~..'..................*..
        // addi x13, x13, 0        // .............~.'...................*.

        addi a0, a0, 16
        bne a0, gp, poly_basemul_8l_acc_end_rv64im_looper
                                  // Instructions:    37
                                  // Expected cycles: 20
                                  // Expected IPC:    1.85
                                  //
                                  // Cycle bound:     20.0
                                  // IPC bound:       1.85
                                  //
                                  // Wall time:     0.58s
                                  // User time:     0.58s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        mul x23, x4, x25          // *.............................
        lw x4, 0*4(x11)           // *.............................
        addi x12, x12, 4*4        // .*............................
        lw x29, 1*4(x11)          // .*............................
        mul x7, x4, x31           // ..*...........................
        ld x4, 3*8(x13)           // ..*...........................
        ld x21, 2*8(x13)          // ...*..........................
        mul x29, x29, x19         // ...*..........................
        add x1, x4, x23           // ....*.........................
        ld x8, 0*8(x13)           // ....*.........................
        mul x24, x1, x15          // .....*........................
        add x21, x21, x5          // .....*........................
        mul x19, x21, x15         // ......*.......................
        ld x30, 1*8(x13)          // ......*.......................
        add x21, x8, x7           // .......*......................
        add x9, x30, x29          // ........*.....................
        mul x29, x21, x15         // ........*.....................
        srai x7, x24, 32          // .........*....................
        mul x8, x9, x15           // .........*....................
        srai x20, x19, 32         // ..........*...................
        addi x21, x7, 256         // ..........*...................
        addi x28, x20, 256        // ...........*..................
        mulh x30, x21, x14        // ...........*..................
        mulh x18, x28, x14        // ............*.................
        srai x31, x29, 32         // ............*.................
        srai x24, x8, 32          // .............*................
        addi x1, x31, 256         // .............*................
        addi x7, x24, 256         // ..............*...............
        mulh x19, x1, x14         // ..............*...............
        addi x17, x13, 8*4        // ...............*..............
        mulh x26, x7, x14         // ...............*..............
        sw x30, 3*4(x10)          // ................*.............
        addi x11, x11, 4*4        // .................*............
        sw x18, 2*4(x10)          // .................*............
        addi x13, x17, 0          // ..................*...........
        sw x19, 0*4(x10)          // ..................*...........
        sw x26, 1*4(x10)          // ...................*..........

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // lw x26, 0*4(x11)        // *..............................
        // mul x21, x4, x25        // *..............................
        // lw x25, 1*4(x11)        // .*.............................
        // addi x12, x12, 4*4      // .*.............................
        // ld x23, 2*8(x13)        // ...*...........................
        // mul x26, x26, x31       // ..*............................
        // ld x18, 3*8(x13)        // ..*............................
        // mul x25, x25, x19       // ...*...........................
        // ld x8, 0*8(x13)         // ....*..........................
        // add x20, x23, x5        // .....*.........................
        // mul x19, x20, x15       // ......*........................
        // add x9, x18, x21        // ....*..........................
        // ld x22, 1*8(x13)        // ......*........................
        // mul x24, x9, x15        // .....*.........................
        // add x8, x8, x26         // .......*.......................
        // add x18, x22, x25       // ........*......................
        // mul x27, x8, x15        // ........*......................
        // srai x29, x19, 32       // ..........*....................
        // mul x17, x18, x15       // .........*.....................
        // addi x25, x29, 256      // ...........*...................
        // srai x6, x24, 32        // .........*.....................
        // mulh x29, x25, x14      // ............*..................
        // addi x24, x6, 256       // ..........*....................
        // srai x25, x27, 32       // ............*..................
        // mulh x18, x24, x14      // ...........*...................
        // addi x23, x25, 256      // .............*.................
        // mulh x23, x23, x14      // ..............*................
        // srai x20, x17, 32       // .............*.................
        // sw x18, 3*4(x10)        // ................*..............
        // addi x30, x20, 256      // ..............*................
        // sw x29, 2*4(x10)        // .................*.............
        // mulh x30, x30, x14      // ...............*...............
        // sw x23, 0*4(x10)        // ..................*............
        // addi x11, x11, 4*4      // .................*.............
        // addi x9, x13, 8*4       // ...............*...............
        // addi x13, x9, 0         // ..................*............
        // sw x30, 1*4(x10)        // ...................*...........

    restore_regs
    addi sp, sp, 8*15
    ret