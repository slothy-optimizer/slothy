.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

.macro plant_red_x4 q32, qinv, a_0, a_1, a_2, a_3
  mul  \a_0, \a_0, \qinv
  mul  \a_1, \a_1, \qinv
  mul  \a_2, \a_2, \qinv
  mul  \a_3, \a_3, \qinv
  srai \a_0, \a_0, 32
  srai \a_1, \a_1, 32
  srai \a_2, \a_2, 32
  srai \a_3, \a_3, 32
  addi \a_0, \a_0, 256
  addi \a_1, \a_1, 256
  addi \a_2, \a_2, 256
  addi \a_3, \a_3, 256
  mulh \a_0, \a_0, \q32
  mulh \a_1, \a_1, \q32
  mulh \a_2, \a_2, \q32
  mulh \a_3, \a_3, \q32
.endm
.equ q,    8380417
.equ q32,  0x7fe00100000000               // q << 32
.equ qinv, 0x180a406003802001             // q^-1 mod 2^64
.equ plantconst, 0x200801c0602            // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0xb7b9f10ccf939804      // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

# void poly_basemul_8l_acc_end_rv64im(int32_t r[256], const int32_t a[256], const int32_t b[256], int64_t r_double[256])
.globl poly_basemul_8l_acc_end_rv64im_opt_c908
.align 2
poly_basemul_8l_acc_end_rv64im_opt_c908:
    addi sp, sp, -8*15
    save_regs
    li a4, q32
    li a5, qinv
    // loop control
    li gp, 64*4*4
    add gp, gp, a0
                                  // Instructions:    7
                                  // Expected cycles: 6
                                  // Expected IPC:    1.17
                                  //
                                  // Cycle bound:     6.0
                                  // IPC bound:       1.17
                                  //
                                  // Wall time:     0.02s
                                  // User time:     0.02s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        lw x9, 0*4(x12)           // *.............................
        lw x24, 3*4(x12)          // .*............................
        lw x30, 2*4(x12)          // ..*...........................
        lw x26, 1*4(x12)          // ...*..........................
        addi x12, x12, 4*4        // ...*..........................
        lw x22, 2*4(x11)          // ....*.........................
        lw x19, 3*4(x11)          // .....*........................

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // lw x9, 0*4(x12)         // *..............................
        // lw x24, 3*4(x12)        // .*.............................
        // lw x30, 2*4(x12)        // ..*............................
        // lw x26, 1*4(x12)        // ...*...........................
        // lw x22, 2*4(x11)        // ....*..........................
        // lw x19, 3*4(x11)        // .....*.........................
        // addi x12, x12, 4*4      // ...*...........................

        addi gp, gp, 16
poly_basemul_8l_acc_end_rv64im_looper:
                                  // Instructions:    44
                                  // Expected cycles: 22
                                  // Expected IPC:    2.00
                                  //
                                  // Cycle bound:     29.0
                                  // IPC bound:       1.52
                                  //
                                  // Wall time:     35.19s
                                  // User time:     35.19s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        lw x16, 0*4(x11)          // *.............................
        mul x25, x19, x24         // *.............................
        lw x19, 1*4(x11)          // .*............................
        mul x4, x22, x30          // .*............................
        ld x27, 3*8(x13)          // ..*...........................
        mul x6, x16, x9           // ..*...........................
        mul x23, x19, x26         // ...*..........................
        ld x9, 2*8(x13)           // ...*..........................
        ld x8, 1*8(x13)           // ....*.........................
        add x1, x27, x25          // ....*.........................
        add x30, x9, x4           // .....*........................
        mul x18, x1, x15          // .....*........................
        ld x26, 0*8(x13)          // ......*.......................
        mul x5, x30, x15          // ......*.......................
        lw x9, 0*4(x12)           // .......e......................
        add x27, x8, x23          // .......*......................
        add x6, x26, x6           // ........*.....................
        mul x30, x27, x15         // ........*.....................
        mul x31, x6, x15          // .........*....................
        lw x24, 3*4(x12)          // .........e....................
        srai x18, x18, 32         // ..........*...................
        srai x29, x5, 32          // ..........*...................
        addi x27, x29, 256        // ...........*..................
        addi x17, x18, 256        // ...........*..................
        srai x20, x30, 32         // ............*.................
        mulh x4, x27, x14         // ............*.................
        addi x20, x20, 256        // .............*................
        mulh x1, x17, x14         // .............*................
        lw x30, 2*4(x12)          // ..............e...............
        mulh x17, x20, x14        // ..............*...............
        lw x26, 1*4(x12)          // ...............e..............
        srai x5, x31, 32          // ...............*..............
        sw x4, 2*4(x10)           // ................*.............
        addi x20, x5, 256         // ................*.............
        sw x1, 3*4(x10)           // .................*............
        mulh x18, x20, x14        // .................*............
        addi x11, x11, 4*4        // ..................*...........
        sw x17, 1*4(x10)          // ..................*...........
        lw x22, 2*4(x11)          // ...................e..........
        addi x21, x13, 8*4        // ...................*..........
        lw x19, 3*4(x11)          // ....................e.........
        addi x13, x21, 0          // ....................*.........
        addi x12, x12, 4*4        // .....................e........
        sw x18, 0*4(x10)          // .....................*........

                                   // --------- cycle (expected) --------->
                                   // 0                        25
                                   // |------------------------|-----------
        // lw x8, 0*4(x11)         // ...............*.....................
        // lw x9, 1*4(x11)         // ...............'*....................
        // lw x18, 2*4(x11)        // ............e..'..................~..
        // lw x19, 3*4(x11)        // .............e.'...................~.
        // lw x5, 0*4(x12)         // e..............'......~..............
        // lw x6, 1*4(x12)         // ........e......'..............~......
        // lw x7, 2*4(x12)         // .......e.......'.............~.......
        // lw x28, 3*4(x12)        // ..e............'........~............
        // ld x20, 0*8(x13)        // ...............'.....*...............
        // ld x22, 1*8(x13)        // ...............'...*.................
        // ld x24, 2*8(x13)        // ...............'..*..................
        // ld x26, 3*8(x13)        // ...............'.*...................
        // mul x29, x8, x5         // ...............'.*...................
        // mul x16, x9, x6         // ...............'..*..................
        // mul x31, x18, x7        // ...............'*....................
        // mul x17, x19, x28       // ...............*.....................
        // add x20, x20, x29       // .~.............'.......*.............
        // add x22, x22, x16       // ~..............'......*..............
        // add x24, x24, x31       // ...............'....*................
        // add x26, x26, x17       // ...............'...*.................
        // mul  x20, x20, x15      // ..~............'........*............
        // mul  x22, x22, x15      // .~.............'.......*.............
        // mul  x24, x24, x15      // ...............'.....*...............
        // mul  x26, x26, x15      // ...............'....*................
        // srai x20, x20, 32       // ........~......'..............*......
        // srai x22, x22, 32       // .....~.........'...........*.........
        // srai x24, x24, 32       // ...~...........'.........*...........
        // srai x26, x26, 32       // ...~...........'.........*...........
        // addi x20, x20, 256      // .........~.....'...............*.....
        // addi x22, x22, 256      // ......~........'............*........
        // addi x24, x24, 256      // ....~..........'..........*..........
        // addi x26, x26, 256      // ....~..........'..........*..........
        // mulh x20, x20, x14      // ..........~....'................*....
        // mulh x22, x22, x14      // .......~.......'.............*.......
        // mulh x24, x24, x14      // .....~.........'...........*.........
        // mulh x26, x26, x14      // ......~........'............*........
        // sw x20, 0*4(x10)        // ..............~'....................*
        // sw x22, 1*4(x10)        // ...........~...'.................*...
        // sw x24, 2*4(x10)        // .........~.....'...............*.....
        // sw x26, 3*4(x10)        // ..........~....'................*....
        // addi x11, x11, 4*4      // ...........~...'.................*...
        // addi x12, x12, 4*4      // ..............e'.....................
        // addi x13, x13, 8*4      // ............~..'..................*..
        // addi x13, x13, 0        // .............~.'...................*.

        addi gp, gp, 16
        bne gp, a0, poly_basemul_8l_acc_end_rv64im_looper
                                  // Instructions:    37
                                  // Expected cycles: 20
                                  // Expected IPC:    1.85
                                  //
                                  // Cycle bound:     20.0
                                  // IPC bound:       1.85
                                  //
                                  // Wall time:     0.55s
                                  // User time:     0.55s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        mul x19, x19, x24         // *.............................
        lw x20, 0*4(x11)          // *.............................
        lw x24, 1*4(x11)          // .*............................
        mul x5, x22, x30          // .*............................
        mul x1, x20, x9           // ..*...........................
        ld x23, 3*8(x13)          // ..*...........................
        mul x20, x24, x26         // ...*..........................
        ld x4, 2*8(x13)           // ...*..........................
        add x16, x23, x19         // ....*.........................
        ld x30, 0*8(x13)          // ....*.........................
        mul x25, x16, x15         // .....*........................
        add x7, x4, x5            // .....*........................
        ld x24, 1*8(x13)          // ......*.......................
        mul x21, x7, x15          // ......*.......................
        add x22, x30, x1          // .......*......................
        mul x16, x22, x15         // ........*.....................
        add x19, x24, x20         // ........*.....................
        srai x8, x25, 32          // .........*....................
        mul x19, x19, x15         // .........*....................
        addi x4, x8, 256          // ..........*...................
        srai x29, x21, 32         // ..........*...................
        mulh x8, x4, x14          // ...........*..................
        addi x31, x29, 256        // ...........*..................
        mulh x30, x31, x14        // ............*.................
        srai x23, x16, 32         // ............*.................
        addi x25, x23, 256        // .............*................
        srai x19, x19, 32         // .............*................
        mulh x5, x25, x14         // ..............*...............
        addi x20, x19, 256        // ..............*...............
        mulh x16, x20, x14        // ...............*..............
        addi x31, x13, 8*4        // ................*.............
        sw x8, 3*4(x10)           // ................*.............
        sw x30, 2*4(x10)          // .................*............
        addi x11, x11, 4*4        // ..................*...........
        sw x5, 0*4(x10)           // ..................*...........
        addi x13, x31, 0          // ...................*..........
        sw x16, 1*4(x10)          // ...................*..........

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // lw x16, 0*4(x11)        // *..............................
        // mul x25, x19, x24       // *..............................
        // lw x19, 1*4(x11)        // .*.............................
        // mul x4, x22, x30        // .*.............................
        // ld x27, 3*8(x13)        // ..*............................
        // mul x6, x16, x9         // ..*............................
        // mul x23, x19, x26       // ...*...........................
        // ld x9, 2*8(x13)         // ...*...........................
        // ld x8, 1*8(x13)         // ......*........................
        // add x1, x27, x25        // ....*..........................
        // add x30, x9, x4         // .....*.........................
        // mul x18, x1, x15        // .....*.........................
        // ld x26, 0*8(x13)        // ....*..........................
        // mul x5, x30, x15        // ......*........................
        // add x27, x8, x23        // ........*......................
        // add x6, x26, x6         // .......*.......................
        // mul x30, x27, x15       // .........*.....................
        // mul x31, x6, x15        // ........*......................
        // srai x18, x18, 32       // .........*.....................
        // srai x29, x5, 32        // ..........*....................
        // addi x27, x29, 256      // ...........*...................
        // addi x17, x18, 256      // ..........*....................
        // srai x20, x30, 32       // .............*.................
        // mulh x4, x27, x14       // ............*..................
        // addi x20, x20, 256      // ..............*................
        // mulh x1, x17, x14       // ...........*...................
        // mulh x17, x20, x14      // ...............*...............
        // srai x5, x31, 32        // ............*..................
        // sw x4, 2*4(x10)         // .................*.............
        // addi x20, x5, 256       // .............*.................
        // sw x1, 3*4(x10)         // ................*..............
        // mulh x18, x20, x14      // ..............*................
        // addi x11, x11, 4*4      // ..................*............
        // sw x17, 1*4(x10)        // ...................*...........
        // addi x21, x13, 8*4      // ................*..............
        // addi x13, x21, 0        // ...................*...........
        // sw x18, 0*4(x10)        // ..................*............

    restore_regs
    addi sp, sp, 8*15
    ret