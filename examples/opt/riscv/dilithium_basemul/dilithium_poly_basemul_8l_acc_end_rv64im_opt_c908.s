.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

.macro plant_red_x4 q32, qinv, a_0, a_1, a_2, a_3
  mul  \a_0, \a_0, \qinv
  mul  \a_1, \a_1, \qinv
  mul  \a_2, \a_2, \qinv
  mul  \a_3, \a_3, \qinv
  srai \a_0, \a_0, 32
  srai \a_1, \a_1, 32
  srai \a_2, \a_2, 32
  srai \a_3, \a_3, 32
  addi \a_0, \a_0, 256
  addi \a_1, \a_1, 256
  addi \a_2, \a_2, 256
  addi \a_3, \a_3, 256
  mulh \a_0, \a_0, \q32
  mulh \a_1, \a_1, \q32
  mulh \a_2, \a_2, \q32
  mulh \a_3, \a_3, \q32
.endm
.equ q,    8380417
.equ q32,  0x7fe00100000000               // q << 32
.equ qinv, 0x180a406003802001             // q^-1 mod 2^64
.equ plantconst, 0x200801c0602            // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0xb7b9f10ccf939804      // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

# void poly_basemul_8l_acc_end_rv64im(int32_t r[256], const int32_t a[256], const int32_t b[256], int64_t r_double[256])
.globl poly_basemul_8l_acc_end_rv64im
.align 2
poly_basemul_8l_acc_end_rv64im:
    addi sp, sp, -8*15
    save_regs
    li a4, q32
    li a5, qinv
    // loop control
    li gp, 64*4*4
    add gp, gp, a0
poly_basemul_8l_acc_end_rv64im_looper:
                                  // Instructions:    44
                                  // Expected cycles: 26
                                  // Expected IPC:    1.69
                                  //
                                  // Cycle bound:     26.0
                                  // IPC bound:       1.69
                                  //
                                  // Wall time:     1.33s
                                  // User time:     1.33s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        addi x6, x13, 8*4         // *.............................
        lw x29, 0*4(x11)          // *.............................
        lw x30, 0*4(x12)          // .*............................
        lw x1, 1*4(x12)           // ..*...........................
        lw x5, 1*4(x11)           // ...*..........................
        mul x29, x29, x30         // ...*..........................
        ld x30, 0*8(x13)          // ....*.........................
        mul x17, x5, x1           // .....*........................
        lw x24, 2*4(x12)          // .....*........................
        lw x22, 2*4(x11)          // ......*.......................
        add x29, x30, x29         // .......*......................
        lw x7, 3*4(x11)           // .......*......................
        lw x26, 3*4(x12)          // ........*.....................
        mul x22, x22, x24         // ........*.....................
        ld x9, 1*8(x13)           // .........*....................
        mul x29, x29, x15         // .........*....................
        mul x7, x7, x26           // ..........*...................
        ld x31, 2*8(x13)          // ..........*...................
        add x30, x9, x17          // ...........*..................
        ld x28, 3*8(x13)          // ...........*..................
        add x9, x31, x22          // ............*.................
        mul x30, x30, x15         // ............*.................
        addi x12, x12, 4*4        // .............*................
        mul x9, x9, x15           // .............*................
        add x31, x28, x7          // ..............*...............
        srai x29, x29, 32         // ...............*..............
        mul x31, x31, x15         // ...............*..............
        addi x29, x29, 256        // ................*.............
        srai x30, x30, 32         // ................*.............
        addi x30, x30, 256        // .................*............
        srai x9, x9, 32           // .................*............
        addi x9, x9, 256          // ..................*...........
        mulh x29, x29, x14        // ..................*...........
        srai x31, x31, 32         // ...................*..........
        mulh x30, x30, x14        // ...................*..........
        mulh x9, x9, x14          // ....................*.........
        addi x31, x31, 256        // ....................*.........
        addi x11, x11, 4*4        // .....................*........
        mulh x31, x31, x14        // .....................*........
        sw x29, 0*4(x10)          // ......................*.......
        addi x13, x6, 0           // .......................*......
        sw x30, 1*4(x10)          // .......................*......
        sw x9, 2*4(x10)           // ........................*.....
        sw x31, 3*4(x10)          // .........................*....

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // lw x8, 0*4(x11)         // *..............................
        // lw x9, 1*4(x11)         // ...*...........................
        // lw x18, 2*4(x11)        // ......*........................
        // lw x19, 3*4(x11)        // .......*.......................
        // lw x5, 0*4(x12)         // .*.............................
        // lw x6, 1*4(x12)         // ..*............................
        // lw x7, 2*4(x12)         // .....*.........................
        // lw x28, 3*4(x12)        // ........*......................
        // ld x20, 0*8(x13)        // ....*..........................
        // ld x22, 1*8(x13)        // .........*.....................
        // ld x24, 2*8(x13)        // ..........*....................
        // ld x26, 3*8(x13)        // ...........*...................
        // mul x29, x8, x5         // ...*...........................
        // mul x16, x9, x6         // .....*.........................
        // mul x31, x18, x7        // ........*......................
        // mul x17, x19, x28       // ..........*....................
        // add x20, x20, x29       // .......*.......................
        // add x22, x22, x16       // ...........*...................
        // add x24, x24, x31       // ............*..................
        // add x26, x26, x17       // ..............*................
        // mul  x20, x20, x15      // .........*.....................
        // mul  x22, x22, x15      // ............*..................
        // mul  x24, x24, x15      // .............*.................
        // mul  x26, x26, x15      // ...............*...............
        // srai x20, x20, 32       // ...............*...............
        // srai x22, x22, 32       // ................*..............
        // srai x24, x24, 32       // .................*.............
        // srai x26, x26, 32       // ...................*...........
        // addi x20, x20, 256      // ................*..............
        // addi x22, x22, 256      // .................*.............
        // addi x24, x24, 256      // ..................*............
        // addi x26, x26, 256      // ....................*..........
        // mulh x20, x20, x14      // ..................*............
        // mulh x22, x22, x14      // ...................*...........
        // mulh x24, x24, x14      // ....................*..........
        // mulh x26, x26, x14      // .....................*.........
        // sw x20, 0*4(x10)        // ......................*........
        // sw x22, 1*4(x10)        // .......................*.......
        // sw x24, 2*4(x10)        // ........................*......
        // sw x26, 3*4(x10)        // .........................*.....
        // addi x11, x11, 4*4      // .....................*.........
        // addi x12, x12, 4*4      // .............*.................
        // addi x13, x13, 8*4      // *..............................
        // addi x13, x13, 0        // .......................*.......

        addi gp, gp, 16
        bne gp, a0, poly_basemul_8l_acc_end_rv64im_looper
    restore_regs
    addi sp, sp, 8*15
    ret