.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

.macro plant_red_x4 q32, qinv, a_0, a_1, a_2, a_3
  mul  \a_0, \a_0, \qinv
  mul  \a_1, \a_1, \qinv
  mul  \a_2, \a_2, \qinv
  mul  \a_3, \a_3, \qinv
  srai \a_0, \a_0, 32
  srai \a_1, \a_1, 32
  srai \a_2, \a_2, 32
  srai \a_3, \a_3, 32
  addi \a_0, \a_0, 256
  addi \a_1, \a_1, 256
  addi \a_2, \a_2, 256
  addi \a_3, \a_3, 256
  mulh \a_0, \a_0, \q32
  mulh \a_1, \a_1, \q32
  mulh \a_2, \a_2, \q32
  mulh \a_3, \a_3, \q32
.endm

.equ q,    8380417
.equ q32,  0x7fe00100000000               // q << 32
.equ qinv, 0x180a406003802001             // q^-1 mod 2^64
.equ plantconst, 0x200801c0602            // (((-2**64) % q) * qinv) % (2**64)
.equ plantconst2, 0xb7b9f10ccf939804      // (((-2**64) % q) * ((-2**64) % q) * qinv) % (2**64)

# void poly_basemul_8l_rv64im(int32_t r[256], const int32_t a[256], const int32_t b[256])
.globl poly_basemul_8l_rv64im_dual
.align 2
poly_basemul_8l_rv64im_dual:
    addi sp, sp, -8*15
    save_regs
    li a4, q32
    li a5, qinv
    # loop control
    li gp, 32*8*4
    add gp, gp, a0
poly_basemul_8l_rv64im_looper:
                                  // Instructions:    66
                                  // Expected cycles: 37
                                  // Expected IPC:    1.78
                                  //
                                  // Cycle bound:     35.0
                                  // IPC bound:       1.89
                                  //
                                  // Wall time:     866.89s
                                  // User time:     866.89s
                                  //
                                  // --------- cycle (expected) --------->
                                  // 0                        25
                                  // |------------------------|-----------
        lw x24, 1*4(x11)          // *....................................
        lw x18, 1*4(x12)          // .*...................................
        lw x7, 0*4(x12)           // ..*..................................
        mul x24, x18, x24         // ...*.................................
        lw x27, 0*4(x11)          // ...*.................................
        lw x6, 3*4(x12)           // ....*................................
        lw x16, 3*4(x11)          // .....*...............................
        mul x7, x7, x27           // .....*...............................
        lw x28, 2*4(x12)          // ......*..............................
        lw x27, 2*4(x11)          // .......*.............................
        mul x18, x6, x16          // .......*.............................
        lw x6, 7*4(x12)           // ........*............................
        mul x24, x24, x15         // ........*............................
        lw x16, 7*4(x11)          // .........*...........................
        mul x28, x28, x27         // .........*...........................
        lw x30, 5*4(x11)          // ..........*..........................
        mul x7, x7, x15           // ..........*..........................
        mul x6, x6, x16           // ...........*.........................
        mul x18, x18, x15         // ...........*.........................
        lw x17, 5*4(x12)          // ............*........................
        srai x24, x24, 32         // ............*........................
        mul x22, x28, x15         // .............*.......................
        addi x21, x24, 256        // .............*.......................
        lw x27, 4*4(x11)          // ..............*......................
        mul x24, x17, x30         // ..............*......................
        srai x7, x7, 32           // ...............*.....................
        srai x29, x18, 32         // ...............*.....................
        addi x1, x7, 256          // ................*....................
        lw x26, 4*4(x12)          // ................*....................
        srai x7, x22, 32          // .................*...................
        lw x28, 6*4(x11)          // .................*...................
        addi x9, x7, 256          // ..................*..................
        lw x18, 6*4(x12)          // ..................*..................
        mulh x16, x9, x14         // ...................*.................
        mul x7, x26, x27          // ...................*.................
        mul x18, x18, x28         // ....................*................
        mulh x30, x1, x14         // .....................*...............
        addi x1, x29, 256         // .....................*...............
        mul x24, x24, x15         // ......................*..............
        mulh x21, x21, x14        // ......................*..............
        sw x16, 2*4(x10)          // .......................*.............
        mul x7, x7, x15           // .......................*.............
        mulh x28, x1, x14         // ........................*............
        mul x18, x18, x15         // ........................*............
        sw x30, 0*4(x10)          // .........................*...........
        mul x6, x6, x15           // .........................*...........
        srai x24, x24, 32         // ..........................*..........
        sw x21, 1*4(x10)          // ..........................*..........
        srai x7, x7, 32           // ...........................*.........
        addi x24, x24, 256        // ...........................*.........
        mulh x26, x24, x14        // ............................*........
        srai x18, x18, 32         // ............................*........
        srai x16, x6, 32          // .............................*.......
        addi x7, x7, 256          // .............................*.......
        addi x24, x16, 256        // ..............................*......
        mulh x6, x7, x14          // ..............................*......
        addi x7, x18, 256         // ...............................*.....
        mulh x24, x24, x14        // ...............................*.....
        sw x28, 3*4(x10)          // ................................*....
        mulh x7, x7, x14          // ................................*....
        sw x26, 5*4(x10)          // .................................*...
        addi x12, x12, 4*8        // ..................................*..
        sw x6, 4*4(x10)           // ..................................*..
        sw x24, 7*4(x10)          // ...................................*.
        addi x11, x11, 4*8        // ....................................*
        sw x7, 6*4(x10)           // ....................................*

                                   // --------- cycle (expected) --------->
                                   // 0                        25
                                   // |------------------------|-----------
        // lw x5, 0*4(x11)         // ...*.................................
        // lw x6, 1*4(x11)         // *....................................
        // lw x8, 0*4(x12)         // ..*..................................
        // lw x9, 1*4(x12)         // .*...................................
        // lw x7, 2*4(x11)         // .......*.............................
        // lw x28, 3*4(x11)        // .....*...............................
        // lw x18, 2*4(x12)        // ......*..............................
        // lw x19, 3*4(x12)        // ....*................................
        // mul x24, x8, x5         // .....*...............................
        // mul x25, x9, x6         // ...*.................................
        // lw x29, 4*4(x11)        // ..............*......................
        // lw x30, 5*4(x11)        // ..........*..........................
        // mul x26, x18, x7        // .........*...........................
        // mul x27, x19, x28       // .......*.............................
        // lw x20, 4*4(x12)        // ................*....................
        // lw x21, 5*4(x12)        // ............*........................
        // mul  x24, x24, x15      // ..........*..........................
        // mul  x25, x25, x15      // ........*............................
        // mul  x26, x26, x15      // .............*.......................
        // mul  x27, x27, x15      // ...........*.........................
        // srai x24, x24, 32       // ...............*.....................
        // srai x25, x25, 32       // ............*........................
        // srai x26, x26, 32       // .................*...................
        // srai x27, x27, 32       // ...............*.....................
        // addi x24, x24, 256      // ................*....................
        // addi x25, x25, 256      // .............*.......................
        // addi x26, x26, 256      // ..................*..................
        // addi x27, x27, 256      // .....................*...............
        // mulh x24, x24, x14      // .....................*...............
        // mulh x25, x25, x14      // ......................*..............
        // mulh x26, x26, x14      // ...................*.................
        // mulh x27, x27, x14      // ........................*............
        // lw x31, 6*4(x11)        // .................*...................
        // lw x4, 7*4(x11)         // .........*...........................
        // lw x22, 6*4(x12)        // ..................*..................
        // lw x23, 7*4(x12)        // ........*............................
        // mul x8, x20, x29        // ...................*.................
        // mul x9, x21, x30        // ..............*......................
        // sw x24, 0*4(x10)        // .........................*...........
        // sw x25, 1*4(x10)        // ..........................*..........
        // mul x18, x22, x31       // ....................*................
        // mul x19, x23, x4        // ...........*.........................
        // sw x26, 2*4(x10)        // .......................*.............
        // sw x27, 3*4(x10)        // ................................*....
        // mul  x8, x8, x15        // .......................*.............
        // mul  x9, x9, x15        // ......................*..............
        // mul  x18, x18, x15      // ........................*............
        // mul  x19, x19, x15      // .........................*...........
        // srai x8, x8, 32         // ...........................*.........
        // srai x9, x9, 32         // ..........................*..........
        // srai x18, x18, 32       // ............................*........
        // srai x19, x19, 32       // .............................*.......
        // addi x8, x8, 256        // .............................*.......
        // addi x9, x9, 256        // ...........................*.........
        // addi x18, x18, 256      // ...............................*.....
        // addi x19, x19, 256      // ..............................*......
        // mulh x8, x8, x14        // ..............................*......
        // mulh x9, x9, x14        // ............................*........
        // mulh x18, x18, x14      // ................................*....
        // mulh x19, x19, x14      // ...............................*.....
        // sw x8, 4*4(x10)         // ..................................*..
        // sw x9, 5*4(x10)         // .................................*...
        // addi x11, x11, 4*8      // ....................................*
        // addi x12, x12, 4*8      // ..................................*..
        // sw x18, 6*4(x10)        // ....................................*
        // sw x19, 7*4(x10)        // ...................................*.

        addi gp, gp, 32
        bne gp, a0, poly_basemul_8l_rv64im_looper
    restore_regs
    addi sp, sp, 8*15
    ret