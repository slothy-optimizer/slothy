.macro save_regs
  sd s0,  0*8(sp)
  sd s1,  1*8(sp)
  sd s2,  2*8(sp)
  sd s3,  3*8(sp)
  sd s4,  4*8(sp)
  sd s5,  5*8(sp)
  sd s6,  6*8(sp)
  sd s7,  7*8(sp)
  sd s8,  8*8(sp)
  sd s9,  9*8(sp)
  sd s10, 10*8(sp)
  sd s11, 11*8(sp)
  sd gp,  12*8(sp)
  sd tp,  13*8(sp)
  sd ra,  14*8(sp)
.endm

.macro restore_regs
  ld s0,  0*8(sp)
  ld s1,  1*8(sp)
  ld s2,  2*8(sp)
  ld s3,  3*8(sp)
  ld s4,  4*8(sp)
  ld s5,  5*8(sp)
  ld s6,  6*8(sp)
  ld s7,  7*8(sp)
  ld s8,  8*8(sp)
  ld s9,  9*8(sp)
  ld s10, 10*8(sp)
  ld s11, 11*8(sp)
  ld gp,  12*8(sp)
  ld tp,  13*8(sp)
  ld ra,  14*8(sp)
.endm

# void poly_basemul_8l_init_rv64im(int64_t r[256], const int32_t a[256], const int32_t b[256])
.globl poly_basemul_8l_init_rv64im_opt_c908
.align 2
poly_basemul_8l_init_rv64im_opt_c908:
    addi sp, sp, -8*15
    save_regs
    // loop control
    li gp, 32*8*8
    add gp, gp, a0
poly_basemul_8l_init_rv64im_looper:
                                  // Instructions:    34
                                  // Expected cycles: 24
                                  // Expected IPC:    1.42
                                  //
                                  // Cycle bound:     24.0
                                  // IPC bound:       1.42
                                  //
                                  // Wall time:     0.15s
                                  // User time:     0.15s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        lw x13, 0*4(x11)          // *.............................
        lw x20, 1*4(x11)          // .*............................
        lw x26, 2*4(x11)          // ..*...........................
        lw x30, 3*4(x11)          // ...*..........................
        lw x16, 4*4(x11)          // ....*.........................
        lw x8, 5*4(x11)           // .....*........................
        lw x7, 6*4(x11)           // ......*.......................
        lw x5, 7*4(x11)           // .......*......................
        addi x11, x11, 4*8        // .......*......................
        lw x18, 0*4(x12)          // ........*.....................
        lw x22, 1*4(x12)          // .........*....................
        mul x13, x13, x18         // ..........*...................
        lw x18, 2*4(x12)          // ..........*...................
        mul x20, x20, x22         // ...........*..................
        lw x22, 3*4(x12)          // ...........*..................
        mul x26, x26, x18         // ............*.................
        lw x18, 4*4(x12)          // ............*.................
        mul x30, x30, x22         // .............*................
        lw x22, 5*4(x12)          // .............*................
        mul x16, x16, x18         // ..............*...............
        lw x18, 6*4(x12)          // ..............*...............
        mul x8, x8, x22           // ...............*..............
        lw x22, 7*4(x12)          // ...............*..............
        mul x7, x7, x18           // ................*.............
        sd x13, 0*8(x10)          // ................*.............
        mul x13, x5, x22          // .................*............
        sd x20, 1*8(x10)          // .................*............
        sd x26, 2*8(x10)          // ..................*...........
        sd x30, 3*8(x10)          // ...................*..........
        sd x16, 4*8(x10)          // ....................*.........
        sd x8, 5*8(x10)           // .....................*........
        sd x7, 6*8(x10)           // ......................*.......
        sd x13, 7*8(x10)          // .......................*......
        addi x10, x10, 8*8        // .......................*......

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // lw x5, 0*4(x11)         // *..............................
        // lw x8, 0*4(x12)         // ........*......................
        // lw x6, 1*4(x11)         // .*.............................
        // lw x9, 1*4(x12)         // .........*.....................
        // lw x7, 2*4(x11)         // ..*............................
        // lw x18, 2*4(x12)        // ..........*....................
        // lw x28, 3*4(x11)        // ...*...........................
        // lw x19, 3*4(x12)        // ...........*...................
        // mul x24, x5, x8         // ..........*....................
        // mul x26, x6, x9         // ...........*...................
        // mul x13, x7, x18        // ............*..................
        // mul x15, x28, x19       // .............*.................
        // sd x24, 0*8(x10)        // ................*..............
        // sd x26, 1*8(x10)        // .................*.............
        // sd x13, 2*8(x10)        // ..................*............
        // sd x15, 3*8(x10)        // ...................*...........
        // lw x29, 4*4(x11)        // ....*..........................
        // lw x20, 4*4(x12)        // ............*..................
        // lw x30, 5*4(x11)        // .....*.........................
        // lw x21, 5*4(x12)        // .............*.................
        // lw x31, 6*4(x11)        // ......*........................
        // lw x22, 6*4(x12)        // ..............*................
        // lw x4, 7*4(x11)         // .......*.......................
        // lw x23, 7*4(x12)        // ...............*...............
        // mul x24, x29, x20       // ..............*................
        // mul x26, x30, x21       // ...............*...............
        // mul x13, x31, x22       // ................*..............
        // mul x15, x4, x23        // .................*.............
        // sd x24, 4*8(x10)        // ....................*..........
        // sd x26, 5*8(x10)        // .....................*.........
        // sd x13, 6*8(x10)        // ......................*........
        // sd x15, 7*8(x10)        // .......................*.......
        // addi x10, x10, 8*8      // .......................*.......
        // addi x11, x11, 4*8      // .......*.......................

        addi gp, gp, 4
        bne gp, a0, poly_basemul_8l_init_rv64im_looper
    restore_regs
    addi sp, sp, 8*15
    ret
