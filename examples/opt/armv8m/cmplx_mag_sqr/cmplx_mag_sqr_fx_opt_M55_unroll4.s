        .syntax unified
        .type   cmplx_mag_sqr_fx_opt_M55_unroll4, %function
        .global cmplx_mag_sqr_fx_opt_M55_unroll4

        .text
        .align 4
cmplx_mag_sqr_fx_opt_M55_unroll4:
        push {r4-r12,lr}
        vpush {d0-d15}

        out   .req r0
        in    .req r1
        sz    .req r2

        qr   .req q0
        qi   .req q1
        qtmp .req q2
        qout .req q3

        lsr lr, sz, #2
        wls lr, lr, end
        vld20.32 {q2,q3}, [r1]          // *..............
        // gap                          // ...............
        vld21.32 {q2,q3}, [r1]!         // .*.............
        // gap                          // ...............
        vld20.32 {q0,q1}, [r1]          // ..*............
        vmulh.s32 q2, q2, q2            // ...*...........
        vld20.32 {q5,q6}, [r1]          // ....*..........
        vmulh.s32 q4, q3, q3            // .....*.........
        vld21.32 {q5,q6}, [r1]!         // ......*........
        vhadd.s32 q3, q4, q2            // .......*.......
        vmulh.s32 q2, q5, q5            // ...........*...
        vld20.32 {q4,q5}, [r1]          // ........*......
        vmulh.s32 q7, q6, q6            // .........*.....
        vld21.32 {q4,q5}, [r1]!         // ..........*....
        vhadd.s32 q6, q7, q2            // ..............*
        vld21.32 {q0,q1}, [r1]!         // ............*..
        vmulh.s32 q5, q5, q5            // .............*.
        
        // original source code
        // vld20.32 {q4,q5}, [r1]       // *.............. 
        // vld21.32 {q4,q5}, [r1]!      // .*............. 
        // vld20.32 {q0,q1}, [r1]       // ..*............ 
        // vmulh.s32 q3, q4, q4         // ...*........... 
        // vld20.32 {q6,q7}, [r1]       // ....*.......... 
        // vmulh.s32 q4, q5, q5         // .....*......... 
        // vld21.32 {q6,q7}, [r1]!      // ......*........ 
        // vhadd.s32 q3, q4, q3         // .......*....... 
        // vld20.32 {q4,q5}, [r1]       // .........*..... 
        // vmulh.s32 q7, q7, q7         // ..........*.... 
        // vld21.32 {q4,q5}, [r1]!      // ...........*... 
        // vmulh.s32 q6, q6, q6         // ........*...... 
        // vld21.32 {q0,q1}, [r1]!      // .............*. 
        // vmulh.s32 q5, q5, q5         // ..............* 
        // vhadd.s32 q6, q7, q6         // ............*.. 
        
        lsr lr, lr, #2
        sub lr, lr, #1
.p2align 2
start:
        vstrw.u32 q3, [r0] , #16         // .....*..................
        vmulh.s32 q4, q4, q4             // ..............*.........
        vstrw.u32 q6, [r0] , #16         // ...........*............
        vhadd.s32 q7, q5, q4             // ................*.......
        vstrw.u32 q7, [r0] , #16         // .................*......
        // gap                           // ........................
        vmulh.s32 q6, q0, q0             // ....................*...
        vld20.32 {q4,q5}, [r1]           // e.......................
        vmulh.s32 q2, q1, q1             // .....................*..
        vld21.32 {q4,q5}, [r1]!          // .e......................
        vhadd.s32 q2, q2, q6             // ......................*.
        vld20.32 {q0,q1}, [r1]           // ..................e.....
        vmulh.s32 q3, q4, q4             // ..e.....................
        vld20.32 {q6,q7}, [r1]           // ......e.................
        vmulh.s32 q4, q5, q5             // ...e....................
        vld21.32 {q6,q7}, [r1]!          // .......e................
        vhadd.s32 q3, q4, q3             // ....e...................
        vld20.32 {q4,q5}, [r1]           // ............e...........
        vmulh.s32 q7, q7, q7             // .........e..............
        vld21.32 {q4,q5}, [r1]!          // .............e..........
        vmulh.s32 q6, q6, q6             // ........e...............
        vld21.32 {q0,q1}, [r1]!          // ...................e....
        vmulh.s32 q5, q5, q5             // ...............e........
        vstrw.u32 q2, [r0] , #16         // .......................*
        vhadd.s32 q6, q7, q6             // ..........e.............
        
        // original source code
        // vld20.32 {q0,q1}, [r1]        // e........................................ 
        // vld21.32 {q0,q1}, [r1]!       // ..e...................................... 
        // vmulh.s32 q2, q0, q0          // .....e................................... 
        // vmulh.s32 q3, q1, q1          // .......e................................. 
        // vhadd.s32 q3, q3, q2          // .........e............................... 
        // vstrw.u32 q3, [r0] , #16      // ..................*...................... 
        // vld20.32 {q0,q1}, [r1]        // ......e.................................. 
        // vld21.32 {q0,q1}, [r1]!       // ........e................................ 
        // vmulh.s32 q2, q0, q0          // .............e........................... 
        // vmulh.s32 q3, q1, q1          // ...........e............................. 
        // vhadd.s32 q3, q3, q2          // .................e....................... 
        // vstrw.u32 q3, [r0] , #16      // ....................*.................... 
        // vld20.32 {q0,q1}, [r1]        // ..........e.............................. 
        // vld21.32 {q0,q1}, [r1]!       // ............e............................ 
        // vmulh.s32 q2, q0, q0          // ...................*..................... 
        // vmulh.s32 q3, q1, q1          // ...............e......................... 
        // vhadd.s32 q3, q3, q2          // .....................*................... 
        // vstrw.u32 q3, [r0] , #16      // ......................*.................. 
        // vld20.32 {q0,q1}, [r1]        // ....e.................................... 
        // vld21.32 {q0,q1}, [r1]!       // ..............e.......................... 
        // vmulh.s32 q2, q0, q0          // .......................*................. 
        // vmulh.s32 q3, q1, q1          // .........................*............... 
        // vhadd.s32 q3, q3, q2          // ...........................*............. 
        // vstrw.u32 q3, [r0] , #16      // ........................................* 
        
        le lr, start
        vstrw.u32 q3, [r0] , #16         // *........
        vmulh.s32 q2, q4, q4             // .*.......
        vstrw.u32 q6, [r0] , #16         // ..*......
        vmulh.s32 q4, q0, q0             // .....*...
        vhadd.s32 q0, q5, q2             // ...*.....
        vmulh.s32 q6, q1, q1             // ......*..
        vstrw.u32 q0, [r0] , #16         // ....*....
        vhadd.s32 q6, q6, q4             // .......*.
        vstrw.u32 q6, [r0] , #16         // ........*
        
        // original source code
        // vstrw.u32 q3, [r0] , #16      // *........ 
        // vmulh.s32 q4, q4, q4          // .*....... 
        // vstrw.u32 q6, [r0] , #16      // ..*...... 
        // vhadd.s32 q7, q5, q4          // ....*.... 
        // vstrw.u32 q7, [r0] , #16      // ......*.. 
        // vmulh.s32 q6, q0, q0          // ...*..... 
        // vmulh.s32 q2, q1, q1          // .....*... 
        // vhadd.s32 q2, q2, q6          // .......*. 
        // vstrw.u32 q2, [r0] , #16      // ........* 
        
end:

        vpop {d0-d15}
        pop {r4-r12,lr}

        bx lr