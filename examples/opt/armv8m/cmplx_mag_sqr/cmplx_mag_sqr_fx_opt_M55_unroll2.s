        .syntax unified
        .type   cmplx_mag_sqr_fx_opt_M55_unroll2, %function
        .global cmplx_mag_sqr_fx_opt_M55_unroll2

        .text
        .align 4
cmplx_mag_sqr_fx_opt_M55_unroll2:
        push {r4-r12,lr}
        vpush {d0-d15}

        out   .req r0
        in    .req r1
        sz    .req r2

        qr   .req q0
        qi   .req q1
        qtmp .req q2
        qout .req q3

        lsr lr, sz, #2
        wls lr, lr, end
        vld20.32 {q2,q3}, [r1]          // *......
        // gap                          // .......
        vld21.32 {q2,q3}, [r1]!         // .*.....
        // gap                          // .......
        vld20.32 {q4,q5}, [r1]          // ..*....
        vmulh.s32 q0, q2, q2            // ...*...
        vld21.32 {q4,q5}, [r1]!         // ....*..
        // gap                          // .......
        vmulh.s32 q7, q4, q4            // .....*.
        // gap                          // .......
        vmulh.s32 q4, q3, q3            // ......*
        
        // original source code
        // vld20.32 {q2,q3}, [r1]       // *...... 
        // vld21.32 {q2,q3}, [r1]!      // .*..... 
        // vld20.32 {q4,q5}, [r1]       // ..*.... 
        // vmulh.s32 q0, q2, q2         // ...*... 
        // vld21.32 {q4,q5}, [r1]!      // ....*.. 
        // vmulh.s32 q7, q4, q4         // .....*. 
        // vmulh.s32 q4, q3, q3         // ......* 
        
        lsr lr, lr, #1
        sub lr, lr, #1
.p2align 2
start:
        vld20.32 {q2,q3}, [r1]           // e...........
        vmulh.s32 q6, q5, q5             // .........*..
        vld21.32 {q2,q3}, [r1]!          // .e..........
        vhadd.s32 q1, q4, q0             // ....*.......
        vld20.32 {q4,q5}, [r1]           // ......e.....
        vmulh.s32 q0, q2, q2             // ..e.........
        vld21.32 {q4,q5}, [r1]!          // .......e....
        vhadd.s32 q2, q6, q7             // ..........*.
        vstrw.u32 q1, [r0] , #16         // .....*......
        vmulh.s32 q7, q4, q4             // ........e...
        vstrw.u32 q2, [r0] , #16         // ...........*
        vmulh.s32 q4, q3, q3             // ...e........
        // gap                           // ............
        
        // original source code
        // vld20.32 {q0,q1}, [r1]        // e...................... 
        // vld21.32 {q0,q1}, [r1]!       // ..e.................... 
        // vmulh.s32 q2, q0, q0          // .....e................. 
        // vmulh.s32 q3, q1, q1          // ...........e........... 
        // vhadd.s32 q3, q3, q2          // ...............*....... 
        // vstrw.u32 q3, [r0] , #16      // ....................*.. 
        // vld20.32 {q0,q1}, [r1]        // ....e.................. 
        // vld21.32 {q0,q1}, [r1]!       // ......e................ 
        // vmulh.s32 q2, q0, q0          // .........e............. 
        // vmulh.s32 q3, q1, q1          // .............*......... 
        // vhadd.s32 q3, q3, q2          // ...................*... 
        // vstrw.u32 q3, [r0] , #16      // ......................* 
        
        le lr, start
        vhadd.s32 q6, q4, q0             // .*...
        vmulh.s32 q2, q5, q5             // *....
        vstrw.u32 q6, [r0] , #16         // ...*.
        vhadd.s32 q6, q2, q7             // ..*..
        vstrw.u32 q6, [r0] , #16         // ....*
        
        // original source code
        // vmulh.s32 q6, q5, q5          // .*... 
        // vhadd.s32 q1, q4, q0          // *.... 
        // vhadd.s32 q2, q6, q7          // ...*. 
        // vstrw.u32 q1, [r0] , #16      // ..*.. 
        // vstrw.u32 q2, [r0] , #16      // ....* 
        
end:

        vpop {d0-d15}
        pop {r4-r12,lr}

        bx lr