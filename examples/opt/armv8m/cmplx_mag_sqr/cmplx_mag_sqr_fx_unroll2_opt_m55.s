        .syntax unified
        .type   cmplx_mag_sqr_fx_unroll2_opt_M55, %function
        .global cmplx_mag_sqr_fx_unroll2_opt_M55

        .text
        .align 4
cmplx_mag_sqr_fx_unroll2_opt_M55:
        push {r4-r12,lr}
        vpush {d0-d15}

        out   .req r0
        in    .req r1
        sz    .req r2

        lsr lr, sz, #2
        wls lr, lr, end
.p2align 2
        vld20.32 {q2, q3}, [r1]
        vld20.32 {q6, q7}, [r1]
        vld21.32 {q6, q7}, [r1]!
        vld21.32 {q2, q3}, [r1]!
        vmulh.S32 q4, q7, q7
        vmulh.S32 q0, q2, q2
        lsr lr, lr, #1
        sub lr, lr, #1
.p2align 2
start:
                                        // Instructions:    12
                                        // Expected cycles: 13
                                        // Expected IPC:    0.92
                                        //
                                        // Wall time:     0.11s
                                        // User time:     0.11s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vmulh.S32 q5, q3, q3            // *.............................
        vld20.32 {q2, q3}, [r1]         // ..e...........................
        vmulh.S32 q1, q6, q6            // ...*..........................
        vld20.32 {q6, q7}, [r1]         // ....e.........................
        vhadd.S32 q1, q4, q1            // .....*........................
        vld21.32 {q6, q7}, [r1]!        // ......e.......................
        vhadd.S32 q5, q5, q0            // .......*......................
        vld21.32 {q2, q3}, [r1]!        // ........e.....................
        vmulh.S32 q4, q7, q7            // .........e....................
        vstrw.U32 q1, [r0], #16         // ..........*...................
        vmulh.S32 q0, q2, q2            // ...........e..................
        vstrw.U32 q5, [r0], #16         // ............*.................

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vld20.32 {q4,q5}, [r1]        // ..e........'...~........'...~..
        // vld21.32 {q4,q5}, [r1]!       // ....e......'.....~......'......
        // vmulh.s32 q2, q4, q4          // .~.........'..*.........'..~...
        // vmulh.s32 q4, q5, q5          // .......e...'........~...'......
        // vhadd.s32 q4, q4, q2          // ...~.......'....*.......'....~.
        // vstrw.u32 q4, [r0] , #16      // ........~..'.........*..'......
        // vld20.32 {q4,q5}, [r1]        // e..........'.~..........'.~....
        // vld21.32 {q4,q5}, [r1]!       // ......e....'.......~....'......
        // vmulh.s32 q2, q4, q4          // .........e.'..........~.'......
        // vmulh.s32 q4, q5, q5          // ...........*............~......
        // vhadd.s32 q4, q4, q2          // .....~.....'......*.....'......
        // vstrw.u32 q4, [r0] , #16      // ..........~'...........*'......

        le lr, start
        vmulh.S32 q5, q3, q3
        vmulh.S32 q1, q6, q6
        vhadd.S32 q1, q4, q1
        vhadd.S32 q5, q5, q0
        vstrw.U32 q1, [r0], #16
        vstrw.U32 q5, [r0], #16
end:

        vpop {d0-d15}
        pop {r4-r12,lr}

        bx lr