        .syntax unified
        .type   cmplx_mag_sqr_fx_unroll4_opt_M55, %function
        .global cmplx_mag_sqr_fx_unroll4_opt_M55

        .text
        .align 4
cmplx_mag_sqr_fx_unroll4_opt_M55:
        push {r4-r12,lr}
        vpush {d0-d15}

        out   .req r0
        in    .req r1
        sz    .req r2

        lsr lr, sz, #2
        wls lr, lr, end
.p2align 2
        vld20.32 {q4, q5}, [r1]
        vld21.32 {q4, q5}, [r1]!
        vmulh.S32 q5, q5, q5
        vld20.32 {q6, q7}, [r1]
        vmulh.S32 q1, q4, q4
        vld21.32 {q6, q7}, [r1]!
        vhadd.S32 q3, q5, q1
        vld20.32 {q0, q1}, [r1]
        vmulh.S32 q5, q6, q6
        vld21.32 {q0, q1}, [r1]!
        vmulh.S32 q4, q7, q7
        vld20.32 {q6, q7}, [r1]
        vmulh.S32 q0, q0, q0
        vld21.32 {q6, q7}, [r1]!
        vhadd.S32 q2, q4, q5
        lsr lr, lr, #2
        sub lr, lr, #1
.p2align 2
start:
                                        // Instructions:    24
                                        // Expected cycles: 25
                                        // Expected IPC:    0.96
                                        //
                                        // Wall time:     2.01s
                                        // User time:     2.01s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vld20.32 {q4, q5}, [r1]         // e.............................
        vmulh.S32 q6, q6, q6            // .*............................
        vld21.32 {q4, q5}, [r1]!        // ..e...........................
        vmulh.S32 q7, q7, q7            // ...*..........................
        vstrw.U32 q3, [r0], #16         // ....*.........................
        vmulh.S32 q3, q1, q1            // .....*........................
        vstrw.U32 q2, [r0], #16         // ......*.......................
        vhadd.S32 q0, q3, q0            // .......*......................
        vstrw.U32 q0, [r0], #16         // ........*.....................
        vhadd.S32 q3, q7, q6            // .........*....................
        vstrw.U32 q3, [r0], #16         // ..........*...................
        vmulh.S32 q5, q5, q5            // ............e.................
        vld20.32 {q6, q7}, [r1]         // .............e................
        vmulh.S32 q1, q4, q4            // ..............e...............
        vld21.32 {q6, q7}, [r1]!        // ...............e..............
        vhadd.S32 q3, q5, q1            // ................e.............
        vld20.32 {q0, q1}, [r1]         // .................e............
        vmulh.S32 q5, q6, q6            // ..................e...........
        vld21.32 {q0, q1}, [r1]!        // ...................e..........
        vmulh.S32 q4, q7, q7            // ....................e.........
        vld20.32 {q6, q7}, [r1]         // .....................e........
        vmulh.S32 q0, q0, q0            // ......................e.......
        vld21.32 {q6, q7}, [r1]!        // .......................e......
        vhadd.S32 q2, q4, q5            // ........................e.....

                                         // -------- cycle (expected) --------->
                                         // 0                        25
                                         // |------------------------|----------
        // vld20.32 {q4,q5}, [r1]        // e........................~..........
        // vld21.32 {q4,q5}, [r1]!       // ..e......................'.~........
        // vmulh.s32 q2, q4, q4          // ..............e..........'..........
        // vmulh.s32 q4, q5, q5          // ............e............'..........
        // vhadd.s32 q4, q4, q2          // ................e........'..........
        // vstrw.u32 q4, [r0] , #16      // ....~....................'...*......
        // vld20.32 {q4,q5}, [r1]        // .............e...........'..........
        // vld21.32 {q4,q5}, [r1]!       // ...............e.........'..........
        // vmulh.s32 q2, q4, q4          // ..................e......'..........
        // vmulh.s32 q4, q5, q5          // ....................e....'..........
        // vhadd.s32 q4, q4, q2          // ........................e'..........
        // vstrw.u32 q4, [r0] , #16      // ......~..................'.....*....
        // vld20.32 {q4,q5}, [r1]        // .................e.......'..........
        // vld21.32 {q4,q5}, [r1]!       // ...................e.....'..........
        // vmulh.s32 q2, q4, q4          // ......................e..'..........
        // vmulh.s32 q4, q5, q5          // .....~...................'....*.....
        // vhadd.s32 q4, q4, q2          // .......~.................'......*...
        // vstrw.u32 q4, [r0] , #16      // ........~................'.......*..
        // vld20.32 {q4,q5}, [r1]        // .....................e...'..........
        // vld21.32 {q4,q5}, [r1]!       // .......................e.'..........
        // vmulh.s32 q2, q4, q4          // .~.......................'*.........
        // vmulh.s32 q4, q5, q5          // ...~.....................'..*.......
        // vhadd.s32 q4, q4, q2          // .........~...............'........*.
        // vstrw.u32 q4, [r0] , #16      // ..........~..............'.........*

        le lr, start
        vmulh.S32 q6, q6, q6
        vmulh.S32 q7, q7, q7
        vstrw.U32 q3, [r0], #16
        vmulh.S32 q3, q1, q1
        vstrw.U32 q2, [r0], #16
        vhadd.S32 q0, q3, q0
        vstrw.U32 q0, [r0], #16
        vhadd.S32 q3, q7, q6
        vstrw.U32 q3, [r0], #16
end:

        vpop {d0-d15}
        pop {r4-r12,lr}

        bx lr