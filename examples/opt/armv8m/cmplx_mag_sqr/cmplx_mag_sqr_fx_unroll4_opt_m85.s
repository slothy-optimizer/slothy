        .syntax unified
        .type   cmplx_mag_sqr_fx_unroll4_opt_M85, %function
        .global cmplx_mag_sqr_fx_unroll4_opt_M85

        .text
        .align 4
cmplx_mag_sqr_fx_unroll4_opt_M85:
        push {r4-r12,lr}
        vpush {d0-d15}

        out   .req r0
        in    .req r1
        sz    .req r2

        lsr lr, sz, #2
        wls lr, lr, end
.p2align 2
        vld20.32 {q0, q1}, [r1]
        vld21.32 {q0, q1}, [r1]!
        vmulh.S32 q6, q1, q1
        vld20.32 {q1, q2}, [r1]
        vld21.32 {q1, q2}, [r1]!
        vmulh.S32 q4, q0, q0
        vhadd.S32 q6, q6, q4
        vld20.32 {q4, q5}, [r1]
        vld21.32 {q4, q5}, [r1]!
        vmulh.S32 q7, q2, q2
        vmulh.S32 q1, q1, q1
        vld20.32 {q2, q3}, [r1]
        vhadd.S32 q7, q7, q1
        vmulh.S32 q4, q4, q4
        vld21.32 {q2, q3}, [r1]!
        vmulh.S32 q5, q5, q5
        lsr lr, lr, #2
        sub lr, lr, #1
.p2align 2
start:
                                        // Instructions:    24
                                        // Expected cycles: 24
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     0.94s
                                        // User time:     0.94s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vld20.32 {q0, q1}, [r1]         // e.............................
        vstrw.U32 q6, [r0], #16         // .*............................
        vld21.32 {q0, q1}, [r1]!        // ..e...........................
        vstrw.U32 q7, [r0], #16         // ...*..........................
        vmulh.S32 q7, q2, q2            // ....*.........................
        vhadd.S32 q5, q5, q4            // .....*........................
        vmulh.S32 q6, q1, q1            // ......e.......................
        vld20.32 {q1, q2}, [r1]         // .......e......................
        vmulh.S32 q3, q3, q3            // ........*.....................
        vld21.32 {q1, q2}, [r1]!        // .........e....................
        vmulh.S32 q4, q0, q0            // ..........e...................
        vstrw.U32 q5, [r0], #16         // ...........*..................
        vhadd.S32 q6, q6, q4            // ............e.................
        vld20.32 {q4, q5}, [r1]         // .............e................
        vhadd.S32 q3, q3, q7            // ..............*...............
        vld21.32 {q4, q5}, [r1]!        // ...............e..............
        vmulh.S32 q7, q2, q2            // ................e.............
        vstrw.U32 q3, [r0], #16         // .................*............
        vmulh.S32 q1, q1, q1            // ..................e...........
        vld20.32 {q2, q3}, [r1]         // ...................e..........
        vhadd.S32 q7, q7, q1            // ....................e.........
        vmulh.S32 q4, q4, q4            // .....................e........
        vld21.32 {q2, q3}, [r1]!        // ......................e.......
        vmulh.S32 q5, q5, q5            // .......................e......

                                         // ----------- cycle (expected) ------------>
                                         // 0                        25
                                         // |------------------------|----------------
        // vld20.32 {q4,q5}, [r1]        // e.......................~.................
        // vld21.32 {q4,q5}, [r1]!       // ..e.....................'.~...............
        // vmulh.s32 q2, q4, q4          // ..........e.............'.........~.......
        // vmulh.s32 q4, q5, q5          // ......e.................'.....~...........
        // vhadd.s32 q4, q4, q2          // ............e...........'...........~.....
        // vstrw.u32 q4, [r0] , #16      // .~......................'*................
        // vld20.32 {q4,q5}, [r1]        // .......e................'......~..........
        // vld21.32 {q4,q5}, [r1]!       // .........e..............'........~........
        // vmulh.s32 q2, q4, q4          // ..................e.....'.................
        // vmulh.s32 q4, q5, q5          // ................e.......'...............~.
        // vhadd.s32 q4, q4, q2          // ....................e...'.................
        // vstrw.u32 q4, [r0] , #16      // ...~....................'..*..............
        // vld20.32 {q4,q5}, [r1]        // .............e..........'............~....
        // vld21.32 {q4,q5}, [r1]!       // ...............e........'..............~..
        // vmulh.s32 q2, q4, q4          // .....................e..'.................
        // vmulh.s32 q4, q5, q5          // .......................e'.................
        // vhadd.s32 q4, q4, q2          // .....~..................'....*............
        // vstrw.u32 q4, [r0] , #16      // ...........~............'..........*......
        // vld20.32 {q4,q5}, [r1]        // ...................e....'.................
        // vld21.32 {q4,q5}, [r1]!       // ......................e.'.................
        // vmulh.s32 q2, q4, q4          // ....~...................'...*.............
        // vmulh.s32 q4, q5, q5          // ........~...............'.......*.........
        // vhadd.s32 q4, q4, q2          // ..............~.........'.............*...
        // vstrw.u32 q4, [r0] , #16      // .................~......'................*

        le lr, start
        vstrw.U32 q6, [r0], #16
        vstrw.U32 q7, [r0], #16
        vmulh.S32 q7, q2, q2
        vhadd.S32 q5, q5, q4
        vmulh.S32 q3, q3, q3
        vstrw.U32 q5, [r0], #16
        vhadd.S32 q3, q3, q7
        vstrw.U32 q3, [r0], #16
end:

        vpop {d0-d15}
        pop {r4-r12,lr}

        bx lr