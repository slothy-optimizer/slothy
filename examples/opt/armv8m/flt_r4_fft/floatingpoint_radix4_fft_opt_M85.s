        .syntax unified
        .type   floatingpoint_radix4_fft_opt_M85, %function
        .global floatingpoint_radix4_fft_opt_M85


        inA .req r0
        pW0 .req r1 // Use the same twiddle data for TESTING ONLY
        sz  .req r2

        inB .req r3
        inC .req r4
        inD .req r5

        pW1 .req r6
        pW2 .req r7
        pW3 .req r8

        // NOTE:
        // We deliberately leave some aliases undefined
        // SLOTHY will fill them in as part of a 'dry-run'
        // merely concretizing symbolic registers, but not
        // yet reordering.

        .text
        .align 4
floatingpoint_radix4_fft_opt_M85:
        push    {r4-r12,lr}
        vpush   {d0-d15}

        add     inB, inA, sz
        add     inC, inB, sz
        add     inD, inC, sz

        add     pW1, pW0, sz
        add     pW2, pW1, sz
        add     pW3, pW2, sz

        lsr     lr, sz, #4
        wls     lr, lr, end

.macro load_data
        vldrw.32   qA, [inA]
        vldrw.32   qB, [inB]
        vldrw.32   qC, [inC]
        vldrw.32   qD, [inD]
.endm

.macro load_twiddles
        vldrw.s32  qTw1, [pW1], #16
        vldrw.s32  qTw2, [pW2], #16
        vldrw.s32  qTw3, [pW3], #16
.endm

.macro store_data
        vstrw.32   qA, [inA], #16
        vstrw.32   qB, [inB], #16
        vstrw.32   qC, [inC], #16
        vstrw.32   qD, [inD], #16
.endm

.macro cmul_flt out, in0, in1
        vcmul.f32  \out, \in0, \in1, #0
        vcmla.f32  \out, \in0, \in1, #270
.endm

        vldrw.32 q6, [r3]                // *............
        // gap                           // .............
        vldrw.32 q0, [r5]                // .*...........
        vadd.f32 q5, q6, q0              // ....*........
        vldrw.32 q2, [r0]                // ...*.........
        vsub.f32 q6, q6, q0              // ..*..........
        vldrw.32 q3, [r4]                // .....*.......
        vadd.f32 q0, q2, q3              // .......*.....
        vldrw.s32 q7, [r6] , #16         // ........*....
        vadd.f32 q4, q0, q5              // ..........*..
        vstrw.u32 q4, [r0] , #16         // ...........*.
        vsub.f32 q0, q0, q5              // ............*
        vldrw.s32 q5, [r8] , #16         // .........*...
        vsub.f32 q2, q2, q3              // ......*......
        
        // original source code
        // vldrw.32 q2, [r3]             // *............ 
        // vldrw.32 q4, [r5]             // .*........... 
        // vsub.f32 q6, q2, q4           // ....*........ 
        // vldrw.32 q3, [r0]             // ...*......... 
        // vadd.f32 q4, q2, q4           // ..*.......... 
        // vldrw.32 q7, [r4]             // .....*....... 
        // vsub.f32 q2, q3, q7           // ............* 
        // vadd.f32 q3, q3, q7           // ......*...... 
        // vldrw.s32 q7, [r6] , #16      // .......*..... 
        // vldrw.s32 q5, [r8] , #16      // ...........*. 
        // vadd.f32 q0, q3, q4           // ........*.... 
        // vstrw.u32 q0, [r0] , #16      // .........*... 
        // vsub.f32 q0, q3, q4           // ..........*.. 
        
        sub lr, lr, #1
.p2align 2
flt_radix4_fft_loop_start:
        vcadd.f32 q4, q2, q6, #90          // ..............*..........
        vcmul.f32 q3, q7, q0, #0           // ...............*.........
        vcadd.f32 q1, q2, q6, #270         // .............*...........
        vcmul.f32 q6, q5, q4, #0           // ...................*.....
        vldrw.32 q2, [r3, #16]             // .e.......................
        vcmla.f32 q3, q7, q0, #270         // ................*........
        vldrw.s32 q0, [r7] , #16           // .....*...................
        vcmla.f32 q6, q5, q4, #270         // ....................*....
        vstrw.u32 q3, [r3] , #16           // ......................*..
        vldrw.32 q4, [r5, #16]             // ...e.....................
        vstrw.u32 q6, [r5] , #16           // ........................*
        vsub.f32 q6, q2, q4                // ..........e..............
        vldrw.32 q3, [r0]                  // e........................
        vadd.f32 q4, q2, q4                // ........e................
        vldrw.32 q7, [r4, #16]             // ..e......................
        vsub.f32 q2, q3, q7                // .........e...............
        vcmul.f32 q5, q0, q1, #0           // .................*.......
        vadd.f32 q3, q3, q7                // .......e.................
        vldrw.s32 q7, [r6] , #16           // ....e....................
        vcmla.f32 q5, q0, q1, #270         // ..................*......
        // gap                             // .........................
        vstrw.u32 q5, [r4] , #16           // .......................*.
        vldrw.s32 q5, [r8] , #16           // ......e..................
        vadd.f32 q0, q3, q4                // ...........e.............
        vstrw.u32 q0, [r0] , #16           // .....................e...
        vsub.f32 q0, q3, q4                // ............e............
        // gap                             // .........................
        
        // original source code
        // vldrw.32 qA, [r0]                    // ........e................................. 
        // vldrw.32 qB, [r3]                    // e......................................... 
        // vldrw.32 qC, [r4]                    // ..........e............................... 
        // vldrw.32 qD, [r5]                    // .....e.................................... 
        // vldrw.s32 qTw1, [r6] , #16           // ..............e........................... 
        // vldrw.s32 qTw2, [r7] , #16           // ...........................*.............. 
        // vldrw.s32 qTw3, [r8] , #16           // .................e........................ 
        // vadd.f32 qSm0, qA, qC                // .............e............................ 
        // vadd.f32 qSm1, qB, qD                // .........e................................ 
        // vsub.f32 qDf0, qA, qC                // ...........e.............................. 
        // vsub.f32 qDf1, qB, qD                // .......e.................................. 
        // vadd.f32 qA, qSm0, qSm1              // ..................e....................... 
        // vsub.f32 qBp, qSm0, qSm1             // ....................e..................... 
        // vcadd.f32 qCp, qDf0, qDf1, #270      // .......................*.................. 
        // vcadd.f32 qDp, qDf0, qDf1, #90       // .....................*.................... 
        // vcmul.f32 qB, qTw1, qBp, #0          // ......................*................... 
        // vcmla.f32 qB, qTw1, qBp, #270        // ..........................*............... 
        // vcmul.f32 qC, qTw2, qCp, #0          // .....................................*.... 
        // vcmla.f32 qC, qTw2, qCp, #270        // ........................................*. 
        // vcmul.f32 qD, qTw3, qDp, #0          // ........................*................. 
        // vcmla.f32 qD, qTw3, qDp, #270        // ............................*............. 
        // vstrw.u32 qA, [r0] , #16             // ...................e...................... 
        // vstrw.u32 qB, [r3] , #16             // .............................*............ 
        // vstrw.u32 qC, [r4] , #16             // .........................................* 
        // vstrw.u32 qD, [r5] , #16             // ...............................*.......... 
        
        le lr, flt_radix4_fft_loop_start
        vcadd.f32 q4, q2, q6, #90          // *...........
        vcmul.f32 q1, q7, q0, #0           // .*..........
        vcadd.f32 q3, q2, q6, #270         // ..*.........
        vcmul.f32 q6, q5, q4, #0           // ...*........
        vldrw.s32 q2, [r7] , #16           // .....*......
        vcmla.f32 q1, q7, q0, #270         // ....*.......
        // gap                             // ............
        vstrw.u32 q1, [r3] , #16           // .......*....
        // gap                             // ............
        vcmul.f32 q1, q2, q3, #0           // .........*..
        // gap                             // ............
        vcmla.f32 q6, q5, q4, #270         // ......*.....
        // gap                             // ............
        vcmla.f32 q1, q2, q3, #270         // ..........*.
        vstrw.u32 q6, [r5] , #16           // ........*...
        // gap                             // ............
        vstrw.u32 q1, [r4] , #16           // ...........*
        
        // original source code
        // vcadd.f32 q4, q2, q6, #90       // *........... 
        // vcmul.f32 q3, q7, q0, #0        // .*.......... 
        // vcadd.f32 q1, q2, q6, #270      // ..*......... 
        // vcmul.f32 q6, q5, q4, #0        // ...*........ 
        // vcmla.f32 q3, q7, q0, #270      // .....*...... 
        // vldrw.s32 q0, [r7] , #16        // ....*....... 
        // vcmla.f32 q6, q5, q4, #270      // ........*... 
        // vstrw.u32 q3, [r3] , #16        // ......*..... 
        // vstrw.u32 q6, [r5] , #16        // ..........*. 
        // vcmul.f32 q5, q0, q1, #0        // .......*.... 
        // vcmla.f32 q5, q0, q1, #270      // .........*.. 
        // vstrw.u32 q5, [r4] , #16        // ...........* 
        

end:
        vpop    {d0-d15}
        pop     {r4-r12,lr}
        bx      lr