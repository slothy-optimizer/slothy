        .syntax unified
        .type   floatingpoint_radix4_fft_opt_M55, %function
        .global floatingpoint_radix4_fft_opt_M55


        inA .req r0
        pW0 .req r1 // Use the same twiddle data for TESTING ONLY
        sz  .req r2

        inB .req r3
        inC .req r4
        inD .req r5

        pW1 .req r6
        pW2 .req r7
        pW3 .req r8

        // NOTE:
        // We deliberately leave some aliases undefined
        // SLOTHY will fill them in as part of a 'dry-run'
        // merely concretizing symbolic registers, but not
        // yet reordering.

        .text
        .align 4
floatingpoint_radix4_fft_opt_M55:
        push    {r4-r12,lr}
        vpush   {d0-d15}

        add     inB, inA, sz
        add     inC, inB, sz
        add     inD, inC, sz

        add     pW1, pW0, sz
        add     pW2, pW1, sz
        add     pW3, pW2, sz

        lsr     lr, sz, #4
        wls     lr, lr, end

.macro load_data
        vldrw.32   qA, [inA]
        vldrw.32   qB, [inB]
        vldrw.32   qC, [inC]
        vldrw.32   qD, [inD]
.endm

.macro load_twiddles
        vldrw.s32  qTw1, [pW1], #16
        vldrw.s32  qTw2, [pW2], #16
        vldrw.s32  qTw3, [pW3], #16
.endm

.macro store_data
        vstrw.32   qA, [inA], #16
        vstrw.32   qB, [inB], #16
        vstrw.32   qC, [inC], #16
        vstrw.32   qD, [inD], #16
.endm

.macro cmul_flt out, in0, in1
        vcmul.f32  \out, \in0, \in1, #0
        vcmla.f32  \out, \in0, \in1, #270
.endm

        vldrw.32 q2, [r5]          // *..
        // gap                     // ...
        vldrw.32 q1, [r0]          // .*.
        // gap                     // ...
        vldrw.32 q0, [r4]          // ..*
        
        // original source code
        // vldrw.32 q2, [r5]       // *.. 
        // vldrw.32 q1, [r0]       // .*. 
        // vldrw.32 q0, [r4]       // ..* 
        
        sub lr, lr, #1
.p2align 2
flt_radix4_fft_loop_start:
        vadd.f32 q3, q1, q0                // .......*.................
        // gap                             // .........................
        vsub.f32 q5, q1, q0                // .........*...............
        vldrw.32 q1, [r3]                  // .*.......................
        vadd.f32 q4, q1, q2                // ........*................
        vldrw.s32 q7, [r7] , #16           // .....*...................
        vadd.f32 q0, q3, q4                // ...........*.............
        vstrw.u32 q0, [r0] , #16           // .....................*...
        vsub.f32 q4, q3, q4                // ............*............
        // gap                             // .........................
        vsub.f32 q1, q1, q2                // ..........*..............
        vldrw.s32 q0, [r6] , #16           // ....*....................
        vcmul.f32 q3, q0, q4, #0           // ...............*.........
        vldrw.32 q2, [r5, #16]             // ...e.....................
        vcmla.f32 q3, q0, q4, #270         // ................*........
        vstrw.u32 q3, [r3] , #16           // ......................*..
        vcadd.f32 q0, q5, q1, #90          // ..............*..........
        // gap                             // .........................
        vcadd.f32 q6, q5, q1, #270         // .............*...........
        vldrw.s32 q3, [r8] , #16           // ......*..................
        vcmul.f32 q4, q3, q0, #0           // ...................*.....
        vldrw.32 q1, [r0]                  // e........................
        vcmla.f32 q4, q3, q0, #270         // ....................*....
        vldrw.32 q0, [r4, #16]             // ..e......................
        vcmul.f32 q5, q7, q6, #0           // .................*.......
        vstrw.u32 q4, [r5] , #16           // ........................*
        vcmla.f32 q5, q7, q6, #270         // ..................*......
        vstrw.u32 q5, [r4] , #16           // .......................*.
        
        // original source code
        // vldrw.32 qA, [r0]                    // .......e............................... 
        // vldrw.32 qB, [r3]                    // ................*...................... 
        // vldrw.32 qC, [r4]                    // .........e............................. 
        // vldrw.32 qD, [r5]                    // e...................................... 
        // vldrw.s32 qTw1, [r6] , #16           // .......................*............... 
        // vldrw.s32 qTw2, [r7] , #16           // ..................*.................... 
        // vldrw.s32 qTw3, [r8] , #16           // ..............................*........ 
        // vadd.f32 qSm0, qA, qC                // ..............*........................ 
        // vadd.f32 qSm1, qB, qD                // .................*..................... 
        // vsub.f32 qDf0, qA, qC                // ...............*....................... 
        // vsub.f32 qDf1, qB, qD                // ......................*................ 
        // vadd.f32 qA, qSm0, qSm1              // ...................*................... 
        // vsub.f32 qBp, qSm0, qSm1             // .....................*................. 
        // vcadd.f32 qCp, qDf0, qDf1, #270      // .............................*......... 
        // vcadd.f32 qDp, qDf0, qDf1, #90       // ............................*.......... 
        // vcmul.f32 qB, qTw1, qBp, #0          // ........................*.............. 
        // vcmla.f32 qB, qTw1, qBp, #270        // ..........................*............ 
        // vcmul.f32 qC, qTw2, qCp, #0          // ...................................*... 
        // vcmla.f32 qC, qTw2, qCp, #270        // .....................................*. 
        // vcmul.f32 qD, qTw3, qDp, #0          // ...............................*....... 
        // vcmla.f32 qD, qTw3, qDp, #270        // .................................*..... 
        // vstrw.u32 qA, [r0] , #16             // ....................*.................. 
        // vstrw.u32 qB, [r3] , #16             // ...........................*........... 
        // vstrw.u32 qC, [r4] , #16             // ......................................* 
        // vstrw.u32 qD, [r5] , #16             // ....................................*.. 
        
        le lr, flt_radix4_fft_loop_start
        vadd.f32 q4, q1, q0                // *.....................
        vldrw.s32 q5, [r7] , #16           // ....*.................
        vsub.f32 q1, q1, q0                // .*....................
        vldrw.32 q3, [r3]                  // ..*...................
        vadd.f32 q6, q3, q2                // ...*..................
        vldrw.s32 q0, [r8] , #16           // ...............*......
        vsub.f32 q2, q3, q2                // ........*.............
        vldrw.s32 q3, [r6] , #16           // .........*............
        vadd.f32 q7, q4, q6                // .....*................
        vstrw.u32 q7, [r0] , #16           // ......*...............
        vsub.f32 q4, q4, q6                // .......*..............
        // gap                             // ......................
        vcmul.f32 q6, q3, q4, #0           // ..........*...........
        // gap                             // ......................
        vcmla.f32 q6, q3, q4, #270         // ...........*..........
        vstrw.u32 q6, [r3] , #16           // ............*.........
        vcadd.f32 q4, q1, q2, #90          // .............*........
        // gap                             // ......................
        vcmul.f32 q3, q0, q4, #0           // ................*.....
        // gap                             // ......................
        vcmla.f32 q3, q0, q4, #270         // .................*....
        vstrw.u32 q3, [r5] , #16           // ...................*..
        vcadd.f32 q4, q1, q2, #270         // ..............*.......
        // gap                             // ......................
        vcmul.f32 q2, q5, q4, #0           // ..................*...
        // gap                             // ......................
        vcmla.f32 q2, q5, q4, #270         // ....................*.
        vstrw.u32 q2, [r4] , #16           // .....................*
        
        // original source code
        // vadd.f32 q3, q1, q0             // *..................... 
        // vsub.f32 q5, q1, q0             // ..*................... 
        // vldrw.32 q1, [r3]               // ...*.................. 
        // vadd.f32 q4, q1, q2             // ....*................. 
        // vldrw.s32 q7, [r7] , #16        // .*.................... 
        // vadd.f32 q0, q3, q4             // ........*............. 
        // vstrw.u32 q0, [r0] , #16        // .........*............ 
        // vsub.f32 q4, q3, q4             // ..........*........... 
        // vsub.f32 q1, q1, q2             // ......*............... 
        // vldrw.s32 q0, [r6] , #16        // .......*.............. 
        // vcmul.f32 q3, q0, q4, #0        // ...........*.......... 
        // vcmla.f32 q3, q0, q4, #270      // ............*......... 
        // vstrw.u32 q3, [r3] , #16        // .............*........ 
        // vcadd.f32 q0, q5, q1, #90       // ..............*....... 
        // vcadd.f32 q6, q5, q1, #270      // ..................*... 
        // vldrw.s32 q3, [r8] , #16        // .....*................ 
        // vcmul.f32 q4, q3, q0, #0        // ...............*...... 
        // vcmla.f32 q4, q3, q0, #270      // ................*..... 
        // vcmul.f32 q5, q7, q6, #0        // ...................*.. 
        // vstrw.u32 q4, [r5] , #16        // .................*.... 
        // vcmla.f32 q5, q7, q6, #270      // ....................*. 
        // vstrw.u32 q5, [r4] , #16        // .....................* 
        

end:
        vpop    {d0-d15}
        pop     {r4-r12,lr}
        bx      lr