
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots_inv:
#include "intt_kyber_1_23_45_67_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s16       \dst,  \src, \const
        vqrdmulh.s16   \src,  \src, \const_twisted
        vmla.s16       \dst,  \src, modulus
.endm

.macro gs_butterfly a, b, root, root_twisted
        vsub.u16       tmp, \a,  \b
        vadd.u16       \a,  \a,  \b
        mulmod         \b,  tmp, \root, \root_twisted
.endm

.macro load_first_root root0, root0_twisted
        ldrd root0, root0_twisted, [root_ptr], #+8
.endm

.macro load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted
        ldrd root0, root0_twisted, [root_ptr], #+24
        ldrd root1, root1_twisted, [root_ptr, #(-16)]
        ldrd root2, root2_twisted, [root_ptr, #(-8)]
.endm

.align 4
roots_addr: .word roots_inv
.syntax unified
.type intt_kyber_1_23_45_67_opt_m55, %function
.global intt_kyber_1_23_45_67_opt_m55
intt_kyber_1_23_45_67_opt_m55:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus     .req r12
        root_ptr    .req r11

        .equ modulus_const, -3329
        movw modulus, #:lower16:modulus_const
        ldr  root_ptr, roots_addr

        in .req r0

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6

        tmp .req q4

        // Layers 6,7

        mov lr, #8
                                        // Instructions:    3
                                        // Expected cycles: 5
                                        // Expected IPC:    0.60
                                        //
                                        // Wall time:     0.00s
                                        // User time:     0.00s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vldrw.U32 q2, [r11, #80]        // *.............................
        vldrw.U32 q3, [r0, #48]         // ..*...........................
        vldrw.U32 q5, [r0, #32]         // ....*.........................

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vldrw.U32 q5, [r0, #32]       // ....*..........................
        // vldrw.U32 q3, [r0, #48]       // ..*............................
        // vldrw.U32 q2, [r11, #80]      // *..............................

        sub lr, lr, #1
.p2align 2
layer67_loop:
                                            // Instructions:    34
                                            // Expected cycles: 34
                                            // Expected IPC:    1.00
                                            //
                                            // Wall time:     3.19s
                                            // User time:     3.19s
                                            //
                                            // ------- cycle (expected) -------->
                                            // 0                        25
                                            // |------------------------|--------
        vsub.U16 q6, q5, q3                 // *.................................
        vqrdmulh.S16 q4, q6, q2             // .*................................
        vldrw.U32 q7, [r11, #64]            // ..*...............................
        vadd.U16 q1, q5, q3                 // ...*..............................
        vldrw.U32 q2, [r0, #16]             // ....*.............................
        vmul.S16 q7, q6, q7                 // .....*............................
        vldrw.U32 q3, [r0]                  // ......*...........................
        vadd.U16 q5, q3, q2                 // .......*..........................
        vldrw.U32 q0, [r11, #48]            // ........*.........................
        vsub.U16 q6, q3, q2                 // .........*........................
        vldrw.U32 q2, [r11, #32]            // ..........*.......................
        vmul.S16 q2, q6, q2                 // ...........*......................
        vadd.U16 q3, q5, q1                 // ............*.....................
        vqrdmulh.S16 q6, q6, q0             // .............*....................
        vsub.U16 q1, q5, q1                 // ..............*...................
        vmla.S16 q2, q6, r12                // ...............*..................
        vldrw.U32 q0, [r11, #16]            // ................*.................
        vmla.S16 q7, q4, r12                // .................*................
        vldrw.U32 q4, [r11], #(3*32)        // ..................*...............
        vqrdmulh.S16 q6, q1, q0             // ...................*..............
        vldrw.U32 q5, [r0, #96]             // ....................e.............
        vmul.S16 q1, q1, q4                 // .....................*............
        vstrw.U32 q3, [r0], #64             // ......................*...........
        vadd.U16 q3, q2, q7                 // .......................*..........
        vstrw.U32 q3, [r0, #-48]            // ........................*.........
        vsub.U16 q2, q2, q7                 // .........................*........
        vmul.S16 q7, q2, q4                 // ..........................*.......
        vldrw.U32 q3, [r0, #48]             // ...........................e......
        vqrdmulh.S16 q4, q2, q0             // ............................*.....
        vldrw.U32 q2, [r11, #80]            // .............................e....
        vmla.S16 q7, q4, r12                // ..............................*...
        vstrw.U32 q7, [r0, #-16]            // ...............................*..
        vmla.S16 q1, q6, r12                // ................................*.
        vstrw.U32 q1, [r0, #-32]            // .................................*

                                                     // -------------- cycle (expected) --------------->
                                                     // 0                        25
                                                     // |------------------------|----------------------
        // vldrw.u32 q0, [r0]                        // ..............'.....*...........................
        // vldrw.u32 q1, [r0, #(4*4*1)]              // ..............'...*.............................
        // vldrw.u32 q2, [r0, #(4*4*2)]              // e.............'...................~.............
        // vldrw.u32 q3, [r0, #(4*4*3)]              // .......e......'..........................~......
        // vldrw.u32 q5,         [r11, #(32)]        // ..............'.........*.......................
        // vldrw.u32 q6, [r11, #(32+16)]             // ..............'.......*.........................
        // vsub.u16       q4, q0,  q1                // ..............'........*........................
        // vadd.u16       q0,  q0,  q1               // ..............'......*..........................
        // vmul.s16       q1,  q4, q5                // ..............'..........*......................
        // vqrdmulh.s16   q4,  q4, q6                // ..............'............*....................
        // vmla.s16       q1,  q4, r12               // ..............'..............*..................
        // vldrw.u32 q5,         [r11, #(64)]        // ..............'.*...............................
        // vldrw.u32 q6, [r11, #(64+16)]             // .........e....'............................~....
        // vsub.u16       q4, q2,  q3                // ..............*.................................
        // vadd.u16       q2,  q2,  q3               // ..............'..*..............................
        // vmul.s16       q3,  q4, q5                // ..............'....*............................
        // vqrdmulh.s16   q4,  q4, q6                // ..............'*................................
        // vmla.s16       q3,  q4, r12               // ..............'................*................
        // vldrw.u32 q5,         [r11], #(3*32)      // ..............'.................*...............
        // vldrw.u32 q6, [r11, #(16 - 3*32)]         // ..............'...............*.................
        // vsub.u16       q4, q0,  q2                // ..............'.............*...................
        // vadd.u16       q0,  q0,  q2               // ..............'...........*.....................
        // vmul.s16       q2,  q4, q5                // .~............'....................*............
        // vqrdmulh.s16   q4,  q4, q6                // ..............'..................*..............
        // vmla.s16       q2,  q4, r12               // ............~.'...............................*.
        // vsub.u16       q4, q1,  q3                // .....~........'........................*........
        // vadd.u16       q1,  q1,  q3               // ...~..........'......................*..........
        // vmul.s16       q3,  q4, q5                // ......~.......'.........................*.......
        // vqrdmulh.s16   q4,  q4, q6                // ........~.....'...........................*.....
        // vmla.s16       q3,  q4, r12               // ..........~...'.............................*...
        // vstrw.u32 q0, [r0], #64                   // ..~...........'.....................*...........
        // vstrw.u32 q1, [r0, #(4*4*1 - 64)]         // ....~.........'.......................*.........
        // vstrw.u32 q2, [r0, #(4*4*2 - 64)]         // .............~'................................*
        // vstrw.u32 q3, [r0, #(4*4*3 - 64)]         // ...........~..'..............................*..

        le lr, layer67_loop
                                            // Instructions:    31
                                            // Expected cycles: 31
                                            // Expected IPC:    1.00
                                            //
                                            // Wall time:     0.45s
                                            // User time:     0.45s
                                            //
                                            // ------ cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|-----
        vldrw.U32 q0, [r11, #64]            // *..............................
        vsub.U16 q7, q5, q3                 // .*.............................
        vqrdmulh.S16 q1, q7, q2             // ..*............................
        vldrw.U32 q6, [r0, #16]             // ...*...........................
        vadd.U16 q2, q5, q3                 // ....*..........................
        vldrw.U32 q4, [r0]                  // .....*.........................
        vadd.U16 q3, q4, q6                 // ......*........................
        vmul.S16 q7, q7, q0                 // .......*.......................
        vsub.U16 q6, q4, q6                 // ........*......................
        vldrw.U32 q4, [r11, #32]            // .........*.....................
        vmul.S16 q0, q6, q4                 // ..........*....................
        vldrw.U32 q4, [r11, #48]            // ...........*...................
        vqrdmulh.S16 q4, q6, q4             // ............*..................
        vadd.U16 q5, q3, q2                 // .............*.................
        vmla.S16 q0, q4, r12                // ..............*................
        vsub.U16 q4, q3, q2                 // ...............*...............
        vmla.S16 q7, q1, r12                // ................*..............
        vldrw.U32 q1, [r11, #16]            // .................*.............
        vqrdmulh.S16 q2, q4, q1             // ..................*............
        vldrw.U32 q6, [r11], #(3*32)        // ...................*...........
        vmul.S16 q3, q4, q6                 // ....................*..........
        vstrw.U32 q5, [r0], #64             // .....................*.........
        vsub.U16 q4, q0, q7                 // ......................*........
        vmul.S16 q6, q4, q6                 // .......................*.......
        vadd.U16 q5, q0, q7                 // ........................*......
        vqrdmulh.S16 q4, q4, q1             // .........................*.....
        vstrw.U32 q5, [r0, #-48]            // ..........................*....
        vmla.S16 q3, q2, r12                // ...........................*...
        vstrw.U32 q3, [r0, #-32]            // ............................*..
        vmla.S16 q6, q4, r12                // .............................*.
        vstrw.U32 q6, [r0, #-16]            // ..............................*

                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        // vsub.U16 q6, q5, q3               // .*.............................
        // vqrdmulh.S16 q4, q6, q2           // ..*............................
        // vldrw.U32 q7, [r11, #64]          // *..............................
        // vadd.U16 q1, q5, q3               // ....*..........................
        // vldrw.U32 q2, [r0, #16]           // ...*...........................
        // vmul.S16 q7, q6, q7               // .......*.......................
        // vldrw.U32 q3, [r0]                // .....*.........................
        // vadd.U16 q5, q3, q2               // ......*........................
        // vldrw.U32 q0, [r11, #48]          // ...........*...................
        // vsub.U16 q6, q3, q2               // ........*......................
        // vldrw.U32 q2, [r11, #32]          // .........*.....................
        // vmul.S16 q2, q6, q2               // ..........*....................
        // vadd.U16 q3, q5, q1               // .............*.................
        // vqrdmulh.S16 q6, q6, q0           // ............*..................
        // vsub.U16 q1, q5, q1               // ...............*...............
        // vmla.S16 q2, q6, r12              // ..............*................
        // vldrw.U32 q0, [r11, #16]          // .................*.............
        // vmla.S16 q7, q4, r12              // ................*..............
        // vldrw.U32 q4, [r11], #(3*32)      // ...................*...........
        // vqrdmulh.S16 q6, q1, q0           // ..................*............
        // vmul.S16 q1, q1, q4               // ....................*..........
        // vstrw.U32 q3, [r0], #64           // .....................*.........
        // vadd.U16 q3, q2, q7               // ........................*......
        // vstrw.U32 q3, [r0, #-48]          // ..........................*....
        // vsub.U16 q2, q2, q7               // ......................*........
        // vmul.S16 q7, q2, q4               // .......................*.......
        // vqrdmulh.S16 q4, q2, q0           // .........................*.....
        // vmla.S16 q7, q4, r12              // .............................*.
        // vstrw.U32 q7, [r0, #-16]          // ..............................*
        // vmla.S16 q1, q6, r12              // ...........................*...
        // vstrw.U32 q1, [r0, #-32]          // ............................*..


        sub in, in, #(4*128)

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        .equ const_barrett, 10079
        .equ barrett_shift, 10

        // TEMPORARY: Barrett reduction

        // This is grossly inefficient and largely unnecessary, but it's just outside
        // the scope of our work to optimize this: We only want to demonstrate the
        // ability of Helight to optimize the core loops.
        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        mov lr, #32
1:
        vldrh.u16 data0, [in]
        vqdmulh.s16 tmp, data0, barrett_const
        vrshr.s16 tmp, tmp, barrett_shift
        vmla.s16 data0, tmp, modulus
        vstrh.u16 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*128)
        .unreq barrett_const

        // Layers 4,5

        mov lr, #8
                                                 // Instructions:    5
                                                 // Expected cycles: 8
                                                 // Expected IPC:    0.62
                                                 //
                                                 // Wall time:     0.01s
                                                 // User time:     0.01s
                                                 //
                                                 // ----- cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|----
        vld40.U32 {q0, q1, q2, q3}, [r0]         // *.............................
        vld41.U32 {q0, q1, q2, q3}, [r0]         // ..*...........................
        vld42.U32 {q0, q1, q2, q3}, [r0]         // ....*.........................
        ldrd r4, r6, [r11, #16]                  // ......*.......................
        vld43.U32 {q0, q1, q2, q3}, [r0]!        // .......*......................

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vld40.U32 {q0, q1, q2, q3}, [r0]       // *..............................
        // vld41.U32 {q0, q1, q2, q3}, [r0]       // ..*............................
        // vld42.U32 {q0, q1, q2, q3}, [r0]       // ....*..........................
        // ldrd r4, r6, [r11, #16]                // ......*........................
        // vld43.U32 {q0, q1, q2, q3}, [r0]!      // .......*.......................

        sub lr, lr, #1
.p2align 2
layer45_loop:
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     2.38s
                                                 // User time:     2.38s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vsub.U16 q4, q2, q3                      // *..............................
        vmul.S16 q5, q4, r4                      // .*.............................
        vadd.U16 q3, q2, q3                      // ..*............................
        ldrd r8, r3, [r11], #+24                 // ...*...........................
        vadd.U16 q7, q0, q1                      // ....*..........................
        ldrd r7, r10, [r11, #-16]                // .....*.........................
        vsub.U16 q2, q7, q3                      // ......*........................
        vmul.S16 q6, q2, r8                      // .......*.......................
        vadd.U16 q3, q7, q3                      // ........*......................
        vqrdmulh.S16 q7, q2, r3                  // .........*.....................
        vstrw.U32 q3, [r0, #-64]                 // ..........*....................
        vmla.S16 q6, q7, r12                     // ...........*...................
        vstrw.U32 q6, [r0, #-32]                 // ............*..................
        vqrdmulh.S16 q7, q4, r6                  // .............*.................
        vsub.U16 q6, q0, q1                      // ..............*................
        vmla.S16 q5, q7, r12                     // ...............*...............
        vld40.U32 {q0, q1, q2, q3}, [r0]         // ................e..............
        vmul.S16 q7, q6, r7                      // .................*.............
        vld41.U32 {q0, q1, q2, q3}, [r0]         // ..................e............
        vqrdmulh.S16 q6, q6, r10                 // ...................*...........
        vld42.U32 {q0, q1, q2, q3}, [r0]         // ....................e..........
        vmla.S16 q7, q6, r12                     // .....................*.........
        ldrd r4, r6, [r11, #16]                  // ......................e........
        vsub.U16 q6, q7, q5                      // .......................*.......
        vld43.U32 {q0, q1, q2, q3}, [r0]!        // ........................e......
        vqrdmulh.S16 q4, q6, r3                  // .........................*.....
        vadd.U16 q5, q7, q5                      // ..........................*....
        vmul.S16 q6, q6, r8                      // ...........................*...
        vstrw.U32 q5, [r0, #-112]                // ............................*..
        vmla.S16 q6, q4, r12                     // .............................*.
        vstrw.U32 q6, [r0, #-80]                 // ..............................*

                                                  // ------------- cycle (expected) -------------->
                                                  // 0                        25
                                                  // |------------------------|--------------------
        // ldrd r2, r3, [r11], #+24               // ...............'..*...........................
        // ldrd r4, r5, [r11, #(-16)]             // ...............'....*.........................
        // ldrd r6, r7, [r11, #(-8)]              // ......e........'.....................~........
        // vld40.u32 {q0, q1, q2, q3}, [r0]       // e..............'...............~..............
        // vld41.u32 {q0, q1, q2, q3}, [r0]       // ..e............'.................~............
        // vld42.u32 {q0, q1, q2, q3}, [r0]       // ....e..........'...................~..........
        // vld43.u32 {q0, q1, q2, q3}, [r0]!      // ........e......'.......................~......
        // vsub.u16       q4, q0,  q1             // ...............'.............*................
        // vadd.u16       q0,  q0,  q1            // ...............'...*..........................
        // vmul.s16       q1,  q4, r4             // .~.............'................*.............
        // vqrdmulh.s16   q4,  q4, r5             // ...~...........'..................*...........
        // vmla.s16       q1,  q4, r12            // .....~.........'....................*.........
        // vsub.u16       q4, q2,  q3             // ...............*..............................
        // vadd.u16       q2,  q2,  q3            // ...............'.*............................
        // vmul.s16       q3,  q4, r6             // ...............'*.............................
        // vqrdmulh.s16   q4,  q4, r7             // ...............'............*.................
        // vmla.s16       q3,  q4, r12            // ...............'..............*...............
        // vsub.u16       q4, q0,  q2             // ...............'.....*........................
        // vadd.u16       q0,  q0,  q2            // ...............'.......*......................
        // vmul.s16       q2,  q4, r2             // ...............'......*.......................
        // vqrdmulh.s16   q4,  q4, r3             // ...............'........*.....................
        // vmla.s16       q2,  q4, r12            // ...............'..........*...................
        // vsub.u16       q4, q1,  q3             // .......~.......'......................*.......
        // vadd.u16       q1,  q1,  q3            // ..........~....'.........................*....
        // vmul.s16       q3,  q4, r2             // ...........~...'..........................*...
        // vqrdmulh.s16   q4,  q4, r3             // .........~.....'........................*.....
        // vmla.s16       q3,  q4, r12            // .............~.'............................*.
        // vstrw.u32 q0, [r0, #(4*4*0 - 64)]      // ...............'.........*....................
        // vstrw.u32 q1, [r0, #(4*4*1 - 64)]      // ............~..'...........................*..
        // vstrw.u32 q2, [r0, #(4*4*2 - 64)]      // ...............'...........*..................
        // vstrw.u32 q3, [r0, #(4*4*3 - 64)]      // ..............~'.............................*

        le lr, layer45_loop
                                         // Instructions:    26
                                         // Expected cycles: 26
                                         // Expected IPC:    1.00
                                         //
                                         // Wall time:     0.23s
                                         // User time:     0.23s
                                         //
                                         // ----- cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|----
        vsub.U16 q4, q2, q3              // *.............................
        vqrdmulh.S16 q7, q4, r6          // .*............................
        vadd.U16 q6, q0, q1              // ..*...........................
        vmul.S16 q5, q4, r4              // ...*..........................
        ldrd r4, r6, [r11, #8]           // ....*.........................
        vsub.U16 q1, q0, q1              // .....*........................
        vmla.S16 q5, q7, r12             // ......*.......................
        vadd.U16 q4, q2, q3              // .......*......................
        vmul.S16 q2, q1, r4              // ........*.....................
        vsub.U16 q0, q6, q4              // .........*....................
        vqrdmulh.S16 q7, q1, r6          // ..........*...................
        ldrd r11, r6, [r11], #+24        // ...........*..................
        vmla.S16 q2, q7, r12             // ............*.................
        vadd.U16 q7, q6, q4              // .............*................
        vqrdmulh.S16 q4, q0, r6          // ..............*...............
        vsub.U16 q6, q2, q5              // ...............*..............
        vmul.S16 q1, q6, r11             // ................*.............
        vadd.U16 q5, q2, q5              // .................*............
        vqrdmulh.S16 q6, q6, r6          // ..................*...........
        vstrw.U32 q7, [r0, #-64]         // ...................*..........
        vmla.S16 q1, q6, r12             // ....................*.........
        vstrw.U32 q1, [r0, #-16]         // .....................*........
        vmul.S16 q6, q0, r11             // ......................*.......
        vstrw.U32 q5, [r0, #-48]         // .......................*......
        vmla.S16 q6, q4, r12             // ........................*.....
        vstrw.U32 q6, [r0, #-32]         // .........................*....

                                          // ------ cycle (expected) ------>
                                          // 0                        25
                                          // |------------------------|-----
        // vsub.U16 q4, q2, q3            // *..............................
        // vmul.S16 q5, q4, r4            // ...*...........................
        // vadd.U16 q3, q2, q3            // .......*.......................
        // ldrd r8, r3, [r11], #+24       // ...........*...................
        // vadd.U16 q7, q0, q1            // ..*............................
        // ldrd r7, r10, [r11, #-16]      // ....*..........................
        // vsub.U16 q2, q7, q3            // .........*.....................
        // vmul.S16 q6, q2, r8            // ......................*........
        // vadd.U16 q3, q7, q3            // .............*.................
        // vqrdmulh.S16 q7, q2, r3        // ..............*................
        // vstrw.U32 q3, [r0, #-64]       // ...................*...........
        // vmla.S16 q6, q7, r12           // ........................*......
        // vstrw.U32 q6, [r0, #-32]       // .........................*.....
        // vqrdmulh.S16 q7, q4, r6        // .*.............................
        // vsub.U16 q6, q0, q1            // .....*.........................
        // vmla.S16 q5, q7, r12           // ......*........................
        // vmul.S16 q7, q6, r7            // ........*......................
        // vqrdmulh.S16 q6, q6, r10       // ..........*....................
        // vmla.S16 q7, q6, r12           // ............*..................
        // vsub.U16 q6, q7, q5            // ...............*...............
        // vqrdmulh.S16 q4, q6, r3        // ..................*............
        // vadd.U16 q5, q7, q5            // .................*.............
        // vmul.S16 q6, q6, r8            // ................*..............
        // vstrw.U32 q5, [r0, #-48]       // .......................*.......
        // vmla.S16 q6, q4, r12           // ....................*..........
        // vstrw.U32 q6, [r0, #-16]       // .....................*.........


        sub in, in, #(4*128)

        // TEMPORARY: Barrett reduction

        // This is grossly inefficient and largely unnecessary, but it's just outside
        // the scope of our work to optimize this: We only want to demonstrate the
        // ability of Helight to optimize the core loops.

        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        mov lr, #32
1:
        vldrh.u16 data0, [in]
        vqdmulh.s16 tmp, data0, barrett_const
        vrshr.s16 tmp, tmp, barrett_shift
        vmla.s16 data0, tmp, modulus
        vstrh.u16 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*128)
        .unreq barrett_const

        // Layers 2,3

        count .req r1
        mov count, #2
out_start:
        load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted

        mov lr, #4
                                        // Instructions:    2
                                        // Expected cycles: 3
                                        // Expected IPC:    0.67
                                        //
                                        // Wall time:     0.00s
                                        // User time:     0.00s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vldrw.U32 q5, [r0, #192]        // *.............................
        vldrw.U32 q6, [r0, #128]        // ..*...........................

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vldrw.U32 q6, [r0, #128]      // ..*............................
        // vldrw.U32 q5, [r0, #192]      // *..............................

        sub lr, lr, #1
.p2align 2
layer23_loop:
                                         // Instructions:    28
                                         // Expected cycles: 28
                                         // Expected IPC:    1.00
                                         //
                                         // Wall time:     1.26s
                                         // User time:     1.26s
                                         //
                                         // ----- cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|----
        vsub.U16 q3, q6, q5              // *.............................
        vmul.S16 q7, q3, r6              // .*............................
        vldrw.U32 q1, [r0]               // ..*...........................
        vadd.U16 q5, q6, q5              // ...*..........................
        vqrdmulh.S16 q3, q3, r7          // ....*.........................
        vldrw.U32 q6, [r0, #64]          // .....*........................
        vsub.U16 q2, q1, q6              // ......*.......................
        vmla.S16 q7, q3, r12             // .......*......................
        vadd.U16 q3, q1, q6              // ........*.....................
        vqrdmulh.S16 q4, q2, r5          // .........*....................
        vsub.U16 q0, q3, q5              // ..........*...................
        vmul.S16 q1, q0, r2              // ...........*..................
        vldrw.U32 q6, [r0, #144]         // ............e.................
        vmul.S16 q2, q2, r4              // .............*................
        vadd.U16 q3, q3, q5              // ..............*...............
        vmla.S16 q2, q4, r12             // ...............*..............
        vstrw.U32 q3, [r0], #(16)        // ................*.............
        vqrdmulh.S16 q0, q0, r3          // .................*............
        vsub.U16 q3, q2, q7              // ..................*...........
        vldrw.U32 q5, [r0, #192]         // ...................e..........
        vmla.S16 q1, q0, r12             // ....................*.........
        vadd.U16 q0, q2, q7              // .....................*........
        vmul.S16 q7, q3, r2              // ......................*.......
        vstrw.U32 q0, [r0, #48]          // .......................*......
        vqrdmulh.S16 q0, q3, r3          // ........................*.....
        vstrw.U32 q1, [r0, #112]         // .........................*....
        vmla.S16 q7, q0, r12             // ..........................*...
        vstrw.U32 q7, [r0, #176]         // ...........................*..

                                                   // ------------ cycle (expected) ------------->
                                                   // 0                        25
                                                   // |------------------------|------------------
        // vldrw.u32 q0, [r0]                      // ................'.*.........................
        // vldrw.u32 q1, [r0, #(4*1*16)]           // ................'....*......................
        // vldrw.u32 q2, [r0, #(4*2*16)]           // e...............'...........~...............
        // vldrw.u32 q3, [r0, #(4*3*16)]           // .......e........'..................~........
        // vsub.u16       q4, q0,  q1              // ................'.....*.....................
        // vadd.u16       q0,  q0,  q1             // ................'.......*...................
        // vmul.s16       q1,  q4, r4              // .~..............'............*..............
        // vqrdmulh.s16   q4,  q4, r5              // ................'........*..................
        // vmla.s16       q1,  q4, r12             // ...~............'..............*............
        // vsub.u16       q4, q2,  q3              // ................*...........................
        // vadd.u16       q2,  q2,  q3             // ................'..*........................
        // vmul.s16       q3,  q4, r6              // ................'*..........................
        // vqrdmulh.s16   q4,  q4, r7              // ................'...*.......................
        // vmla.s16       q3,  q4, r12             // ................'......*....................
        // vsub.u16       q4, q0,  q2              // ................'.........*.................
        // vadd.u16       q0,  q0,  q2             // ..~.............'.............*.............
        // vmul.s16       q2,  q4, r2              // ................'..........*................
        // vqrdmulh.s16   q4,  q4, r3              // .....~..........'................*..........
        // vmla.s16       q2,  q4, r12             // ........~.......'...................*.......
        // vsub.u16       q4, q1,  q3              // ......~.........'.................*.........
        // vadd.u16       q1,  q1,  q3             // .........~......'....................*......
        // vmul.s16       q3,  q4, r2              // ..........~.....'.....................*.....
        // vqrdmulh.s16   q4,  q4, r3              // ............~...'.......................*...
        // vmla.s16       q3,  q4, r12             // ..............~.'.........................*.
        // vstrw.u32 q0, [r0], #(16)               // ....~...........'...............*...........
        // vstrw.u32 q1, [r0, #(4*16*1 - 16)]      // ...........~....'......................*....
        // vstrw.u32 q2, [r0, #(4*16*2 - 16)]      // .............~..'........................*..
        // vstrw.u32 q3, [r0, #(4*16*3 - 16)]      // ...............~'..........................*

        le lr, layer23_loop
                                         // Instructions:    26
                                         // Expected cycles: 26
                                         // Expected IPC:    1.00
                                         //
                                         // Wall time:     0.13s
                                         // User time:     0.13s
                                         //
                                         // ----- cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|----
        vldrw.U32 q4, [r0]               // *.............................
        vsub.U16 q7, q6, q5              // .*............................
        vqrdmulh.S16 q3, q7, r7          // ..*...........................
        vldrw.U32 q2, [r0, #64]          // ...*..........................
        vmul.S16 q0, q7, r6              // ....*.........................
        vsub.U16 q7, q4, q2              // .....*........................
        vmul.S16 q1, q7, r4              // ......*.......................
        vadd.U16 q2, q4, q2              // .......*......................
        vqrdmulh.S16 q7, q7, r5          // ........*.....................
        vadd.U16 q5, q6, q5              // .........*....................
        vmla.S16 q1, q7, r12             // ..........*...................
        vadd.U16 q6, q2, q5              // ...........*..................
        vmla.S16 q0, q3, r12             // ............*.................
        vsub.U16 q4, q2, q5              // .............*................
        vqrdmulh.S16 q3, q4, r3          // ..............*...............
        vsub.U16 q2, q1, q0              // ...............*..............
        vqrdmulh.S16 q7, q2, r3          // ................*.............
        vadd.U16 q0, q1, q0              // .................*............
        vmul.S16 q5, q2, r2              // ..................*...........
        vstrw.U32 q0, [r0, #64]          // ...................*..........
        vmla.S16 q5, q7, r12             // ....................*.........
        vstrw.U32 q5, [r0, #192]         // .....................*........
        vmul.S16 q7, q4, r2              // ......................*.......
        vstrw.U32 q6, [r0], #(16)        // .......................*......
        vmla.S16 q7, q3, r12             // ........................*.....
        vstrw.U32 q7, [r0, #112]         // .........................*....

                                          // ------ cycle (expected) ------>
                                          // 0                        25
                                          // |------------------------|-----
        // vsub.U16 q3, q6, q5            // .*.............................
        // vmul.S16 q7, q3, r6            // ....*..........................
        // vldrw.U32 q1, [r0]             // *..............................
        // vadd.U16 q5, q6, q5            // .........*.....................
        // vqrdmulh.S16 q3, q3, r7        // ..*............................
        // vldrw.U32 q6, [r0, #64]        // ...*...........................
        // vsub.U16 q2, q1, q6            // .....*.........................
        // vmla.S16 q7, q3, r12           // ............*..................
        // vadd.U16 q3, q1, q6            // .......*.......................
        // vqrdmulh.S16 q4, q2, r5        // ........*......................
        // vsub.U16 q0, q3, q5            // .............*.................
        // vmul.S16 q1, q0, r2            // ......................*........
        // vmul.S16 q2, q2, r4            // ......*........................
        // vadd.U16 q3, q3, q5            // ...........*...................
        // vmla.S16 q2, q4, r12           // ..........*....................
        // vstrw.U32 q3, [r0], #(16)      // .......................*.......
        // vqrdmulh.S16 q0, q0, r3        // ..............*................
        // vsub.U16 q3, q2, q7            // ...............*...............
        // vmla.S16 q1, q0, r12           // ........................*......
        // vadd.U16 q0, q2, q7            // .................*.............
        // vmul.S16 q7, q3, r2            // ..................*............
        // vstrw.U32 q0, [r0, #48]        // ...................*...........
        // vqrdmulh.S16 q0, q3, r3        // ................*..............
        // vstrw.U32 q1, [r0, #112]       // .........................*.....
        // vmla.S16 q7, q0, r12           // ....................*..........
        // vstrw.U32 q7, [r0, #176]       // .....................*.........

        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*128)

        // TEMPORARY: Barrett reduction

        // This is grossly inefficient and largely unnecessary, but it's just outside
        // the scope of our work to optimize this: We only want to demonstrate the
        // ability of Helight to optimize the core loops.
        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        mov lr, #32
1:
        vldrh.u16 data0, [in]
        vqdmulh.s16 tmp, data0, barrett_const
        vrshr.s16 tmp, tmp, barrett_shift
        vmla.s16 data0, tmp, modulus
        vstrh.u16 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*128)
        .unreq barrett_const

        in_low       .req r0
        in_high      .req r1
        add in_high, in_low, #(4*64)

        // Layers 1

        load_first_root root0, root0_twisted

        mov lr, #16
                                  // Instructions:    2
                                  // Expected cycles: 3
                                  // Expected IPC:    0.67
                                  //
                                  // Wall time:     0.00s
                                  // User time:     0.00s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        vldrw.U32 q0, [r0]        // *.............................
        vldrw.U32 q1, [r1]        // ..*...........................

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // vldrw.U32 q1, [r1]      // ..*............................
        // vldrw.U32 q0, [r0]      // *..............................

        sub lr, lr, #1
.p2align 2
layer1_loop:
                                       // Instructions:    9
                                       // Expected cycles: 9
                                       // Expected IPC:    1.00
                                       //
                                       // Wall time:     0.03s
                                       // User time:     0.03s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vadd.U16 q3, q0, q1            // *.............................
        vstrw.U32 q3, [r0], #16        // .*............................
        vsub.U16 q2, q0, q1            // ..*...........................
        vqrdmulh.S16 q5, q2, r3        // ...*..........................
        vldrw.U32 q1, [r1, #16]        // ....e.........................
        vmul.S16 q7, q2, r2            // .....*........................
        vldrw.U32 q0, [r0]             // ......e.......................
        vmla.S16 q7, q5, r12           // .......*......................
        vstrw.U32 q7, [r1], #16        // ........*.....................

                                            // ------ cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|-----
        // vldrw.u32 q0, [r0]               // ..e..'.....~..'.....~..'.....~.
        // vldrw.u32 q1, [r1]               // e....'...~....'...~....'...~...
        // vsub.u16       q4, q0,  q1       // .....'.*......'.~......'.~.....
        // vadd.u16       q0,  q0,  q1      // .....*........~........~.......
        // vmul.s16       q1,  q4, r2       // .~...'....*...'....~...'....~..
        // vqrdmulh.s16   q4,  q4, r3       // .....'..*.....'..~.....'..~....
        // vmla.s16       q1,  q4, r12      // ...~.'......*.'......~.'.......
        // vstrw.u32 q0, [r0], #16          // .....'*.......'~.......'~......
        // vstrw.u32 q1, [r1], #16          // ....~'.......*'.......~'.......

        le lr, layer1_loop
                                       // Instructions:    7
                                       // Expected cycles: 7
                                       // Expected IPC:    1.00
                                       //
                                       // Wall time:     0.00s
                                       // User time:     0.00s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vsub.U16 q7, q0, q1            // *.............................
        vqrdmulh.S16 q3, q7, r3        // .*............................
        vadd.U16 q1, q0, q1            // ..*...........................
        vmul.S16 q7, q7, r2            // ...*..........................
        vstrw.U32 q1, [r0], #16        // ....*.........................
        vmla.S16 q7, q3, r12           // .....*........................
        vstrw.U32 q7, [r1], #16        // ......*.......................

                                        // ------ cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|-----
        // vadd.U16 q3, q0, q1          // ..*............................
        // vstrw.U32 q3, [r0], #16      // ....*..........................
        // vsub.U16 q2, q0, q1          // *..............................
        // vqrdmulh.S16 q5, q2, r3      // .*.............................
        // vmul.S16 q7, q2, r2          // ...*...........................
        // vmla.S16 q7, q5, r12         // .....*.........................
        // vstrw.U32 q7, [r1], #16      // ......*........................


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
