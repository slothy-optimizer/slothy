
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots_inv:
#include "intt_kyber_1_23_45_67_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s16       \dst,  \src, \const
        vqrdmulh.s16   \src,  \src, \const_twisted
        vmla.s16       \dst,  \src, modulus
.endm

.macro gs_butterfly a, b, root, root_twisted
        vsub.u16       tmp, \a,  \b
        vadd.u16       \a,  \a,  \b
        mulmod         \b,  tmp, \root, \root_twisted
.endm

.macro load_first_root root0, root0_twisted
        ldrd root0, root0_twisted, [root_ptr], #+8
.endm

.macro load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted
        ldrd root0, root0_twisted, [root_ptr], #+24
        ldrd root1, root1_twisted, [root_ptr, #(-16)]
        ldrd root2, root2_twisted, [root_ptr, #(-8)]
.endm

.align 4
roots_addr: .word roots_inv
.syntax unified
.type intt_kyber_1_23_45_67_opt_m85, %function
.global intt_kyber_1_23_45_67_opt_m85
intt_kyber_1_23_45_67_opt_m85:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus     .req r12
        root_ptr    .req r11

        .equ modulus_const, -3329
        movw modulus, #:lower16:modulus_const
        ldr  root_ptr, roots_addr

        in .req r0

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6

        tmp .req q4

        // Layers 6,7

        mov lr, #8
                                  // Instructions:    1
                                  // Expected cycles: 1
                                  // Expected IPC:    1.00
                                  //
                                  // Wall time:     0.00s
                                  // User time:     0.00s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        vldrw.U32 q2, [r0]        // *.............................

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // vldrw.U32 q2, [r0]      // *..............................

        sub lr, lr, #1
.p2align 2
layer67_loop:
                                            // Instructions:    34
                                            // Expected cycles: 34
                                            // Expected IPC:    1.00
                                            //
                                            // Wall time:     2.79s
                                            // User time:     2.79s
                                            //
                                            // ------- cycle (expected) -------->
                                            // 0                        25
                                            // |------------------------|--------
        vldrw.U32 q7, [r0, #16]             // *.................................
        vsub.U16 q4, q2, q7                 // .*................................
        vldrw.U32 q1, [r11, #32]            // ..*...............................
        vadd.U16 q3, q2, q7                 // ...*..............................
        vldrw.U32 q6, [r11, #48]            // ....*.............................
        vmul.S16 q7, q4, q1                 // .....*............................
        vldrw.U32 q5, [r0, #48]             // ......*...........................
        vqrdmulh.S16 q4, q4, q6             // .......*..........................
        vldrw.U32 q6, [r0, #32]             // ........*.........................
        vsub.U16 q2, q6, q5                 // .........*........................
        vmla.S16 q7, q4, r12                // ..........*.......................
        vldrw.U32 q4, [r11, #64]            // ...........*......................
        vmul.S16 q4, q2, q4                 // ............*.....................
        vldrw.U32 q1, [r11, #80]            // .............*....................
        vqrdmulh.S16 q2, q2, q1             // ..............*...................
        vadd.U16 q5, q6, q5                 // ...............*..................
        vmla.S16 q4, q2, r12                // ................*.................
        vldrw.U32 q2, [r11, #16]            // .................*................
        vadd.U16 q1, q7, q4                 // ..................*...............
        vstrw.U32 q1, [r0, #16]             // ...................*..............
        vsub.U16 q0, q7, q4                 // ....................*.............
        vqrdmulh.S16 q6, q0, q2             // .....................*............
        vldrw.U32 q1, [r11], #(3*32)        // ......................*...........
        vsub.U16 q7, q3, q5                 // .......................*..........
        vqrdmulh.S16 q4, q7, q2             // ........................*.........
        vldrw.U32 q2, [r0, #64]             // .........................e........
        vmul.S16 q7, q7, q1                 // ..........................*.......
        vadd.U16 q3, q3, q5                 // ...........................*......
        vmla.S16 q7, q4, r12                // ............................*.....
        vstrw.U32 q3, [r0], #64             // .............................*....
        vmul.S16 q4, q0, q1                 // ..............................*...
        vstrw.U32 q7, [r0, #-32]            // ...............................*..
        vmla.S16 q4, q6, r12                // ................................*.
        vstrw.U32 q4, [r0, #-16]            // .................................*

                                                     // ------------ cycle (expected) ------------>
                                                     // 0                        25
                                                     // |------------------------|-----------------
        // vldrw.u32 q0, [r0]                        // e........'........................~........
        // vldrw.u32 q1, [r0, #(4*4*1)]              // .........*.................................
        // vldrw.u32 q2, [r0, #(4*4*2)]              // .........'.......*.........................
        // vldrw.u32 q3, [r0, #(4*4*3)]              // .........'.....*...........................
        // vldrw.u32 q5,         [r11, #(32)]        // .........'.*...............................
        // vldrw.u32 q6, [r11, #(32+16)]             // .........'...*.............................
        // vsub.u16       q4, q0,  q1                // .........'*................................
        // vadd.u16       q0,  q0,  q1               // .........'..*..............................
        // vmul.s16       q1,  q4, q5                // .........'....*............................
        // vqrdmulh.s16   q4,  q4, q6                // .........'......*..........................
        // vmla.s16       q1,  q4, r12               // .........'.........*.......................
        // vldrw.u32 q5,         [r11, #(64)]        // .........'..........*......................
        // vldrw.u32 q6, [r11, #(64+16)]             // .........'............*....................
        // vsub.u16       q4, q2,  q3                // .........'........*........................
        // vadd.u16       q2,  q2,  q3               // .........'..............*..................
        // vmul.s16       q3,  q4, q5                // .........'...........*.....................
        // vqrdmulh.s16   q4,  q4, q6                // .........'.............*...................
        // vmla.s16       q3,  q4, r12               // .........'...............*.................
        // vldrw.u32 q5,         [r11], #(3*32)      // .........'.....................*...........
        // vldrw.u32 q6, [r11, #(16 - 3*32)]         // .........'................*................
        // vsub.u16       q4, q0,  q2                // .........'......................*..........
        // vadd.u16       q0,  q0,  q2               // ..~......'..........................*......
        // vmul.s16       q2,  q4, q5                // .~.......'.........................*.......
        // vqrdmulh.s16   q4,  q4, q6                // .........'.......................*.........
        // vmla.s16       q2,  q4, r12               // ...~.....'...........................*.....
        // vsub.u16       q4, q1,  q3                // .........'...................*.............
        // vadd.u16       q1,  q1,  q3               // .........'.................*...............
        // vmul.s16       q3,  q4, q5                // .....~...'.............................*...
        // vqrdmulh.s16   q4,  q4, q6                // .........'....................*............
        // vmla.s16       q3,  q4, r12               // .......~.'...............................*.
        // vstrw.u32 q0, [r0], #64                   // ....~....'............................*....
        // vstrw.u32 q1, [r0, #(4*4*1 - 64)]         // .........'..................*..............
        // vstrw.u32 q2, [r0, #(4*4*2 - 64)]         // ......~..'..............................*..
        // vstrw.u32 q3, [r0, #(4*4*3 - 64)]         // ........~'................................*

        le lr, layer67_loop
                                            // Instructions:    33
                                            // Expected cycles: 33
                                            // Expected IPC:    1.00
                                            //
                                            // Wall time:     0.23s
                                            // User time:     0.23s
                                            //
                                            // ------- cycle (expected) ------->
                                            // 0                        25
                                            // |------------------------|-------
        vldrw.U32 q7, [r0, #16]             // *................................
        vadd.U16 q6, q2, q7                 // .*...............................
        vldrw.U32 q5, [r11, #32]            // ..*..............................
        vsub.U16 q1, q2, q7                 // ...*.............................
        vmul.S16 q7, q1, q5                 // ....*............................
        vldrw.U32 q0, [r11, #48]            // .....*...........................
        vqrdmulh.S16 q2, q1, q0             // ......*..........................
        vldrw.U32 q1, [r0, #32]             // .......*.........................
        vmla.S16 q7, q2, r12                // ........*........................
        vldrw.U32 q5, [r0, #48]             // .........*.......................
        vadd.U16 q4, q1, q5                 // ..........*......................
        vldrw.U32 q2, [r11, #64]            // ...........*.....................
        vsub.U16 q3, q6, q4                 // ............*....................
        vldrw.U32 q0, [r11], #(3*32)        // .............*...................
        vadd.U16 q4, q6, q4                 // ..............*..................
        vmul.S16 q6, q3, q0                 // ...............*.................
        vsub.U16 q1, q1, q5                 // ................*................
        vmul.S16 q2, q1, q2                 // .................*...............
        vldrw.U32 q5, [r11, #-16]           // ..................*..............
        vqrdmulh.S16 q5, q1, q5             // ...................*.............
        vldrw.U32 q1, [r11, #-80]           // ....................*............
        vmla.S16 q2, q5, r12                // .....................*...........
        vstrw.U32 q4, [r0], #64             // ......................*..........
        vqrdmulh.S16 q3, q3, q1             // .......................*.........
        vadd.U16 q5, q7, q2                 // ........................*........
        vmla.S16 q6, q3, r12                // .........................*.......
        vsub.U16 q2, q7, q2                 // ..........................*......
        vmul.S16 q0, q2, q0                 // ...........................*.....
        vstrw.U32 q6, [r0, #-32]            // ............................*....
        vqrdmulh.S16 q6, q2, q1             // .............................*...
        vstrw.U32 q5, [r0, #-48]            // ..............................*..
        vmla.S16 q0, q6, r12                // ...............................*.
        vstrw.U32 q0, [r0, #-16]            // ................................*

                                             // ------- cycle (expected) ------->
                                             // 0                        25
                                             // |------------------------|-------
        // vldrw.U32 q7, [r0, #16]           // *................................
        // vsub.U16 q4, q2, q7               // ...*.............................
        // vldrw.U32 q1, [r11, #32]          // ..*..............................
        // vadd.U16 q3, q2, q7               // .*...............................
        // vldrw.U32 q6, [r11, #48]          // .....*...........................
        // vmul.S16 q7, q4, q1               // ....*............................
        // vldrw.U32 q5, [r0, #48]           // .........*.......................
        // vqrdmulh.S16 q4, q4, q6           // ......*..........................
        // vldrw.U32 q6, [r0, #32]           // .......*.........................
        // vsub.U16 q2, q6, q5               // ................*................
        // vmla.S16 q7, q4, r12              // ........*........................
        // vldrw.U32 q4, [r11, #64]          // ...........*.....................
        // vmul.S16 q4, q2, q4               // .................*...............
        // vldrw.U32 q1, [r11, #80]          // ..................*..............
        // vqrdmulh.S16 q2, q2, q1           // ...................*.............
        // vadd.U16 q5, q6, q5               // ..........*......................
        // vmla.S16 q4, q2, r12              // .....................*...........
        // vldrw.U32 q2, [r11, #16]          // ....................*............
        // vadd.U16 q1, q7, q4               // ........................*........
        // vstrw.U32 q1, [r0, #16]           // ..............................*..
        // vsub.U16 q0, q7, q4               // ..........................*......
        // vqrdmulh.S16 q6, q0, q2           // .............................*...
        // vldrw.U32 q1, [r11], #(3*32)      // .............*...................
        // vsub.U16 q7, q3, q5               // ............*....................
        // vqrdmulh.S16 q4, q7, q2           // .......................*.........
        // vmul.S16 q7, q7, q1               // ...............*.................
        // vadd.U16 q3, q3, q5               // ..............*..................
        // vmla.S16 q7, q4, r12              // .........................*.......
        // vstrw.U32 q3, [r0], #64           // ......................*..........
        // vmul.S16 q4, q0, q1               // ...........................*.....
        // vstrw.U32 q7, [r0, #-32]          // ............................*....
        // vmla.S16 q4, q6, r12              // ...............................*.
        // vstrw.U32 q4, [r0, #-16]          // ................................*


        sub in, in, #(4*128)

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        .equ const_barrett, 10079
        .equ barrett_shift, 10

        // TEMPORARY: Barrett reduction

        // This is grossly inefficient and largely unnecessary, but it's just outside
        // the scope of our work to optimize this: We only want to demonstrate the
        // ability of Helight to optimize the core loops.
        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        mov lr, #32
1:
        vldrh.u16 data0, [in]
        vqdmulh.s16 tmp, data0, barrett_const
        vrshr.s16 tmp, tmp, barrett_shift
        vmla.s16 data0, tmp, modulus
        vstrh.u16 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*128)
        .unreq barrett_const

        // Layers 4,5

        mov lr, #8
                                                 // Instructions:    4
                                                 // Expected cycles: 11
                                                 // Expected IPC:    0.36
                                                 //
                                                 // Wall time:     0.00s
                                                 // User time:     0.00s
                                                 //
                                                 // ----- cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|----
        vld40.U32 {q0, q1, q2, q3}, [r0]         // *.............................
        vld41.U32 {q0, q1, q2, q3}, [r0]         // ..*...........................
        vld42.U32 {q0, q1, q2, q3}, [r0]         // ......*.......................
        vld43.U32 {q0, q1, q2, q3}, [r0]!        // ..........*...................

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vld40.U32 {q0, q1, q2, q3}, [r0]       // *..............................
        // vld41.U32 {q0, q1, q2, q3}, [r0]       // ..*............................
        // vld42.U32 {q0, q1, q2, q3}, [r0]       // ......*........................
        // vld43.U32 {q0, q1, q2, q3}, [r0]!      // ..........*....................

        sub lr, lr, #1
.p2align 2
layer45_loop:
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     1.86s
                                                 // User time:     1.86s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vadd.U16 q7, q0, q1                      // *..............................
        ldrd r5, r6, [r11], #+24                 // .*.............................
        vsub.U16 q6, q0, q1                      // ..*............................
        ldrd r9, r2, [r11, #-8]                  // ...*...........................
        vsub.U16 q0, q2, q3                      // ....*..........................
        vqrdmulh.S16 q1, q0, r2                  // .....*.........................
        vadd.U16 q3, q2, q3                      // ......*........................
        vmul.S16 q4, q0, r9                      // .......*.......................
        vsub.U16 q5, q7, q3                      // ........*......................
        vqrdmulh.S16 q0, q5, r6                  // .........*.....................
        vadd.U16 q7, q7, q3                      // ..........*....................
        vmul.S16 q2, q5, r5                      // ...........*...................
        ldrd r8, r3, [r11, #-16]                 // ............*..................
        vmla.S16 q2, q0, r12                     // .............*.................
        vstrw.U32 q2, [r0, #-32]                 // ..............*................
        vmla.S16 q4, q1, r12                     // ...............*...............
        vld40.U32 {q0, q1, q2, q3}, [r0]         // ................e..............
        vqrdmulh.S16 q5, q6, r3                  // .................*.............
        vld41.U32 {q0, q1, q2, q3}, [r0]         // ..................e............
        vmul.S16 q6, q6, r8                      // ...................*...........
        vstrw.U32 q7, [r0, #-64]                 // ....................*..........
        vmla.S16 q6, q5, r12                     // .....................*.........
        vld42.U32 {q0, q1, q2, q3}, [r0]         // ......................e........
        vsub.U16 q5, q6, q4                      // .......................*.......
        vqrdmulh.S16 q7, q5, r6                  // ........................*......
        vadd.U16 q6, q6, q4                      // .........................*.....
        vmul.S16 q4, q5, r5                      // ..........................*....
        vld43.U32 {q0, q1, q2, q3}, [r0]!        // ...........................e...
        vstrw.U32 q6, [r0, #-112]                // ............................*..
        vmla.S16 q4, q7, r12                     // .............................*.
        vstrw.U32 q4, [r0, #-80]                 // ..............................*

                                                  // ------------- cycle (expected) -------------->
                                                  // 0                        25
                                                  // |------------------------|--------------------
        // ldrd r2, r3, [r11], #+24               // ...............'*.............................
        // ldrd r4, r5, [r11, #(-16)]             // ...............'...........*..................
        // ldrd r6, r7, [r11, #(-8)]              // ...............'..*...........................
        // vld40.u32 {q0, q1, q2, q3}, [r0]       // e..............'...............~..............
        // vld41.u32 {q0, q1, q2, q3}, [r0]       // ..e............'.................~............
        // vld42.u32 {q0, q1, q2, q3}, [r0]       // ......e........'.....................~........
        // vld43.u32 {q0, q1, q2, q3}, [r0]!      // ...........e...'..........................~...
        // vsub.u16       q4, q0,  q1             // ...............'.*............................
        // vadd.u16       q0,  q0,  q1            // ...............*..............................
        // vmul.s16       q1,  q4, r4             // ...~...........'..................*...........
        // vqrdmulh.s16   q4,  q4, r5             // .~.............'................*.............
        // vmla.s16       q1,  q4, r12            // .....~.........'....................*.........
        // vsub.u16       q4, q2,  q3             // ...............'...*..........................
        // vadd.u16       q2,  q2,  q3            // ...............'.....*........................
        // vmul.s16       q3,  q4, r6             // ...............'......*.......................
        // vqrdmulh.s16   q4,  q4, r7             // ...............'....*.........................
        // vmla.s16       q3,  q4, r12            // ...............'..............*...............
        // vsub.u16       q4, q0,  q2             // ...............'.......*......................
        // vadd.u16       q0,  q0,  q2            // ...............'.........*....................
        // vmul.s16       q2,  q4, r2             // ...............'..........*...................
        // vqrdmulh.s16   q4,  q4, r3             // ...............'........*.....................
        // vmla.s16       q2,  q4, r12            // ...............'............*.................
        // vsub.u16       q4, q1,  q3             // .......~.......'......................*.......
        // vadd.u16       q1,  q1,  q3            // .........~.....'........................*.....
        // vmul.s16       q3,  q4, r2             // ..........~....'.........................*....
        // vqrdmulh.s16   q4,  q4, r3             // ........~......'.......................*......
        // vmla.s16       q3,  q4, r12            // .............~.'............................*.
        // vstrw.u32 q0, [r0, #(4*4*0 - 64)]      // ....~..........'...................*..........
        // vstrw.u32 q1, [r0, #(4*4*1 - 64)]      // ............~..'...........................*..
        // vstrw.u32 q2, [r0, #(4*4*2 - 64)]      // ...............'.............*................
        // vstrw.u32 q3, [r0, #(4*4*3 - 64)]      // ..............~'.............................*

        le lr, layer45_loop
                                        // Instructions:    27
                                        // Expected cycles: 27
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     0.13s
                                        // User time:     0.13s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        ldrd r6, r8, [r11, #8]          // *.............................
        vsub.U16 q4, q0, q1             // .*............................
        vmul.S16 q7, q4, r6             // ..*...........................
        vadd.U16 q6, q2, q3             // ...*..........................
        vqrdmulh.S16 q4, q4, r8         // ....*.........................
        ldrd r6, r8, [r11, #16]         // .....*........................
        vmla.S16 q7, q4, r12            // ......*.......................
        vsub.U16 q3, q2, q3             // .......*......................
        vqrdmulh.S16 q5, q3, r8         // ........*.....................
        vadd.U16 q0, q0, q1             // .........*....................
        vmul.S16 q3, q3, r6             // ..........*...................
        vadd.U16 q4, q0, q6             // ...........*..................
        vmla.S16 q3, q5, r12            // ............*.................
        ldrd r5, r6, [r11], #+24        // .............*................
        vsub.U16 q2, q7, q3             // ..............*...............
        vmul.S16 q5, q2, r5             // ...............*..............
        vadd.U16 q3, q7, q3             // ................*.............
        vqrdmulh.S16 q2, q2, r6         // .................*............
        vsub.U16 q7, q0, q6             // ..................*...........
        vmla.S16 q5, q2, r12            // ...................*..........
        vstrw.U32 q5, [r0, #-16]        // ....................*.........
        vmul.S16 q2, q7, r5             // .....................*........
        vstrw.U32 q3, [r0, #-48]        // ......................*.......
        vqrdmulh.S16 q1, q7, r6         // .......................*......
        vstrw.U32 q4, [r0, #-64]        // ........................*.....
        vmla.S16 q2, q1, r12            // .........................*....
        vstrw.U32 q2, [r0, #-32]        // ..........................*...

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vadd.U16 q7, q0, q1           // .........*.....................
        // ldrd r5, r6, [r11], #+24      // .............*.................
        // vsub.U16 q6, q0, q1           // .*.............................
        // ldrd r9, r2, [r11, #-8]       // .....*.........................
        // vsub.U16 q0, q2, q3           // .......*.......................
        // vqrdmulh.S16 q1, q0, r2       // ........*......................
        // vadd.U16 q3, q2, q3           // ...*...........................
        // vmul.S16 q4, q0, r9           // ..........*....................
        // vsub.U16 q5, q7, q3           // ..................*............
        // vqrdmulh.S16 q0, q5, r6       // .......................*.......
        // vadd.U16 q7, q7, q3           // ...........*...................
        // vmul.S16 q2, q5, r5           // .....................*.........
        // ldrd r8, r3, [r11, #-16]      // *..............................
        // vmla.S16 q2, q0, r12          // .........................*.....
        // vstrw.U32 q2, [r0, #-32]      // ..........................*....
        // vmla.S16 q4, q1, r12          // ............*..................
        // vqrdmulh.S16 q5, q6, r3       // ....*..........................
        // vmul.S16 q6, q6, r8           // ..*............................
        // vstrw.U32 q7, [r0, #-64]      // ........................*......
        // vmla.S16 q6, q5, r12          // ......*........................
        // vsub.U16 q5, q6, q4           // ..............*................
        // vqrdmulh.S16 q7, q5, r6       // .................*.............
        // vadd.U16 q6, q6, q4           // ................*..............
        // vmul.S16 q4, q5, r5           // ...............*...............
        // vstrw.U32 q6, [r0, #-48]      // ......................*........
        // vmla.S16 q4, q7, r12          // ...................*...........
        // vstrw.U32 q4, [r0, #-16]      // ....................*..........


        sub in, in, #(4*128)

        // TEMPORARY: Barrett reduction

        // This is grossly inefficient and largely unnecessary, but it's just outside
        // the scope of our work to optimize this: We only want to demonstrate the
        // ability of Helight to optimize the core loops.

        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        mov lr, #32
1:
        vldrh.u16 data0, [in]
        vqdmulh.s16 tmp, data0, barrett_const
        vrshr.s16 tmp, tmp, barrett_shift
        vmla.s16 data0, tmp, modulus
        vstrh.u16 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*128)
        .unreq barrett_const

        // Layers 2,3

        count .req r1
        mov count, #2
out_start:
        load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted

        mov lr, #4
                                  // Instructions:    1
                                  // Expected cycles: 1
                                  // Expected IPC:    1.00
                                  //
                                  // Wall time:     0.00s
                                  // User time:     0.00s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        vldrw.U32 q0, [r0]        // *.............................

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // vldrw.U32 q0, [r0]      // *..............................

        sub lr, lr, #1
.p2align 2
layer23_loop:
                                         // Instructions:    28
                                         // Expected cycles: 28
                                         // Expected IPC:    1.00
                                         //
                                         // Wall time:     1.49s
                                         // User time:     1.49s
                                         //
                                         // ----- cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|----
        vldrw.U32 q6, [r0, #64]          // *.............................
        vsub.U16 q7, q0, q6              // .*............................
        vldrw.U32 q3, [r0, #192]         // ..*...........................
        vqrdmulh.S16 q5, q7, r5          // ...*..........................
        vldrw.U32 q4, [r0, #128]         // ....*.........................
        vsub.U16 q2, q4, q3              // .....*........................
        vqrdmulh.S16 q1, q2, r7          // ......*.......................
        vadd.U16 q3, q4, q3              // .......*......................
        vmul.S16 q4, q2, r6              // ........*.....................
        vadd.U16 q2, q0, q6              // .........*....................
        vmla.S16 q4, q1, r12             // ..........*...................
        vsub.U16 q1, q2, q3              // ...........*..................
        vqrdmulh.S16 q6, q1, r3          // ............*.................
        vadd.U16 q3, q2, q3              // .............*................
        vmul.S16 q7, q7, r4              // ..............*...............
        vstrw.U32 q3, [r0], #(16)        // ...............*..............
        vmla.S16 q7, q5, r12             // ................*.............
        vldrw.U32 q0, [r0]               // .................e............
        vmul.S16 q2, q1, r2              // ..................*...........
        vsub.U16 q3, q7, q4              // ...................*..........
        vmla.S16 q2, q6, r12             // ....................*.........
        vstrw.U32 q2, [r0, #112]         // .....................*........
        vmul.S16 q2, q3, r2              // ......................*.......
        vadd.U16 q1, q7, q4              // .......................*......
        vqrdmulh.S16 q4, q3, r3          // ........................*.....
        vstrw.U32 q1, [r0, #48]          // .........................*....
        vmla.S16 q2, q4, r12             // ..........................*...
        vstrw.U32 q2, [r0, #176]         // ...........................*..

                                                   // ---------- cycle (expected) ---------->
                                                   // 0                        25
                                                   // |------------------------|-------------
        // vldrw.u32 q0, [r0]                      // e..........'................~..........
        // vldrw.u32 q1, [r0, #(4*1*16)]           // ...........*...........................
        // vldrw.u32 q2, [r0, #(4*2*16)]           // ...........'...*.......................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // ...........'.*.........................
        // vsub.u16       q4, q0,  q1              // ...........'*..........................
        // vadd.u16       q0,  q0,  q1             // ...........'........*..................
        // vmul.s16       q1,  q4, r4              // ...........'.............*.............
        // vqrdmulh.s16   q4,  q4, r5              // ...........'..*........................
        // vmla.s16       q1,  q4, r12             // ...........'...............*...........
        // vsub.u16       q4, q2,  q3              // ...........'....*......................
        // vadd.u16       q2,  q2,  q3             // ...........'......*....................
        // vmul.s16       q3,  q4, r6              // ...........'.......*...................
        // vqrdmulh.s16   q4,  q4, r7              // ...........'.....*.....................
        // vmla.s16       q3,  q4, r12             // ...........'.........*.................
        // vsub.u16       q4, q0,  q2              // ...........'..........*................
        // vadd.u16       q0,  q0,  q2             // ...........'............*..............
        // vmul.s16       q2,  q4, r2              // .~.........'.................*.........
        // vqrdmulh.s16   q4,  q4, r3              // ...........'...........*...............
        // vmla.s16       q2,  q4, r12             // ...~.......'...................*.......
        // vsub.u16       q4, q1,  q3              // ..~........'..................*........
        // vadd.u16       q1,  q1,  q3             // ......~....'......................*....
        // vmul.s16       q3,  q4, r2              // .....~.....'.....................*.....
        // vqrdmulh.s16   q4,  q4, r3              // .......~...'.......................*...
        // vmla.s16       q3,  q4, r12             // .........~.'.........................*.
        // vstrw.u32 q0, [r0], #(16)               // ...........'..............*............
        // vstrw.u32 q1, [r0, #(4*16*1 - 16)]      // ........~..'........................*..
        // vstrw.u32 q2, [r0, #(4*16*2 - 16)]      // ....~......'....................*......
        // vstrw.u32 q3, [r0, #(4*16*3 - 16)]      // ..........~'..........................*

        le lr, layer23_loop
                                         // Instructions:    27
                                         // Expected cycles: 27
                                         // Expected IPC:    1.00
                                         //
                                         // Wall time:     0.08s
                                         // User time:     0.08s
                                         //
                                         // ----- cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|----
        vldrw.U32 q2, [r0, #64]          // *.............................
        vsub.U16 q4, q0, q2              // .*............................
        vqrdmulh.S16 q1, q4, r5          // ..*...........................
        vldrw.U32 q3, [r0, #128]         // ...*..........................
        vmul.S16 q4, q4, r4              // ....*.........................
        vldrw.U32 q6, [r0, #192]         // .....*........................
        vsub.U16 q5, q3, q6              // ......*.......................
        vmul.S16 q7, q5, r6              // .......*......................
        vadd.U16 q3, q3, q6              // ........*.....................
        vqrdmulh.S16 q5, q5, r7          // .........*....................
        vadd.U16 q6, q0, q2              // ..........*...................
        vmla.S16 q7, q5, r12             // ...........*..................
        vsub.U16 q0, q6, q3              // ............*.................
        vmul.S16 q2, q0, r2              // .............*................
        vadd.U16 q5, q6, q3              // ..............*...............
        vmla.S16 q4, q1, r12             // ...............*..............
        vstrw.U32 q5, [r0], #(16)        // ................*.............
        vqrdmulh.S16 q5, q0, r3          // .................*............
        vadd.U16 q0, q4, q7              // ..................*...........
        vmla.S16 q2, q5, r12             // ...................*..........
        vsub.U16 q4, q4, q7              // ....................*.........
        vqrdmulh.S16 q1, q4, r3          // .....................*........
        vstrw.U32 q2, [r0, #112]         // ......................*.......
        vmul.S16 q7, q4, r2              // .......................*......
        vstrw.U32 q0, [r0, #48]          // ........................*.....
        vmla.S16 q7, q1, r12             // .........................*....
        vstrw.U32 q7, [r0, #176]         // ..........................*...

                                          // ------ cycle (expected) ------>
                                          // 0                        25
                                          // |------------------------|-----
        // vldrw.U32 q6, [r0, #64]        // *..............................
        // vsub.U16 q7, q0, q6            // .*.............................
        // vldrw.U32 q3, [r0, #192]       // .....*.........................
        // vqrdmulh.S16 q5, q7, r5        // ..*............................
        // vldrw.U32 q4, [r0, #128]       // ...*...........................
        // vsub.U16 q2, q4, q3            // ......*........................
        // vqrdmulh.S16 q1, q2, r7        // .........*.....................
        // vadd.U16 q3, q4, q3            // ........*......................
        // vmul.S16 q4, q2, r6            // .......*.......................
        // vadd.U16 q2, q0, q6            // ..........*....................
        // vmla.S16 q4, q1, r12           // ...........*...................
        // vsub.U16 q1, q2, q3            // ............*..................
        // vqrdmulh.S16 q6, q1, r3        // .................*.............
        // vadd.U16 q3, q2, q3            // ..............*................
        // vmul.S16 q7, q7, r4            // ....*..........................
        // vstrw.U32 q3, [r0], #(16)      // ................*..............
        // vmla.S16 q7, q5, r12           // ...............*...............
        // vmul.S16 q2, q1, r2            // .............*.................
        // vsub.U16 q3, q7, q4            // ....................*..........
        // vmla.S16 q2, q6, r12           // ...................*...........
        // vstrw.U32 q2, [r0, #112]       // ......................*........
        // vmul.S16 q2, q3, r2            // .......................*.......
        // vadd.U16 q1, q7, q4            // ..................*............
        // vqrdmulh.S16 q4, q3, r3        // .....................*.........
        // vstrw.U32 q1, [r0, #48]        // ........................*......
        // vmla.S16 q2, q4, r12           // .........................*.....
        // vstrw.U32 q2, [r0, #176]       // ..........................*....

        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*128)

        // TEMPORARY: Barrett reduction

        // This is grossly inefficient and largely unnecessary, but it's just outside
        // the scope of our work to optimize this: We only want to demonstrate the
        // ability of Helight to optimize the core loops.
        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        mov lr, #32
1:
        vldrh.u16 data0, [in]
        vqdmulh.s16 tmp, data0, barrett_const
        vrshr.s16 tmp, tmp, barrett_shift
        vmla.s16 data0, tmp, modulus
        vstrh.u16 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*128)
        .unreq barrett_const

        in_low       .req r0
        in_high      .req r1
        add in_high, in_low, #(4*64)

        // Layers 1

        load_first_root root0, root0_twisted

        mov lr, #16
                                  // Instructions:    1
                                  // Expected cycles: 1
                                  // Expected IPC:    1.00
                                  //
                                  // Wall time:     0.00s
                                  // User time:     0.00s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        vldrw.U32 q2, [r1]        // *.............................

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // vldrw.U32 q2, [r1]      // *..............................

        sub lr, lr, #1
.p2align 2
layer1_loop:
                                       // Instructions:    9
                                       // Expected cycles: 9
                                       // Expected IPC:    1.00
                                       //
                                       // Wall time:     0.04s
                                       // User time:     0.04s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vldrw.U32 q4, [r0]             // *.............................
        vsub.U16 q7, q4, q2            // .*............................
        vqrdmulh.S16 q6, q7, r3        // ..*...........................
        vadd.U16 q4, q4, q2            // ...*..........................
        vstrw.U32 q4, [r0], #16        // ....*.........................
        vmul.S16 q7, q7, r2            // .....*........................
        vldrw.U32 q2, [r1, #16]        // ......e.......................
        vmla.S16 q7, q6, r12           // .......*......................
        vstrw.U32 q7, [r1], #16        // ........*.....................

                                            // ------ cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|-----
        // vldrw.u32 q0, [r0]               // ...*........~........~.........
        // vldrw.u32 q1, [r1]               // e..'.....~..'.....~..'.....~...
        // vsub.u16       q4, q0,  q1       // ...'*.......'~.......'~........
        // vadd.u16       q0,  q0,  q1      // ...'..*.....'..~.....'..~......
        // vmul.s16       q1,  q4, r2       // ...'....*...'....~...'....~....
        // vqrdmulh.s16   q4,  q4, r3       // ...'.*......'.~......'.~.......
        // vmla.s16       q1,  q4, r12      // .~.'......*.'......~.'......~..
        // vstrw.u32 q0, [r0], #16          // ...'...*....'...~....'...~.....
        // vstrw.u32 q1, [r1], #16          // ..~'.......*'.......~'.......~.

        le lr, layer1_loop
                                       // Instructions:    8
                                       // Expected cycles: 8
                                       // Expected IPC:    1.00
                                       //
                                       // Wall time:     0.00s
                                       // User time:     0.00s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vldrw.U32 q4, [r0]             // *.............................
        vsub.U16 q6, q4, q2            // .*............................
        vmul.S16 q7, q6, r2            // ..*...........................
        vadd.U16 q0, q4, q2            // ...*..........................
        vqrdmulh.S16 q6, q6, r3        // ....*.........................
        vstrw.U32 q0, [r0], #16        // .....*........................
        vmla.S16 q7, q6, r12           // ......*.......................
        vstrw.U32 q7, [r1], #16        // .......*......................

                                        // ------ cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|-----
        // vldrw.U32 q4, [r0]           // *..............................
        // vsub.U16 q7, q4, q2          // .*.............................
        // vqrdmulh.S16 q6, q7, r3      // ....*..........................
        // vadd.U16 q4, q4, q2          // ...*...........................
        // vstrw.U32 q4, [r0], #16      // .....*.........................
        // vmul.S16 q7, q7, r2          // ..*............................
        // vmla.S16 q7, q6, r12         // ......*........................
        // vstrw.U32 q7, [r1], #16      // .......*.......................


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
