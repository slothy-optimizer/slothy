
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots:
#include "ntt_kyber_1_23_45_67_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s16       \dst,  \src, \const
        vqrdmulh.s16   \src,  \src, \const_twisted
        vmla.s16       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_twisted
        mulmod tmp, \b, \root, \root_twisted
        vsub.u16       \b,    \a, tmp
        vadd.u16       \a,    \a, tmp
.endm

.macro load_first_root root0, root0_twisted
        ldrd root0, root0_twisted, [root_ptr], #+8
.endm

.macro load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted
        ldrd root0, root0_twisted, [root_ptr], #+24
        ldrd root1, root1_twisted, [root_ptr, #(-16)]
        ldrd root2, root2_twisted, [root_ptr, #(-8)]
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_kyber_1_23_45_67_opt_m55, %function
.global ntt_kyber_1_23_45_67_opt_m55
ntt_kyber_1_23_45_67_opt_m55:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus  .req r12
        root_ptr .req r11

        .equ modulus_const, -3329
        movw modulus, #:lower16:modulus_const
        ldr  root_ptr, roots_addr

        in_low       .req r0
        in_high      .req r1

        add in_high, in_low, #(4*64)

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        // Layers 1

        load_first_root root0, root0_twisted

        mov lr, #16
                                  // Instructions:    1
                                  // Expected cycles: 1
                                  // Expected IPC:    1.00
                                  //
                                  // Wall time:     0.00s
                                  // User time:     0.00s
                                  //
                                  // ----- cycle (expected) ------>
                                  // 0                        25
                                  // |------------------------|----
        vldrw.U32 q6, [r1]        // *.............................

                                   // ------ cycle (expected) ------>
                                   // 0                        25
                                   // |------------------------|-----
        // vldrw.U32 q6, [r1]      // *..............................

        sub lr, lr, #1
.p2align 2
layer1_loop:
                                       // Instructions:    9
                                       // Expected cycles: 10
                                       // Expected IPC:    0.90
                                       //
                                       // Wall time:     0.06s
                                       // User time:     0.06s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vqrdmulh.S16 q5, q6, r3        // *.............................
        vmul.S16 q1, q6, r2            // ..*...........................
        vldrw.U32 q0, [r0]             // ...*..........................
        vmla.S16 q1, q5, r12           // ....*.........................
        vldrw.U32 q6, [r1]             // .....e........................
        vadd.U16 q5, q0, q1            // ......*.......................
        vstrw.U32 q5, [r0], #16        // .......*......................
        vsub.U16 q0, q0, q1            // ........*.....................
        vstrw.U32 q0, [r1], #16        // .........*....................

                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        // vldrw.u32 q0, [r0]                // .....'..*......'..~......'..~..
        // vldrw.u32 q1, [r1]                // e....'....~....'....~....'.....
        // vmul.s16       q4,  q1, r2        // .....'.*.......'.~.......'.~...
        // vqrdmulh.s16   q1,  q1, r3        // .....*.........~.........~.....
        // vmla.s16       q4,  q1, r12       // .....'...*.....'...~.....'...~.
        // vsub.u16       q1,    q0, q4      // ...~.'.......*.'.......~.'.....
        // vadd.u16       q0,    q0, q4      // .~...'.....*...'.....~...'.....
        // vstrw.u32 q0, [r0], #16           // ..~..'......*..'......~..'.....
        // vstrw.u32 q1, [r1], #16           // ....~'........*'........~'.....

        le lr, layer1_loop
                                       // Instructions:    8
                                       // Expected cycles: 10
                                       // Expected IPC:    0.80
                                       //
                                       // Wall time:     0.01s
                                       // User time:     0.01s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vqrdmulh.S16 q0, q6, r3        // *.............................
        vmul.S16 q1, q6, r2            // ..*...........................
        vldrw.U32 q6, [r0]             // ...*..........................
        vmla.S16 q1, q0, r12           // ....*.........................
        vsub.U16 q0, q6, q1            // ......*.......................
        vstrw.U32 q0, [r1], #16        // .......*......................
        vadd.U16 q0, q6, q1            // ........*.....................
        vstrw.U32 q0, [r0], #16        // .........*....................

                                        // ------ cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|-----
        // vqrdmulh.S16 q5, q6, r3      // *..............................
        // vmul.S16 q1, q6, r2          // ..*............................
        // vldrw.U32 q0, [r0]           // ...*...........................
        // vmla.S16 q1, q5, r12         // ....*..........................
        // vadd.U16 q5, q0, q1          // ........*......................
        // vstrw.U32 q5, [r0], #16      // .........*.....................
        // vsub.U16 q0, q0, q1          // ......*........................
        // vstrw.U32 q0, [r1], #16      // .......*.......................

        .unreq in_high
        .unreq in_low

        in .req r0
        sub in, in, #(4*64)

        // Layers 2,3

        count .req r1
        mov count, #2

out_start:
        load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted

        mov lr, #4
                                             // Instructions:    5
                                             // Expected cycles: 6
                                             // Expected IPC:    0.83
                                             //
                                             // Wall time:     0.00s
                                             // User time:     0.00s
                                             //
                                             // ----- cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|----
        vldrw.U32 q0, [r0, #(4*3*16)]        // *.............................
        vqrdmulh.S16 q1, q0, r3              // .*............................
        vmul.S16 q0, q0, r2                  // ...*..........................
        vldrw.U32 q2, [r0, #(4*1*16)]        // ....*.........................
        vmla.S16 q0, q1, r12                 // .....*........................

                                              // ------ cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|-----
        // vldrw.U32 q7, [r0, #(4*3*16)]      // *..............................
        // vldrw.U32 q2, [r0, #(4*1*16)]      // ....*..........................
        // vqrdmulh.S16 q1, q7, r3            // .*.............................
        // vmul.S16 q0, q7, r2                // ...*...........................
        // vmla.S16 q0, q1, r12               // .....*.........................

        sub lr, lr, #1
.p2align 2
layer23_loop:
                                                  // Instructions:    28
                                                  // Expected cycles: 28
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     2.97s
                                                  // User time:     2.97s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vadd.U16 q4, q2, q0                       // *.............................
        vqrdmulh.S16 q3, q4, r5                   // .*............................
        vldrw.U32 q5, [r0, #(4*2*16)]             // ..*...........................
        vmul.S16 q1, q5, r2                       // ...*..........................
        vldrw.U32 q7, [r0, #(4*3*16)]             // ....e.........................
        vqrdmulh.S16 q5, q5, r3                   // .....*........................
        vsub.U16 q6, q2, q0                       // ......*.......................
        vmla.S16 q1, q5, r12                      // .......*......................
        vldrw.U32 q5, [r0]                        // ........*.....................
        vmul.S16 q0, q4, r4                       // .........*....................
        vsub.U16 q4, q5, q1                       // ..........*...................
        vmla.S16 q0, q3, r12                      // ...........*..................
        vadd.U16 q1, q5, q1                       // ............*.................
        vqrdmulh.S16 q5, q6, r7                   // .............*................
        vadd.U16 q3, q1, q0                       // ..............*...............
        vldrw.U32 q2, [r0, #(4*1*16)]             // ...............e..............
        vmul.S16 q6, q6, r6                       // ................*.............
        vstrw.U32 q3, [r0], #16                   // .................*............
        vmla.S16 q6, q5, r12                      // ..................*...........
        vsub.U16 q0, q1, q0                       // ...................*..........
        vqrdmulh.S16 q1, q7, r3                   // ....................e.........
        vsub.U16 q3, q4, q6                       // .....................*........
        vstrw.U32 q0, [r0, #(4*1*16 - 16)]        // ......................*.......
        vadd.U16 q4, q4, q6                       // .......................*......
        vmul.S16 q0, q7, r2                       // ........................e.....
        vstrw.U32 q3, [r0, #(4*3*16 - 16)]        // .........................*....
        vmla.S16 q0, q1, r12                      // ..........................e...
        vstrw.U32 q4, [r0, #(4*2*16 - 16)]        // ...........................*..

                                                   // ---------------- cycle (expected) ----------------->
                                                   // 0                        25                       50
                                                   // |------------------------|------------------------|-
        // vldrw.u32 q0, [r0]                      // ....~...................'.......*...................
        // vldrw.u32 q1, [r0, #(4*1*16)]           // ...........e............'..............~............
        // vldrw.u32 q2, [r0, #(4*2*16)]           // ........................'.*.........................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // e.......................'...~.......................
        // vmul.s16       q4,  q2, r2              // ........................'..*........................
        // vqrdmulh.s16   q2,  q2, r3              // .~......................'....*......................
        // vmla.s16       q4,  q2, r12             // ...~....................'......*....................
        // vsub.u16       q2,    q0, q4            // ......~.................'.........*.................
        // vadd.u16       q0,    q0, q4            // ........~...............'...........*...............
        // vmul.s16       q4,  q3, r2              // ....................e...'.......................~...
        // vqrdmulh.s16   q3,  q3, r3              // ................e.......'...................~.......
        // vmla.s16       q4,  q3, r12             // ......................e.'.........................~.
        // vsub.u16       q3,    q1, q4            // ..~.....................'.....*.....................
        // vadd.u16       q1,    q1, q4            // ........................*...........................
        // vmul.s16       q4,  q1, r4              // .....~..................'........*..................
        // vqrdmulh.s16   q1,  q1, r5              // ........................'*..........................
        // vmla.s16       q4,  q1, r12             // .......~................'..........*................
        // vsub.u16       q1,    q0, q4            // ...............~........'..................*........
        // vadd.u16       q0,    q0, q4            // ..........~.............'.............*.............
        // vmul.s16       q4,  q3, r6              // ............~...........'...............*...........
        // vqrdmulh.s16   q3,  q3, r7              // .........~..............'............*..............
        // vmla.s16       q4,  q3, r12             // ..............~.........'.................*.........
        // vsub.u16       q3,    q2, q4            // .................~......'....................*......
        // vadd.u16       q2,    q2, q4            // ...................~....'......................*....
        // vstrw.u32 q0, [r0], #16                 // .............~..........'................*..........
        // vstrw.u32 q1, [r0, #(4*1*16 - 16)]      // ..................~.....'.....................*.....
        // vstrw.u32 q2, [r0, #(4*2*16 - 16)]      // .......................~'..........................*
        // vstrw.u32 q3, [r0, #(4*3*16 - 16)]      // .....................~..'........................*..

        le lr, layer23_loop
                                                  // Instructions:    23
                                                  // Expected cycles: 24
                                                  // Expected IPC:    0.96
                                                  //
                                                  // Wall time:     0.12s
                                                  // User time:     0.12s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vldrw.U32 q1, [r0, #(4*2*16)]             // *.............................
        vqrdmulh.S16 q6, q1, r3                   // .*............................
        vldrw.U32 q7, [r0]                        // ..*...........................
        vmul.S16 q1, q1, r2                       // ...*..........................
        vsub.U16 q4, q2, q0                       // ....*.........................
        vmla.S16 q1, q6, r12                      // .....*........................
        vadd.U16 q5, q2, q0                       // ......*.......................
        vmul.S16 q6, q4, r6                       // .......*......................
        vadd.U16 q3, q7, q1                       // ........*.....................
        vqrdmulh.S16 q0, q4, r7                   // .........*....................
        vsub.U16 q4, q7, q1                       // ..........*...................
        vmla.S16 q6, q0, r12                      // ...........*..................
        vsub.U16 q0, q4, q6                       // .............*................
        vmul.S16 q2, q5, r4                       // ..............*...............
        vstrw.U32 q0, [r0, #(4*3*16 - 16)]        // ...............*..............
        vqrdmulh.S16 q5, q5, r5                   // ................*.............
        vadd.U16 q7, q4, q6                       // .................*............
        vmla.S16 q2, q5, r12                      // ..................*...........
        vstrw.U32 q7, [r0, #(4*2*16 - 16)]        // ...................*..........
        vsub.U16 q4, q3, q2                       // ....................*.........
        vstrw.U32 q4, [r0, #(4*1*16 - 16)]        // .....................*........
        vadd.U16 q0, q3, q2                       // ......................*.......
        vstrw.U32 q0, [r0], #16                   // .......................*......

                                                   // ------ cycle (expected) ------>
                                                   // 0                        25
                                                   // |------------------------|-----
        // vadd.U16 q4, q2, q0                     // ......*........................
        // vqrdmulh.S16 q3, q4, r5                 // ................*..............
        // vldrw.U32 q5, [r0, #(4*2*16)]           // *..............................
        // vmul.S16 q1, q5, r2                     // ...*...........................
        // vqrdmulh.S16 q5, q5, r3                 // .*.............................
        // vsub.U16 q6, q2, q0                     // ....*..........................
        // vmla.S16 q1, q5, r12                    // .....*.........................
        // vldrw.U32 q5, [r0]                      // ..*............................
        // vmul.S16 q0, q4, r4                     // ..............*................
        // vsub.U16 q4, q5, q1                     // ..........*....................
        // vmla.S16 q0, q3, r12                    // ..................*............
        // vadd.U16 q1, q5, q1                     // ........*......................
        // vqrdmulh.S16 q5, q6, r7                 // .........*.....................
        // vadd.U16 q3, q1, q0                     // ......................*........
        // vmul.S16 q6, q6, r6                     // .......*.......................
        // vstrw.U32 q3, [r0], #16                 // .......................*.......
        // vmla.S16 q6, q5, r12                    // ...........*...................
        // vsub.U16 q0, q1, q0                     // ....................*..........
        // vsub.U16 q3, q4, q6                     // .............*.................
        // vstrw.U32 q0, [r0, #(4*1*16 - 16)]      // .....................*.........
        // vadd.U16 q4, q4, q6                     // .................*.............
        // vstrw.U32 q3, [r0, #(4*3*16 - 16)]      // ...............*...............
        // vstrw.U32 q4, [r0, #(4*2*16 - 16)]      // ...................*...........


        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*128)

        // Layers 4,5

        mov lr, #8
                                         // Instructions:    7
                                         // Expected cycles: 7
                                         // Expected IPC:    1.00
                                         //
                                         // Wall time:     0.00s
                                         // User time:     0.00s
                                         //
                                         // ----- cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|----
        ldrd r6, r1, [r11], #+24         // *.............................
        vldrw.U32 q5, [r0, #48]          // .*............................
        vqrdmulh.S16 q1, q5, r1          // ..*...........................
        ldrd r9, r4, [r11, #(-8)]        // ...*..........................
        vmul.S16 q4, q5, r6              // ....*.........................
        vldrw.U32 q5, [r0, #16]          // .....*........................
        vmla.S16 q4, q1, r12             // ......*.......................

                                          // ------ cycle (expected) ------>
                                          // 0                        25
                                          // |------------------------|-----
        // vldrw.U32 q7, [r0, #48]        // .*.............................
        // vldrw.U32 q5, [r0, #16]        // .....*.........................
        // ldrd r9, r4, [r11, #(-8)]      // ...*...........................
        // ldrd r6, r1, [r11], #+24       // *..............................
        // vmul.S16 q4, q7, r6            // ....*..........................
        // vqrdmulh.S16 q7, q7, r1        // ..*............................
        // vmla.S16 q4, q7, r12           // ......*........................

        sub lr, lr, #1
.p2align 2
layer45_loop:
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     3.07s
                                                 // User time:     3.07s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vsub.U16 q6, q5, q4                      // *..............................
        vqrdmulh.S16 q7, q6, r4                  // .*.............................
        vldrw.U32 q3, [r0, #32]                  // ..*............................
        vqrdmulh.S16 q0, q3, r1                  // ...*...........................
        ldrd r2, r8, [r11, #(-16)]               // ....*..........................
        vmul.S16 q3, q3, r6                      // .....*.........................
        vadd.U16 q1, q5, q4                      // ......*........................
        vmla.S16 q3, q0, r12                     // .......*.......................
        vldrw.U32 q2, [r0]                       // ........*......................
        vmul.S16 q0, q6, r9                      // .........*.....................
        vsub.U16 q5, q2, q3                      // ..........*....................
        vmla.S16 q0, q7, r12                     // ...........*...................
        vadd.U16 q4, q2, q3                      // ............*..................
        vqrdmulh.S16 q7, q1, r8                  // .............*.................
        vadd.U16 q2, q5, q0                      // ..............*................
        vmul.S16 q6, q1, r2                      // ...............*...............
        vsub.U16 q3, q5, q0                      // ................*..............
        vmla.S16 q6, q7, r12                     // .................*.............
        vldrw.U32 q7, [r0, #48]                  // ..................e............
        vadd.U16 q0, q4, q6                      // ...................*...........
        vldrw.U32 q5, [r0, #16]                  // ....................e..........
        vsub.U16 q1, q4, q6                      // .....................*.........
        ldrd r9, r4, [r11, #(-8)]                // ......................e........
        ldrd r6, r1, [r11], #+24                 // .......................e.......
        vst40.U32 {q0, q1, q2, q3}, [r0]         // ........................*......
        vmul.S16 q4, q7, r6                      // .........................e.....
        vst41.U32 {q0, q1, q2, q3}, [r0]         // ..........................*....
        vqrdmulh.S16 q7, q7, r1                  // ...........................e...
        vst42.U32 {q0, q1, q2, q3}, [r0]         // ............................*..
        vmla.S16 q4, q7, r12                     // .............................e.
        vst43.U32 {q0, q1, q2, q3}, [r0]!        // ..............................*

                                                  // ------------ cycle (expected) ------------->
                                                  // 0                        25
                                                  // |------------------------|------------------
        // ldrd r2, r3, [r11], #+24               // .....e.......'......................~.......
        // ldrd r4, r5, [r11, #(-16)]             // .............'...*..........................
        // ldrd r6, r7, [r11, #(-8)]              // ....e........'.....................~........
        // vldrw.u32 q0, [r0]                     // .............'.......*......................
        // vldrw.u32 q1, [r0, #16]                // ..e..........'...................~..........
        // vldrw.u32 q2, [r0, #32]                // .............'.*............................
        // vldrw.u32 q3, [r0, #48]                // e............'.................~............
        // vmul.s16       q4,  q2, r2             // .............'....*.........................
        // vqrdmulh.s16   q2,  q2, r3             // .............'..*...........................
        // vmla.s16       q4,  q2, r12            // .............'......*.......................
        // vsub.u16       q2,    q0, q4           // .............'.........*....................
        // vadd.u16       q0,    q0, q4           // .............'...........*..................
        // vmul.s16       q4,  q3, r2             // .......e.....'........................~.....
        // vqrdmulh.s16   q3,  q3, r3             // .........e...'..........................~...
        // vmla.s16       q4,  q3, r12            // ...........e.'............................~.
        // vsub.u16       q3,    q1, q4           // .............*..............................
        // vadd.u16       q1,    q1, q4           // .............'.....*........................
        // vmul.s16       q4,  q1, r4             // .............'..............*...............
        // vqrdmulh.s16   q1,  q1, r5             // .............'............*.................
        // vmla.s16       q4,  q1, r12            // .............'................*.............
        // vsub.u16       q1,    q0, q4           // ...~.........'....................*.........
        // vadd.u16       q0,    q0, q4           // .~...........'..................*...........
        // vmul.s16       q4,  q3, r6             // .............'........*.....................
        // vqrdmulh.s16   q3,  q3, r7             // .............'*.............................
        // vmla.s16       q4,  q3, r12            // .............'..........*...................
        // vsub.u16       q3,    q2, q4           // .............'...............*..............
        // vadd.u16       q2,    q2, q4           // .............'.............*................
        // vst40.u32 {q0, q1, q2, q3}, [r0]       // ......~......'.......................*......
        // vst41.u32 {q0, q1, q2, q3}, [r0]       // ........~....'.........................*....
        // vst42.u32 {q0, q1, q2, q3}, [r0]       // ..........~..'...........................*..
        // vst43.u32 {q0, q1, q2, q3}, [r0]!      // ............~'.............................*

        le lr, layer45_loop
                                                 // Instructions:    24
                                                 // Expected cycles: 31
                                                 // Expected IPC:    0.77
                                                 //
                                                 // Wall time:     0.11s
                                                 // User time:     0.11s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vldrw.U32 q1, [r0, #32]                  // *..............................
        vmul.S16 q0, q1, r6                      // .*.............................
        vsub.U16 q7, q5, q4                      // ..*............................
        vqrdmulh.S16 q6, q1, r1                  // ...*...........................
        ldrd r6, r1, [r11, #(-16)]               // ....*..........................
        vmla.S16 q0, q6, r12                     // .....*.........................
        vadd.U16 q4, q5, q4                      // ......*........................
        vqrdmulh.S16 q6, q7, r4                  // .......*.......................
        vldrw.U32 q3, [r0]                       // ........*......................
        vmul.S16 q5, q7, r9                      // .........*.....................
        vadd.U16 q1, q3, q0                      // ..........*....................
        vmla.S16 q5, q6, r12                     // ...........*...................
        vsub.U16 q7, q3, q0                      // ............*..................
        vqrdmulh.S16 q2, q4, r1                  // .............*.................
        vadd.U16 q3, q7, q5                      // ..............*................
        vmul.S16 q0, q4, r6                      // ...............*...............
        vsub.U16 q4, q7, q5                      // ................*..............
        vmla.S16 q0, q2, r12                     // .................*.............
        vsub.U16 q2, q1, q0                      // ...................*...........
        vadd.U16 q1, q1, q0                      // .....................*.........
        vst40.U32 {q1, q2, q3, q4}, [r0]         // ........................*......
        vst41.U32 {q1, q2, q3, q4}, [r0]         // ..........................*....
        vst42.U32 {q1, q2, q3, q4}, [r0]         // ............................*..
        vst43.U32 {q1, q2, q3, q4}, [r0]!        // ..............................*

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vsub.U16 q6, q5, q4                    // ..*............................
        // vqrdmulh.S16 q7, q6, r4                // .......*.......................
        // vldrw.U32 q3, [r0, #32]                // *..............................
        // vqrdmulh.S16 q0, q3, r1                // ...*...........................
        // ldrd r2, r8, [r11, #(-16)]             // ....*..........................
        // vmul.S16 q3, q3, r6                    // .*.............................
        // vadd.U16 q1, q5, q4                    // ......*........................
        // vmla.S16 q3, q0, r12                   // .....*.........................
        // vldrw.U32 q2, [r0]                     // ........*......................
        // vmul.S16 q0, q6, r9                    // .........*.....................
        // vsub.U16 q5, q2, q3                    // ............*..................
        // vmla.S16 q0, q7, r12                   // ...........*...................
        // vadd.U16 q4, q2, q3                    // ..........*....................
        // vqrdmulh.S16 q7, q1, r8                // .............*.................
        // vadd.U16 q2, q5, q0                    // ..............*................
        // vmul.S16 q6, q1, r2                    // ...............*...............
        // vsub.U16 q3, q5, q0                    // ................*..............
        // vmla.S16 q6, q7, r12                   // .................*.............
        // vadd.U16 q0, q4, q6                    // .....................*.........
        // vsub.U16 q1, q4, q6                    // ...................*...........
        // vst40.U32 {q0, q1, q2, q3}, [r0]       // ........................*......
        // vst41.U32 {q0, q1, q2, q3}, [r0]       // ..........................*....
        // vst42.U32 {q0, q1, q2, q3}, [r0]       // ............................*..
        // vst43.U32 {q0, q1, q2, q3}, [r0]!      // ..............................*


        sub in, in, #(4*128)

        // Layers 6,7

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6

        mov lr, #8
                                              // Instructions:    9
                                              // Expected cycles: 12
                                              // Expected IPC:    0.75
                                              //
                                              // Wall time:     0.01s
                                              // User time:     0.01s
                                              //
                                              // ----- cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|----
        vldrh.U16 q4, [r11], #+96             // *.............................
        vldrw.U32 q1, [r0, #32]               // ..*...........................
        vmul.S16 q6, q1, q4                   // ...*..........................
        vldrh.U16 q3, [r11, #(+16-96)]        // ....*.........................
        vqrdmulh.S16 q5, q1, q3               // .....*........................
        vldrw.U32 q0, [r0, #48]               // ......*.......................
        vmla.S16 q6, q5, r12                  // .......*......................
        vmul.S16 q5, q0, q4                   // .........*....................
        vqrdmulh.S16 q4, q0, q3               // ...........*..................

                                               // ------ cycle (expected) ------>
                                               // 0                        25
                                               // |------------------------|-----
        // vldrw.U32 q6, [r0, #32]             // ..*............................
        // vldrh.U16 q5, [r11], #+96           // *..............................
        // vldrh.U16 q4, [r11, #(+16-96)]      // ....*..........................
        // vqrdmulh.S16 q7, q6, q4             // .....*.........................
        // vmul.S16 q6, q6, q5                 // ...*...........................
        // vmla.S16 q6, q7, r12                // .......*.......................
        // vldrw.U32 q7, [r0, #48]             // ......*........................
        // vqrdmulh.S16 q4, q7, q4             // ...........*...................
        // vmul.S16 q5, q7, q5                 // .........*.....................

        sub lr, lr, #1
.p2align 2
layer67_loop:
                                                 // Instructions:    34
                                                 // Expected cycles: 34
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     5.61s
                                                 // User time:     5.61s
                                                 //
                                                 // ------- cycle (expected) -------->
                                                 // 0                        25
                                                 // |------------------------|--------
        vmla.S16 q5, q4, r12                     // *.................................
        vldrw.U32 q4, [r0, #16]                  // .*................................
        vadd.U16 q0, q4, q5                      // ..*...............................
        vldrh.U16 q1, [r11, #(32 - 96)]          // ...*..............................
        vmul.S16 q1, q0, q1                      // ....*.............................
        vsub.U16 q3, q4, q5                      // .....*............................
        vldrh.U16 q5, [r11, #(48 - 96)]          // ......*...........................
        vqrdmulh.S16 q2, q0, q5                  // .......*..........................
        vldrh.U16 q0, [r11, #(80-96)]            // ........*.........................
        vqrdmulh.S16 q5, q3, q0                  // .........*........................
        vldrw.U32 q4, [r0]                       // ..........*.......................
        vmla.S16 q1, q2, r12                     // ...........*......................
        vadd.U16 q2, q4, q6                      // ............*.....................
        vldrh.U16 q7, [r11, #(64-96)]            // .............*....................
        vadd.U16 q0, q2, q1                      // ..............*...................
        vmul.S16 q3, q3, q7                      // ...............*..................
        vsub.U16 q4, q4, q6                      // ................*.................
        vmla.S16 q3, q5, r12                     // .................*................
        vsub.U16 q1, q2, q1                      // ..................*...............
        vldrw.U32 q6, [r0, #32]                  // ...................e..............
        vadd.U16 q2, q4, q3                      // ....................*.............
        vldrh.U16 q5, [r11], #+96                // .....................e............
        vsub.U16 q3, q4, q3                      // ......................*...........
        vldrh.U16 q4, [r11, #(+16-96)]           // .......................e..........
        vqrdmulh.S16 q7, q6, q4                  // ........................e.........
        vst40.U32 {q0, q1, q2, q3}, [r0]         // .........................*........
        vmul.S16 q6, q6, q5                      // ..........................e.......
        vst41.U32 {q0, q1, q2, q3}, [r0]         // ...........................*......
        vmla.S16 q6, q7, r12                     // ............................e.....
        vldrw.U32 q7, [r0, #48]                  // .............................e....
        vqrdmulh.S16 q4, q7, q4                  // ..............................e...
        vst42.U32 {q0, q1, q2, q3}, [r0]         // ...............................*..
        vmul.S16 q5, q7, q5                      // ................................e.
        vst43.U32 {q0, q1, q2, q3}, [r0]!        // .................................*

                                                        // --------------- cycle (expected) --------------->
                                                        // 0                        25
                                                        // |------------------------|-----------------------
        // vldrw.u32 q0, [r0]                           // ...............'.........*.......................
        // vldrw.u32 q1, [r0, #16]                      // ...............'*................................
        // vldrw.u32 q2, [r0, #32]                      // e..............'..................~..............
        // vldrw.u32 q3, [r0, #48]                      // ..........e....'............................~....
        // vldrh.u16 q5,         [r11], #+96            // ..e............'....................~............
        // vldrh.u16 q6, [r11, #(+16-96)]               // ....e..........'......................~..........
        // vmul.s16       q4,  q2, q5                   // .......e.......'.........................~.......
        // vqrdmulh.s16   q2,  q2, q6                   // .....e.........'.......................~.........
        // vmla.s16       q4,  q2, r12                  // .........e.....'...........................~.....
        // vsub.u16       q2,    q0, q4                 // ...............'...............*.................
        // vadd.u16       q0,    q0, q4                 // ...............'...........*.....................
        // vmul.s16       q4,  q3, q5                   // .............e.'...............................~.
        // vqrdmulh.s16   q3,  q3, q6                   // ...........e...'.............................~...
        // vmla.s16       q4,  q3, r12                  // ...............*.................................
        // vsub.u16       q3,    q1, q4                 // ...............'....*............................
        // vadd.u16       q1,    q1, q4                 // ...............'.*...............................
        // vldrh.u16 q5,         [r11, #(32 - 96)]      // ...............'..*..............................
        // vldrh.u16 q6, [r11, #(48 - 96)]              // ...............'.....*...........................
        // vmul.s16       q4,  q1, q5                   // ...............'...*.............................
        // vqrdmulh.s16   q1,  q1, q6                   // ...............'......*..........................
        // vmla.s16       q4,  q1, r12                  // ...............'..........*......................
        // vsub.u16       q1,    q0, q4                 // ...............'.................*...............
        // vadd.u16       q0,    q0, q4                 // ...............'.............*...................
        // vldrh.u16 q5,         [r11, #(64-96)]        // ...............'............*....................
        // vldrh.u16 q6, [r11, #(80-96)]                // ...............'.......*.........................
        // vmul.s16       q4,  q3, q5                   // ...............'..............*..................
        // vqrdmulh.s16   q3,  q3, q6                   // ...............'........*........................
        // vmla.s16       q4,  q3, r12                  // ...............'................*................
        // vsub.u16       q3,    q2, q4                 // ...~...........'.....................*...........
        // vadd.u16       q2,    q2, q4                 // .~.............'...................*.............
        // vst40.u32 {q0, q1, q2, q3}, [r0]             // ......~........'........................*........
        // vst41.u32 {q0, q1, q2, q3}, [r0]             // ........~......'..........................*......
        // vst42.u32 {q0, q1, q2, q3}, [r0]             // ............~..'..............................*..
        // vst43.u32 {q0, q1, q2, q3}, [r0]!            // ..............~'................................*

        le lr, layer67_loop
                                                 // Instructions:    25
                                                 // Expected cycles: 32
                                                 // Expected IPC:    0.78
                                                 //
                                                 // Wall time:     0.19s
                                                 // User time:     0.19s
                                                 //
                                                 // ------ cycle (expected) ------->
                                                 // 0                        25
                                                 // |------------------------|------
        vldrw.U32 q2, [r0, #16]                  // *...............................
        vmla.S16 q5, q4, r12                     // .*..............................
        vldrh.U16 q7, [r11, #(80-96)]            // ..*.............................
        vsub.U16 q4, q2, q5                      // ...*............................
        vqrdmulh.S16 q7, q4, q7                  // ....*...........................
        vldrh.U16 q0, [r11, #(64-96)]            // .....*..........................
        vmul.S16 q0, q4, q0                      // ......*.........................
        vldrh.U16 q1, [r11, #(32 - 96)]          // .......*........................
        vmla.S16 q0, q7, r12                     // ........*.......................
        vldrh.U16 q4, [r11, #(48 - 96)]          // .........*......................
        vadd.U16 q2, q2, q5                      // ..........*.....................
        vldrw.U32 q7, [r0]                       // ...........*....................
        vsub.U16 q3, q7, q6                      // ............*...................
        vmul.S16 q1, q2, q1                      // .............*..................
        vsub.U16 q5, q3, q0                      // ..............*.................
        vqrdmulh.S16 q2, q2, q4                  // ...............*................
        vadd.U16 q4, q3, q0                      // ................*...............
        vmla.S16 q1, q2, r12                     // .................*..............
        vadd.U16 q0, q7, q6                      // ..................*.............
        vsub.U16 q3, q0, q1                      // ....................*...........
        vadd.U16 q2, q0, q1                      // ......................*.........
        vst40.U32 {q2, q3, q4, q5}, [r0]         // .........................*......
        vst41.U32 {q2, q3, q4, q5}, [r0]         // ...........................*....
        vst42.U32 {q2, q3, q4, q5}, [r0]         // .............................*..
        vst43.U32 {q2, q3, q4, q5}, [r0]!        // ...............................*

                                                  // ------ cycle (expected) ------->
                                                  // 0                        25
                                                  // |------------------------|------
        // vmla.S16 q5, q4, r12                   // .*..............................
        // vldrw.U32 q4, [r0, #16]                // *...............................
        // vadd.U16 q0, q4, q5                    // ..........*.....................
        // vldrh.U16 q1, [r11, #(32 - 96)]        // .......*........................
        // vmul.S16 q1, q0, q1                    // .............*..................
        // vsub.U16 q3, q4, q5                    // ...*............................
        // vldrh.U16 q5, [r11, #(48 - 96)]        // .........*......................
        // vqrdmulh.S16 q2, q0, q5                // ...............*................
        // vldrh.U16 q0, [r11, #(80-96)]          // ..*.............................
        // vqrdmulh.S16 q5, q3, q0                // ....*...........................
        // vldrw.U32 q4, [r0]                     // ...........*....................
        // vmla.S16 q1, q2, r12                   // .................*..............
        // vadd.U16 q2, q4, q6                    // ..................*.............
        // vldrh.U16 q7, [r11, #(64-96)]          // .....*..........................
        // vadd.U16 q0, q2, q1                    // ......................*.........
        // vmul.S16 q3, q3, q7                    // ......*.........................
        // vsub.U16 q4, q4, q6                    // ............*...................
        // vmla.S16 q3, q5, r12                   // ........*.......................
        // vsub.U16 q1, q2, q1                    // ....................*...........
        // vadd.U16 q2, q4, q3                    // ................*...............
        // vsub.U16 q3, q4, q3                    // ..............*.................
        // vst40.U32 {q0, q1, q2, q3}, [r0]       // .........................*......
        // vst41.U32 {q0, q1, q2, q3}, [r0]       // ...........................*....
        // vst42.U32 {q0, q1, q2, q3}, [r0]       // .............................*..
        // vst43.U32 {q0, q1, q2, q3}, [r0]!      // ...............................*


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
