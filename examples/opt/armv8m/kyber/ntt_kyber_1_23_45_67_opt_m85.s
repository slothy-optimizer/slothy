
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots:
#include "ntt_kyber_1_23_45_67_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s16       \dst,  \src, \const
        vqrdmulh.s16   \src,  \src, \const_twisted
        vmla.s16       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_twisted
        mulmod tmp, \b, \root, \root_twisted
        vsub.u16       \b,    \a, tmp
        vadd.u16       \a,    \a, tmp
.endm

.macro load_first_root root0, root0_twisted
        ldrd root0, root0_twisted, [root_ptr], #+8
.endm

.macro load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted
        ldrd root0, root0_twisted, [root_ptr], #+24
        ldrd root1, root1_twisted, [root_ptr, #(-16)]
        ldrd root2, root2_twisted, [root_ptr, #(-8)]
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_kyber_1_23_45_67_opt_m85, %function
.global ntt_kyber_1_23_45_67_opt_m85
ntt_kyber_1_23_45_67_opt_m85:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus  .req r12
        root_ptr .req r11

        .equ modulus_const, -3329
        movw modulus, #:lower16:modulus_const
        ldr  root_ptr, roots_addr

        in_low       .req r0
        in_high      .req r1

        add in_high, in_low, #(4*64)

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        // Layers 1

        load_first_root root0, root0_twisted

        mov lr, #16
                                       // Instructions:    2
                                       // Expected cycles: 2
                                       // Expected IPC:    1.00
                                       //
                                       // Wall time:     0.00s
                                       // User time:     0.00s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vldrw.U32 q0, [r1]             // *.............................
        vqrdmulh.S16 q6, q0, r3        // .*............................

                                        // ------ cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|-----
        // vldrw.U32 q0, [r1]           // *..............................
        // vqrdmulh.S16 q6, q0, r3      // .*.............................

        sub lr, lr, #1
.p2align 2
layer1_loop:
                                       // Instructions:    9
                                       // Expected cycles: 9
                                       // Expected IPC:    1.00
                                       //
                                       // Wall time:     0.06s
                                       // User time:     0.06s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vmul.S16 q7, q0, r2            // *.............................
        vldrw.U32 q5, [r0]             // .*............................
        vmla.S16 q7, q6, r12           // ..*...........................
        vldrw.U32 q0, [r1]             // ...e..........................
        vadd.U16 q4, q5, q7            // ....*.........................
        vqrdmulh.S16 q6, q0, r3        // .....e........................
        vstrw.U32 q4, [r0], #16        // ......*.......................
        vsub.U16 q5, q5, q7            // .......*......................
        vstrw.U32 q5, [r1], #16        // ........*.....................

                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        // vldrw.u32 q0, [r0]                // ......'*.......'~.......'~.....
        // vldrw.u32 q1, [r1]                // e.....'..~.....'..~.....'..~...
        // vmul.s16       q4,  q1, r2        // ......*........~........~......
        // vqrdmulh.s16   q1,  q1, r3        // ..e...'....~...'....~...'....~.
        // vmla.s16       q4,  q1, r12       // ......'.*......'.~......'.~....
        // vsub.u16       q1,    q0, q4      // ....~.'......*.'......~.'......
        // vadd.u16       q0,    q0, q4      // .~....'...*....'...~....'...~..
        // vstrw.u32 q0, [r0], #16           // ...~..'.....*..'.....~..'......
        // vstrw.u32 q1, [r1], #16           // .....~'.......*'.......~'......

        le lr, layer1_loop
                                       // Instructions:    7
                                       // Expected cycles: 8
                                       // Expected IPC:    0.88
                                       //
                                       // Wall time:     0.01s
                                       // User time:     0.01s
                                       //
                                       // ----- cycle (expected) ------>
                                       // 0                        25
                                       // |------------------------|----
        vmul.S16 q7, q0, r2            // *.............................
        vmla.S16 q7, q6, r12           // ..*...........................
        vldrw.U32 q0, [r0]             // ...*..........................
        vsub.U16 q5, q0, q7            // ....*.........................
        vstrw.U32 q5, [r1], #16        // .....*........................
        vadd.U16 q5, q0, q7            // ......*.......................
        vstrw.U32 q5, [r0], #16        // .......*......................

                                        // ------ cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|-----
        // vmul.S16 q7, q0, r2          // *..............................
        // vldrw.U32 q5, [r0]           // ...*...........................
        // vmla.S16 q7, q6, r12         // ..*............................
        // vadd.U16 q4, q5, q7          // ......*........................
        // vstrw.U32 q4, [r0], #16      // .......*.......................
        // vsub.U16 q5, q5, q7          // ....*..........................
        // vstrw.U32 q5, [r1], #16      // .....*.........................

        .unreq in_high
        .unreq in_low

        in .req r0
        sub in, in, #(4*64)

        // Layers 2,3

        count .req r1
        mov count, #2

out_start:
        load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted

        mov lr, #4
                                             // Instructions:    2
                                             // Expected cycles: 2
                                             // Expected IPC:    1.00
                                             //
                                             // Wall time:     0.00s
                                             // User time:     0.00s
                                             //
                                             // ----- cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|----
        vldrw.U32 q4, [r0, #(4*2*16)]        // *.............................
        vmul.S16 q7, q4, r2                  // .*............................

                                              // ------ cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|-----
        // vldrw.U32 q4, [r0, #(4*2*16)]      // *..............................
        // vmul.S16 q7, q4, r2                // .*.............................

        sub lr, lr, #1
.p2align 2
layer23_loop:
                                                  // Instructions:    28
                                                  // Expected cycles: 28
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     2.56s
                                                  // User time:     2.56s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vqrdmulh.S16 q0, q4, r3                   // *.............................
        vldrw.U32 q4, [r0, #(4*3*16)]             // .*............................
        vmul.S16 q5, q4, r2                       // ..*...........................
        vldrw.U32 q1, [r0, #(4*1*16)]             // ...*..........................
        vqrdmulh.S16 q4, q4, r3                   // ....*.........................
        vldrw.U32 q6, [r0]                        // .....*........................
        vmla.S16 q5, q4, r12                      // ......*.......................
        vldrw.U32 q4, [r0, #(4*2*16)]             // .......e......................
        vmla.S16 q7, q0, r12                      // ........*.....................
        vadd.U16 q3, q1, q5                       // .........*....................
        vmul.S16 q0, q3, r4                       // ..........*...................
        vsub.U16 q2, q6, q7                       // ...........*..................
        vqrdmulh.S16 q3, q3, r5                   // ............*.................
        vsub.U16 q5, q1, q5                       // .............*................
        vmla.S16 q0, q3, r12                      // ..............*...............
        vadd.U16 q6, q6, q7                       // ...............*..............
        vqrdmulh.S16 q1, q5, r7                   // ................*.............
        vadd.U16 q3, q6, q0                       // .................*............
        vmul.S16 q5, q5, r6                       // ..................*...........
        vsub.U16 q0, q6, q0                       // ...................*..........
        vmla.S16 q5, q1, r12                      // ....................*.........
        vstrw.U32 q0, [r0, #(4*1*16 - 16)]        // .....................*........
        vadd.U16 q1, q2, q5                       // ......................*.......
        vstrw.U32 q3, [r0], #16                   // .......................*......
        vmul.S16 q7, q4, r2                       // ........................e.....
        vstrw.U32 q1, [r0, #(4*2*16 - 16)]        // .........................*....
        vsub.U16 q1, q2, q5                       // ..........................*...
        vstrw.U32 q1, [r0, #(4*3*16 - 16)]        // ...........................*..

                                                   // --------------- cycle (expected) --------------->
                                                   // 0                        25
                                                   // |------------------------|-----------------------
        // vldrw.u32 q0, [r0]                      // .....................'....*......................
        // vldrw.u32 q1, [r0, #(4*1*16)]           // .....................'..*........................
        // vldrw.u32 q2, [r0, #(4*2*16)]           // e....................'......~....................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // .....................'*..........................
        // vmul.s16       q4,  q2, r2              // .................e...'.......................~...
        // vqrdmulh.s16   q2,  q2, r3              // .....................*...........................
        // vmla.s16       q4,  q2, r12             // .~...................'.......*...................
        // vsub.u16       q2,    q0, q4            // ....~................'..........*................
        // vadd.u16       q0,    q0, q4            // ........~............'..............*............
        // vmul.s16       q4,  q3, r2              // .....................'.*.........................
        // vqrdmulh.s16   q3,  q3, r3              // .....................'...*.......................
        // vmla.s16       q4,  q3, r12             // .....................'.....*.....................
        // vsub.u16       q3,    q1, q4            // ......~..............'............*..............
        // vadd.u16       q1,    q1, q4            // ..~..................'........*..................
        // vmul.s16       q4,  q1, r4              // ...~.................'.........*.................
        // vqrdmulh.s16   q1,  q1, r5              // .....~...............'...........*...............
        // vmla.s16       q4,  q1, r12             // .......~.............'.............*.............
        // vsub.u16       q1,    q0, q4            // ............~........'..................*........
        // vadd.u16       q0,    q0, q4            // ..........~..........'................*..........
        // vmul.s16       q4,  q3, r6              // ...........~.........'.................*.........
        // vqrdmulh.s16   q3,  q3, r7              // .........~...........'...............*...........
        // vmla.s16       q4,  q3, r12             // .............~.......'...................*.......
        // vsub.u16       q3,    q2, q4            // ...................~.'.........................*.
        // vadd.u16       q2,    q2, q4            // ...............~.....'.....................*.....
        // vstrw.u32 q0, [r0], #16                 // ................~....'......................*....
        // vstrw.u32 q1, [r0, #(4*1*16 - 16)]      // ..............~......'....................*......
        // vstrw.u32 q2, [r0, #(4*2*16 - 16)]      // ..................~..'........................*..
        // vstrw.u32 q3, [r0, #(4*3*16 - 16)]      // ....................~'..........................*

        le lr, layer23_loop
                                                  // Instructions:    26
                                                  // Expected cycles: 27
                                                  // Expected IPC:    0.96
                                                  //
                                                  // Wall time:     0.13s
                                                  // User time:     0.13s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vqrdmulh.S16 q3, q4, r3                   // *.............................
        vldrw.U32 q5, [r0, #(4*3*16)]             // .*............................
        vmul.S16 q4, q5, r2                       // ..*...........................
        vqrdmulh.S16 q5, q5, r3                   // ....*.........................
        vldrw.U32 q0, [r0]                        // .....*........................
        vmla.S16 q4, q5, r12                      // ......*.......................
        vldrw.U32 q6, [r0, #(4*1*16)]             // .......*......................
        vmla.S16 q7, q3, r12                      // ........*.....................
        vadd.U16 q5, q6, q4                       // .........*....................
        vmul.S16 q3, q5, r4                       // ..........*...................
        vsub.U16 q4, q6, q4                       // ...........*..................
        vqrdmulh.S16 q5, q5, r5                   // ............*.................
        vsub.U16 q1, q0, q7                       // .............*................
        vmla.S16 q3, q5, r12                      // ..............*...............
        vadd.U16 q5, q0, q7                       // ...............*..............
        vmul.S16 q0, q4, r6                       // ................*.............
        vadd.U16 q6, q5, q3                       // .................*............
        vqrdmulh.S16 q4, q4, r7                   // ..................*...........
        vsub.U16 q5, q5, q3                       // ...................*..........
        vstrw.U32 q5, [r0, #(4*1*16 - 16)]        // ....................*.........
        vmla.S16 q0, q4, r12                      // .....................*........
        vstrw.U32 q6, [r0], #16                   // ......................*.......
        vadd.U16 q5, q1, q0                       // .......................*......
        vstrw.U32 q5, [r0, #(4*2*16 - 16)]        // ........................*.....
        vsub.U16 q5, q1, q0                       // .........................*....
        vstrw.U32 q5, [r0, #(4*3*16 - 16)]        // ..........................*...

                                                   // ------ cycle (expected) ------>
                                                   // 0                        25
                                                   // |------------------------|-----
        // vqrdmulh.S16 q0, q4, r3                 // *..............................
        // vldrw.U32 q4, [r0, #(4*3*16)]           // .*.............................
        // vmul.S16 q5, q4, r2                     // ..*............................
        // vldrw.U32 q1, [r0, #(4*1*16)]           // .......*.......................
        // vqrdmulh.S16 q4, q4, r3                 // ....*..........................
        // vldrw.U32 q6, [r0]                      // .....*.........................
        // vmla.S16 q5, q4, r12                    // ......*........................
        // vmla.S16 q7, q0, r12                    // ........*......................
        // vadd.U16 q3, q1, q5                     // .........*.....................
        // vmul.S16 q0, q3, r4                     // ..........*....................
        // vsub.U16 q2, q6, q7                     // .............*.................
        // vqrdmulh.S16 q3, q3, r5                 // ............*..................
        // vsub.U16 q5, q1, q5                     // ...........*...................
        // vmla.S16 q0, q3, r12                    // ..............*................
        // vadd.U16 q6, q6, q7                     // ...............*...............
        // vqrdmulh.S16 q1, q5, r7                 // ..................*............
        // vadd.U16 q3, q6, q0                     // .................*.............
        // vmul.S16 q5, q5, r6                     // ................*..............
        // vsub.U16 q0, q6, q0                     // ...................*...........
        // vmla.S16 q5, q1, r12                    // .....................*.........
        // vstrw.U32 q0, [r0, #(4*1*16 - 16)]      // ....................*..........
        // vadd.U16 q1, q2, q5                     // .......................*.......
        // vstrw.U32 q3, [r0], #16                 // ......................*........
        // vstrw.U32 q1, [r0, #(4*2*16 - 16)]      // ........................*......
        // vsub.U16 q1, q2, q5                     // .........................*.....
        // vstrw.U32 q1, [r0, #(4*3*16 - 16)]      // ..........................*....


        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*128)

        // Layers 4,5

        mov lr, #8
                                        // Instructions:    7
                                        // Expected cycles: 7
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     0.00s
                                        // User time:     0.00s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        ldrd r9, r6, [r11], #+24        // *.............................
        vldrw.U32 q2, [r0, #32]         // .*............................
        vqrdmulh.S16 q7, q2, r6         // ..*...........................
        vldrw.U32 q0, [r0, #48]         // ...*..........................
        vmul.S16 q2, q2, r9             // ....*.........................
        vldrw.U32 q3, [r0, #16]         // .....*........................
        vmla.S16 q2, q7, r12            // ......*.......................

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vldrw.U32 q0, [r0, #48]       // ...*...........................
        // ldrd r9, r6, [r11], #+24      // *..............................
        // vldrw.U32 q1, [r0, #32]       // .*.............................
        // vmul.S16 q2, q1, r9           // ....*..........................
        // vqrdmulh.S16 q1, q1, r6       // ..*............................
        // vldrw.U32 q3, [r0, #16]       // .....*.........................
        // vmla.S16 q2, q1, r12          // ......*........................

        sub lr, lr, #1
.p2align 2
layer45_loop:
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     3.14s
                                                 // User time:     3.14s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vmul.S16 q6, q0, r9                      // *..............................
        ldrd r5, r2, [r11, #(-8)]                // .*.............................
        vqrdmulh.S16 q1, q0, r6                  // ..*............................
        ldrd r8, r4, [r11, #(-16)]               // ...*...........................
        vmla.S16 q6, q1, r12                     // ....*..........................
        vldrw.U32 q7, [r0]                       // .....*.........................
        vadd.U16 q0, q3, q6                      // ......*........................
        vmul.S16 q4, q0, r8                      // .......*.......................
        vsub.U16 q1, q7, q2                      // ........*......................
        vqrdmulh.S16 q5, q0, r4                  // .........*.....................
        vsub.U16 q6, q3, q6                      // ..........*....................
        vmla.S16 q4, q5, r12                     // ...........*...................
        vadd.U16 q3, q7, q2                      // ............*..................
        vqrdmulh.S16 q7, q6, r2                  // .............*.................
        vsub.U16 q5, q3, q4                      // ..............*................
        vmul.S16 q2, q6, r5                      // ...............*...............
        vadd.U16 q4, q3, q4                      // ................*..............
        vmla.S16 q2, q7, r12                     // .................*.............
        vldrw.U32 q0, [r0, #48]                  // ..................e............
        vadd.U16 q6, q1, q2                      // ...................*...........
        ldrd r9, r6, [r11], #+24                 // ....................e..........
        vsub.U16 q7, q1, q2                      // .....................*.........
        vldrw.U32 q1, [r0, #32]                  // ......................e........
        vmul.S16 q2, q1, r9                      // .......................e.......
        vst40.U32 {q4, q5, q6, q7}, [r0]         // ........................*......
        vqrdmulh.S16 q1, q1, r6                  // .........................e.....
        vst41.U32 {q4, q5, q6, q7}, [r0]         // ..........................*....
        vldrw.U32 q3, [r0, #16]                  // ...........................e...
        vst42.U32 {q4, q5, q6, q7}, [r0]         // ............................*..
        vmla.S16 q2, q1, r12                     // .............................e.
        vst43.U32 {q4, q5, q6, q7}, [r0]!        // ..............................*

                                                  // ------------ cycle (expected) ------------->
                                                  // 0                        25
                                                  // |------------------------|------------------
        // ldrd r2, r3, [r11], #+24               // ..e..........'...................~..........
        // ldrd r4, r5, [r11, #(-16)]             // .............'..*...........................
        // ldrd r6, r7, [r11, #(-8)]              // .............'*.............................
        // vldrw.u32 q0, [r0]                     // .............'....*.........................
        // vldrw.u32 q1, [r0, #16]                // .........e...'..........................~...
        // vldrw.u32 q2, [r0, #32]                // ....e........'.....................~........
        // vldrw.u32 q3, [r0, #48]                // e............'.................~............
        // vmul.s16       q4,  q2, r2             // .....e.......'......................~.......
        // vqrdmulh.s16   q2,  q2, r3             // .......e.....'........................~.....
        // vmla.s16       q4,  q2, r12            // ...........e.'............................~.
        // vsub.u16       q2,    q0, q4           // .............'.......*......................
        // vadd.u16       q0,    q0, q4           // .............'...........*..................
        // vmul.s16       q4,  q3, r2             // .............*..............................
        // vqrdmulh.s16   q3,  q3, r3             // .............'.*............................
        // vmla.s16       q4,  q3, r12            // .............'...*..........................
        // vsub.u16       q3,    q1, q4           // .............'.........*....................
        // vadd.u16       q1,    q1, q4           // .............'.....*........................
        // vmul.s16       q4,  q1, r4             // .............'......*.......................
        // vqrdmulh.s16   q1,  q1, r5             // .............'........*.....................
        // vmla.s16       q4,  q1, r12            // .............'..........*...................
        // vsub.u16       q1,    q0, q4           // .............'.............*................
        // vadd.u16       q0,    q0, q4           // .............'...............*..............
        // vmul.s16       q4,  q3, r6             // .............'..............*...............
        // vqrdmulh.s16   q3,  q3, r7             // .............'............*.................
        // vmla.s16       q4,  q3, r12            // .............'................*.............
        // vsub.u16       q3,    q2, q4           // ...~.........'....................*.........
        // vadd.u16       q2,    q2, q4           // .~...........'..................*...........
        // vst40.u32 {q0, q1, q2, q3}, [r0]       // ......~......'.......................*......
        // vst41.u32 {q0, q1, q2, q3}, [r0]       // ........~....'.........................*....
        // vst42.u32 {q0, q1, q2, q3}, [r0]       // ..........~..'...........................*..
        // vst43.u32 {q0, q1, q2, q3}, [r0]!      // ............~'.............................*

        le lr, layer45_loop
                                                 // Instructions:    24
                                                 // Expected cycles: 31
                                                 // Expected IPC:    0.77
                                                 //
                                                 // Wall time:     0.08s
                                                 // User time:     0.08s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vqrdmulh.S16 q1, q0, r6                  // *..............................
        vldrw.U32 q6, [r0]                       // .*.............................
        vmul.S16 q4, q0, r9                      // ..*............................
        ldrd r6, r10, [r11, #(-8)]               // ...*...........................
        vmla.S16 q4, q1, r12                     // ....*..........................
        ldrd r8, r9, [r11, #(-16)]               // .....*.........................
        vadd.U16 q1, q3, q4                      // ......*........................
        vmul.S16 q5, q1, r8                      // .......*.......................
        vsub.U16 q7, q6, q2                      // ........*......................
        vqrdmulh.S16 q1, q1, r9                  // .........*.....................
        vadd.U16 q2, q6, q2                      // ..........*....................
        vmla.S16 q5, q1, r12                     // ...........*...................
        vsub.U16 q0, q3, q4                      // ............*..................
        vqrdmulh.S16 q4, q0, r10                 // .............*.................
        vsub.U16 q3, q2, q5                      // ..............*................
        vmul.S16 q0, q0, r6                      // ...............*...............
        vadd.U16 q2, q2, q5                      // ................*..............
        vmla.S16 q0, q4, r12                     // .................*.............
        vadd.U16 q4, q7, q0                      // ...................*...........
        vsub.U16 q5, q7, q0                      // .....................*.........
        vst40.U32 {q2, q3, q4, q5}, [r0]         // ........................*......
        vst41.U32 {q2, q3, q4, q5}, [r0]         // ..........................*....
        vst42.U32 {q2, q3, q4, q5}, [r0]         // ............................*..
        vst43.U32 {q2, q3, q4, q5}, [r0]!        // ..............................*

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vmul.S16 q6, q0, r9                    // ..*............................
        // ldrd r5, r2, [r11, #(-8)]              // ...*...........................
        // vqrdmulh.S16 q1, q0, r6                // *..............................
        // ldrd r8, r4, [r11, #(-16)]             // .....*.........................
        // vmla.S16 q6, q1, r12                   // ....*..........................
        // vldrw.U32 q7, [r0]                     // .*.............................
        // vadd.U16 q0, q3, q6                    // ......*........................
        // vmul.S16 q4, q0, r8                    // .......*.......................
        // vsub.U16 q1, q7, q2                    // ........*......................
        // vqrdmulh.S16 q5, q0, r4                // .........*.....................
        // vsub.U16 q6, q3, q6                    // ............*..................
        // vmla.S16 q4, q5, r12                   // ...........*...................
        // vadd.U16 q3, q7, q2                    // ..........*....................
        // vqrdmulh.S16 q7, q6, r2                // .............*.................
        // vsub.U16 q5, q3, q4                    // ..............*................
        // vmul.S16 q2, q6, r5                    // ...............*...............
        // vadd.U16 q4, q3, q4                    // ................*..............
        // vmla.S16 q2, q7, r12                   // .................*.............
        // vadd.U16 q6, q1, q2                    // ...................*...........
        // vsub.U16 q7, q1, q2                    // .....................*.........
        // vst40.U32 {q4, q5, q6, q7}, [r0]       // ........................*......
        // vst41.U32 {q4, q5, q6, q7}, [r0]       // ..........................*....
        // vst42.U32 {q4, q5, q6, q7}, [r0]       // ............................*..
        // vst43.U32 {q4, q5, q6, q7}, [r0]!      // ..............................*


        sub in, in, #(4*128)

        // Layers 6,7

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6

        mov lr, #8
                                              // Instructions:    7
                                              // Expected cycles: 8
                                              // Expected IPC:    0.88
                                              //
                                              // Wall time:     0.00s
                                              // User time:     0.00s
                                              //
                                              // ----- cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|----
        vldrw.U32 q2, [r0, #32]               // *.............................
        vldrh.U16 q1, [r11], #+96             // ..*...........................
        vmul.S16 q0, q2, q1                   // ...*..........................
        vldrh.U16 q7, [r11, #(+16-96)]        // ....*.........................
        vqrdmulh.S16 q3, q2, q7               // .....*........................
        vldrw.U32 q2, [r0, #48]               // ......*.......................
        vmla.S16 q0, q3, r12                  // .......*......................

                                               // ------ cycle (expected) ------>
                                               // 0                        25
                                               // |------------------------|-----
        // vldrw.U32 q2, [r0, #32]             // *..............................
        // vldrh.U16 q1, [r11], #+96           // ..*............................
        // vldrh.U16 q7, [r11, #(+16-96)]      // ....*..........................
        // vmul.S16 q0, q2, q1                 // ...*...........................
        // vqrdmulh.S16 q2, q2, q7             // .....*.........................
        // vmla.S16 q0, q2, r12                // .......*.......................
        // vldrw.U32 q2, [r0, #48]             // ......*........................

        sub lr, lr, #1
.p2align 2
layer67_loop:
                                                 // Instructions:    34
                                                 // Expected cycles: 34
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     6.27s
                                                 // User time:     6.27s
                                                 //
                                                 // ------- cycle (expected) -------->
                                                 // 0                        25
                                                 // |------------------------|--------
        vqrdmulh.S16 q7, q2, q7                  // *.................................
        vldrw.U32 q3, [r0]                       // .*................................
        vmul.S16 q4, q2, q1                      // ..*...............................
        vadd.U16 q2, q3, q0                      // ...*..............................
        vmla.S16 q4, q7, r12                     // ....*.............................
        vldrh.U16 q5, [r11, #(48 - 96)]          // .....*............................
        vsub.U16 q7, q3, q0                      // ......*...........................
        vldrw.U32 q0, [r0, #16]                  // .......*..........................
        vadd.U16 q6, q0, q4                      // ........*.........................
        vqrdmulh.S16 q5, q6, q5                  // .........*........................
        vldrh.U16 q3, [r11, #(32 - 96)]          // ..........*.......................
        vmul.S16 q3, q6, q3                      // ...........*......................
        vldrh.U16 q6, [r11, #(64-96)]            // ............*.....................
        vmla.S16 q3, q5, r12                     // .............*....................
        vsub.U16 q0, q0, q4                      // ..............*...................
        vmul.S16 q6, q0, q6                      // ...............*..................
        vldrh.U16 q5, [r11, #(80-96)]            // ................*.................
        vqrdmulh.S16 q0, q0, q5                  // .................*................
        vsub.U16 q4, q2, q3                      // ..................*...............
        vmla.S16 q6, q0, r12                     // ...................*..............
        vadd.U16 q3, q2, q3                      // ....................*.............
        vldrw.U32 q2, [r0, #32]                  // .....................e............
        vadd.U16 q5, q7, q6                      // ......................*...........
        vldrh.U16 q1, [r11], #+96                // .......................e..........
        vsub.U16 q6, q7, q6                      // ........................*.........
        vldrh.U16 q7, [r11, #(+16-96)]           // .........................e........
        vmul.S16 q0, q2, q1                      // ..........................e.......
        vst40.U32 {q3, q4, q5, q6}, [r0]         // ...........................*......
        vqrdmulh.S16 q2, q2, q7                  // ............................e.....
        vst41.U32 {q3, q4, q5, q6}, [r0]         // .............................*....
        vmla.S16 q0, q2, r12                     // ..............................e...
        vst42.U32 {q3, q4, q5, q6}, [r0]         // ...............................*..
        vldrw.U32 q2, [r0, #48]                  // ................................e.
        vst43.U32 {q3, q4, q5, q6}, [r0]!        // .................................*

                                                        // -------------- cycle (expected) -------------->
                                                        // 0                        25
                                                        // |------------------------|---------------------
        // vldrw.u32 q0, [r0]                           // .............'*................................
        // vldrw.u32 q1, [r0, #16]                      // .............'......*..........................
        // vldrw.u32 q2, [r0, #32]                      // e............'....................~............
        // vldrw.u32 q3, [r0, #48]                      // ...........e.'...............................~.
        // vldrh.u16 q5,         [r11], #+96            // ..e..........'......................~..........
        // vldrh.u16 q6, [r11, #(+16-96)]               // ....e........'........................~........
        // vmul.s16       q4,  q2, q5                   // .....e.......'.........................~.......
        // vqrdmulh.s16   q2,  q2, q6                   // .......e.....'...........................~.....
        // vmla.s16       q4,  q2, r12                  // .........e...'.............................~...
        // vsub.u16       q2,    q0, q4                 // .............'.....*...........................
        // vadd.u16       q0,    q0, q4                 // .............'..*..............................
        // vmul.s16       q4,  q3, q5                   // .............'.*...............................
        // vqrdmulh.s16   q3,  q3, q6                   // .............*.................................
        // vmla.s16       q4,  q3, r12                  // .............'...*.............................
        // vsub.u16       q3,    q1, q4                 // .............'.............*...................
        // vadd.u16       q1,    q1, q4                 // .............'.......*.........................
        // vldrh.u16 q5,         [r11, #(32 - 96)]      // .............'.........*.......................
        // vldrh.u16 q6, [r11, #(48 - 96)]              // .............'....*............................
        // vmul.s16       q4,  q1, q5                   // .............'..........*......................
        // vqrdmulh.s16   q1,  q1, q6                   // .............'........*........................
        // vmla.s16       q4,  q1, r12                  // .............'............*....................
        // vsub.u16       q1,    q0, q4                 // .............'.................*...............
        // vadd.u16       q0,    q0, q4                 // .............'...................*.............
        // vldrh.u16 q5,         [r11, #(64-96)]        // .............'...........*.....................
        // vldrh.u16 q6, [r11, #(80-96)]                // .............'...............*.................
        // vmul.s16       q4,  q3, q5                   // .............'..............*..................
        // vqrdmulh.s16   q3,  q3, q6                   // .............'................*................
        // vmla.s16       q4,  q3, r12                  // .............'..................*..............
        // vsub.u16       q3,    q2, q4                 // ...~.........'.......................*.........
        // vadd.u16       q2,    q2, q4                 // .~...........'.....................*...........
        // vst40.u32 {q0, q1, q2, q3}, [r0]             // ......~......'..........................*......
        // vst41.u32 {q0, q1, q2, q3}, [r0]             // ........~....'............................*....
        // vst42.u32 {q0, q1, q2, q3}, [r0]             // ..........~..'..............................*..
        // vst43.u32 {q0, q1, q2, q3}, [r0]!            // ............~'................................*

        le lr, layer67_loop
                                                 // Instructions:    27
                                                 // Expected cycles: 34
                                                 // Expected IPC:    0.79
                                                 //
                                                 // Wall time:     0.21s
                                                 // User time:     0.21s
                                                 //
                                                 // ------- cycle (expected) -------->
                                                 // 0                        25
                                                 // |------------------------|--------
        vqrdmulh.S16 q5, q2, q7                  // *.................................
        vldrh.U16 q7, [r11, #(80-96)]            // .*................................
        vmul.S16 q4, q2, q1                      // ..*...............................
        vldrw.U32 q6, [r0, #16]                  // ...*..............................
        vmla.S16 q4, q5, r12                     // ....*.............................
        vldrh.U16 q1, [r11, #(48 - 96)]          // .....*............................
        vadd.U16 q3, q6, q4                      // ......*...........................
        vqrdmulh.S16 q5, q3, q1                  // .......*..........................
        vldrh.U16 q2, [r11, #(32 - 96)]          // ........*.........................
        vmul.S16 q1, q3, q2                      // .........*........................
        vsub.U16 q6, q6, q4                      // ..........*.......................
        vqrdmulh.S16 q3, q6, q7                  // ...........*......................
        vldrw.U32 q7, [r0]                       // ............*.....................
        vmla.S16 q1, q5, r12                     // .............*....................
        vadd.U16 q2, q7, q0                      // ..............*...................
        vldrh.U16 q4, [r11, #(64-96)]            // ...............*..................
        vsub.U16 q0, q7, q0                      // ................*.................
        vmul.S16 q7, q6, q4                      // .................*................
        vsub.U16 q4, q2, q1                      // ..................*...............
        vmla.S16 q7, q3, r12                     // ...................*..............
        vadd.U16 q3, q2, q1                      // ....................*.............
        vsub.U16 q6, q0, q7                      // ......................*...........
        vadd.U16 q5, q0, q7                      // ........................*.........
        vst40.U32 {q3, q4, q5, q6}, [r0]         // ...........................*......
        vst41.U32 {q3, q4, q5, q6}, [r0]         // .............................*....
        vst42.U32 {q3, q4, q5, q6}, [r0]         // ...............................*..
        vst43.U32 {q3, q4, q5, q6}, [r0]!        // .................................*

                                                  // ------- cycle (expected) -------->
                                                  // 0                        25
                                                  // |------------------------|--------
        // vqrdmulh.S16 q7, q2, q7                // *.................................
        // vldrw.U32 q3, [r0]                     // ............*.....................
        // vmul.S16 q4, q2, q1                    // ..*...............................
        // vadd.U16 q2, q3, q0                    // ..............*...................
        // vmla.S16 q4, q7, r12                   // ....*.............................
        // vldrh.U16 q5, [r11, #(48 - 96)]        // .....*............................
        // vsub.U16 q7, q3, q0                    // ................*.................
        // vldrw.U32 q0, [r0, #16]                // ...*..............................
        // vadd.U16 q6, q0, q4                    // ......*...........................
        // vqrdmulh.S16 q5, q6, q5                // .......*..........................
        // vldrh.U16 q3, [r11, #(32 - 96)]        // ........*.........................
        // vmul.S16 q3, q6, q3                    // .........*........................
        // vldrh.U16 q6, [r11, #(64-96)]          // ...............*..................
        // vmla.S16 q3, q5, r12                   // .............*....................
        // vsub.U16 q0, q0, q4                    // ..........*.......................
        // vmul.S16 q6, q0, q6                    // .................*................
        // vldrh.U16 q5, [r11, #(80-96)]          // .*................................
        // vqrdmulh.S16 q0, q0, q5                // ...........*......................
        // vsub.U16 q4, q2, q3                    // ..................*...............
        // vmla.S16 q6, q0, r12                   // ...................*..............
        // vadd.U16 q3, q2, q3                    // ....................*.............
        // vadd.U16 q5, q7, q6                    // ........................*.........
        // vsub.U16 q6, q7, q6                    // ......................*...........
        // vst40.U32 {q3, q4, q5, q6}, [r0]       // ...........................*......
        // vst41.U32 {q3, q4, q5, q6}, [r0]       // .............................*....
        // vst42.U32 {q3, q4, q5, q6}, [r0]       // ...............................*..
        // vst43.U32 {q3, q4, q5, q6}, [r0]!      // .................................*


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
