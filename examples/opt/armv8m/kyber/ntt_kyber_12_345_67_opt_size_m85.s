
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
.p2align 4
roots:
#include "ntt_kyber_12_345_67_twiddles.s"
.text

#define QSTACK4   (0*16)
#define QSTACK5   (1*16)
#define QSTACK6   (2*16)
#define STACK0 (3*16)

#define POS_ROOT_1   1
#define POS_ROOT_2   2
#define POS_ROOT_3   3
#define POS_ROOT_4   4
#define POS_ROOT_5   5
#define POS_ROOT_6   6

#define STACK_SIZE (3*16 + 8)

.macro qsave loc, a       // @slothy:no-unfold
        vstrw.32 \a, [sp, #\loc\()]
.endm
.macro qrestore a, loc    // @slothy:no-unfold
        vldrw.32 \a, [sp, #\loc\()]
.endm
.macro restored a, b, loc // @slothy:no-unfold
        ldrd \a, \b, [sp, #\loc\()]
.endm
.macro saved loc, a, b    // @slothy:no-unfold
        strd \a, \b, [sp, #\loc\()]
.endm
.macro restore a, loc     // @slothy:no-unfold
        ldr \a, [sp, #\loc\()]
.endm
.macro save loc, a        // @slothy:no-unfold
        str \a, [sp, #\loc\()]
.endm

// Barrett multiplication
.macro mulmod dst, src, const, const_tw
        vmul.s16       \dst,  \src, \const
        vqrdmulh.s16   \src,  \src, \const_tw
        vmla.s16       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_tw
        mulmod tmp, \b, \root, \root_tw
        vsub.u16       \b,    \a, tmp
        vadd.u16       \a,    \a, tmp
.endm

// Aligns stack =0 mod 16
.macro align_stack_do // @slothy:no-unfold
        mov r11, sp
        and r12, r11, #0xC
        sub sp, sp, r12      // Align stack to 16 byte
        sub sp, sp, #16
        str r12, [sp]
.endm

// Reverts initial stack correction
.macro align_stack_undo // @slothy:no-unfold
        ldr r12, [sp]
        add sp, sp, #16
        add sp, sp, r12
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_kyber_12_345_67_opt_size_m85, %function
.global ntt_kyber_12_345_67_opt_size_m85

        modulus  .req r12
        r_ptr .req r11
        .equ modulus_const, -3329

        in           .req r0
        inp          .req r1
        in_low       .req r0
        in_high      .req r1

        root0    .req r2
        root0_tw .req r3
        root1    .req r4
        root1_tw .req r5
        root2    .req r6
        root2_tw .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3
        data4 .req q1
        data5 .req q2
        data6 .req q3
        data7 .req q4

        tmp     .req q7

        rtmp    .req r3
        rtmp_tw .req r4

        qtmp    .req q5
        qtmp_tw .req q6

ntt_kyber_12_345_67_opt_size_m85:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}
        align_stack_do

        sub sp, sp, #STACK_SIZE
        movw modulus, #:lower16:modulus_const
        ldr  r_ptr, roots_addr

        // Layers 1,2

        save STACK0, in
        add in_high, in_low, #(2*128)
        ldrd root0, root0_tw, [r_ptr], #+24
        ldrd root1, root1_tw, [r_ptr, #-16]
        ldrd root2, root2_tw, [r_ptr, #-8]

        mov lr, #8
        .p2align 2
                                          // Instructions:    2
                                          // Expected cycles: 2
                                          // Expected IPC:    1.00
                                          //
                                          // Wall time:     0.00s
                                          // User time:     0.00s
                                          //
                                          // ----- cycle (expected) ------>
                                          // 0                        25
                                          // |------------------------|----
        vldrw.32 q2, [r1, #(2*64)]        // *.............................
        vqrdmulh.S16 q1, q2, r3           // .*............................

                                           // ------ cycle (expected) ------>
                                           // 0                        25
                                           // |------------------------|-----
        // vldrw.32 q2, [r1, #(2*64)]      // *..............................
        // vqrdmulh.S16 q1, q2, r3         // .*.............................

        sub lr, lr, #1
.p2align 2
layer12_loop:
                                                // Instructions:    28
                                                // Expected cycles: 28
                                                // Expected IPC:    1.00
                                                //
                                                // Wall time:     2.85s
                                                // User time:     2.85s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vmul.S16 q5, q2, r2                     // *.............................
        vldrw.32 q6, [r1]                       // .*............................
        vmla.S16 q5, q1, r12                    // ..*...........................
        vldrw.32 q1, [r0, #(2*64)]              // ...*..........................
        vmul.S16 q7, q6, r2                     // ....*.........................
        vsub.U16 q0, q1, q5                     // .....*........................
        vqrdmulh.S16 q3, q6, r3                 // ......*.......................
        vadd.U16 q1, q1, q5                     // .......*......................
        vmla.S16 q7, q3, r12                    // ........*.....................
        vldrw.32 q4, [r0]                       // .........*....................
        vmul.S16 q5, q1, r4                     // ..........*...................
        vadd.U16 q3, q4, q7                     // ...........*..................
        vqrdmulh.S16 q1, q1, r5                 // ............*.................
        vsub.U16 q4, q4, q7                     // .............*................
        vmla.S16 q5, q1, r12                    // ..............*...............
        vldrw.32 q2, [r1, #(2*64)]              // ...............e..............
        vsub.U16 q7, q3, q5                     // ................*.............
        vmul.S16 q6, q0, r6                     // .................*............
        vadd.U16 q3, q3, q5                     // ..................*...........
        vqrdmulh.S16 q1, q0, r7                 // ...................*..........
        vstrw.U32 q3, [r0], #16                 // ....................*.........
        vmla.S16 q6, q1, r12                    // .....................*........
        vstrw.U32 q7, [r0, #(2*64 - 16)]        // ......................*.......
        vqrdmulh.S16 q1, q2, r3                 // .......................e......
        vsub.U16 q7, q4, q6                     // ........................*.....
        vstrw.U32 q7, [r1, #(2*64 - 16)]        // .........................*....
        vadd.U16 q4, q4, q6                     // ..........................*...
        vstrw.U32 q4, [r1], #16                 // ...........................*..

                                                 // ----------- cycle (expected) ----------->
                                                 // 0                        25
                                                 // |------------------------|---------------
        // vldrw.32 q0, [r0]                     // .............'........*..................
        // vldrw.32 q1, [r0, #(2*64)]            // .............'..*........................
        // vldrw.32 q2, [r1]                     // .............'*..........................
        // vldrw.32 q3, [r1, #(2*64)]            // e............'..............~............
        // vmul.s16       q7,  q2, r2            // .............'...*.......................
        // vqrdmulh.s16   q2,  q2, r3            // .............'.....*.....................
        // vmla.s16       q7,  q2, r12           // .............'.......*...................
        // vsub.u16       q2,    q0, q7          // .............'............*..............
        // vadd.u16       q0,    q0, q7          // .............'..........*................
        // vmul.s16       q7,  q3, r2            // .............*...........................
        // vqrdmulh.s16   q3,  q3, r3            // ........e....'......................~....
        // vmla.s16       q7,  q3, r12           // .............'.*.........................
        // vsub.u16       q3,    q1, q7          // .............'....*......................
        // vadd.u16       q1,    q1, q7          // .............'......*....................
        // vmul.s16       q7,  q1, r4            // .............'.........*.................
        // vqrdmulh.s16   q1,  q1, r5            // .............'...........*...............
        // vmla.s16       q7,  q1, r12           // .............'.............*.............
        // vsub.u16       q1,    q0, q7          // .~...........'...............*...........
        // vadd.u16       q0,    q0, q7          // ...~.........'.................*.........
        // vmul.s16       q7,  q3, r6            // ..~..........'................*..........
        // vqrdmulh.s16   q3,  q3, r7            // ....~........'..................*........
        // vmla.s16       q7,  q3, r12           // ......~......'....................*......
        // vsub.u16       q3,    q2, q7          // .........~...'.......................*...
        // vadd.u16       q2,    q2, q7          // ...........~.'.........................*.
        // vstrw.u32 q0, [r0], #16               // .....~.......'...................*.......
        // vstrw.u32 q1, [r0, #(2*64 - 16)]      // .......~.....'.....................*.....
        // vstrw.u32 q2, [r1], #16               // ............~'..........................*
        // vstrw.u32 q3, [r1, #(2*64 - 16)]      // ..........~..'........................*..

        le lr, layer12_loop
layer12_loop_end:// end of loop kernel
                                                // Instructions:    26
                                                // Expected cycles: 27
                                                // Expected IPC:    0.96
                                                //
                                                // Wall time:     0.15s
                                                // User time:     0.15s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vmul.S16 q7, q2, r2                     // *.............................
        vldrw.32 q0, [r0, #(2*64)]              // .*............................
        vmla.S16 q7, q1, r12                    // ..*...........................
        vldrw.32 q6, [r1]                       // ...*..........................
        vmul.S16 q2, q6, r2                     // ....*.........................
        vsub.U16 q3, q0, q7                     // .....*........................
        vqrdmulh.S16 q5, q3, r7                 // ......*.......................
        vldrw.32 q1, [r0]                       // .......*......................
        vqrdmulh.S16 q6, q6, r3                 // ........*.....................
        vadd.U16 q4, q0, q7                     // .........*....................
        vmla.S16 q2, q6, r12                    // ..........*...................
        vmul.S16 q0, q3, r6                     // ............*.................
        vsub.U16 q6, q1, q2                     // .............*................
        vmla.S16 q0, q5, r12                    // ..............*...............
        vadd.U16 q2, q1, q2                     // ...............*..............
        vmul.S16 q7, q4, r4                     // ................*.............
        vadd.U16 q5, q6, q0                     // .................*............
        vqrdmulh.S16 q1, q4, r5                 // ..................*...........
        vstrw.U32 q5, [r1], #16                 // ...................*..........
        vsub.U16 q5, q6, q0                     // ....................*.........
        vmla.S16 q7, q1, r12                    // .....................*........
        vstrw.U32 q5, [r1, #(2*64 - 16)]        // ......................*.......
        vsub.U16 q5, q2, q7                     // .......................*......
        vstrw.U32 q5, [r0, #(2*64 - 16)]        // ........................*.....
        vadd.U16 q2, q2, q7                     // .........................*....
        vstrw.U32 q2, [r0], #16                 // ..........................*...

                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        // vmul.S16 q5, q2, r2                   // *..............................
        // vldrw.32 q6, [r1]                     // ...*...........................
        // vmla.S16 q5, q1, r12                  // ..*............................
        // vldrw.32 q1, [r0, #(2*64)]            // .*.............................
        // vmul.S16 q7, q6, r2                   // ....*..........................
        // vsub.U16 q0, q1, q5                   // .....*.........................
        // vqrdmulh.S16 q3, q6, r3               // ........*......................
        // vadd.U16 q1, q1, q5                   // .........*.....................
        // vmla.S16 q7, q3, r12                  // ..........*....................
        // vldrw.32 q4, [r0]                     // .......*.......................
        // vmul.S16 q5, q1, r4                   // ................*..............
        // vadd.U16 q3, q4, q7                   // ...............*...............
        // vqrdmulh.S16 q1, q1, r5               // ..................*............
        // vsub.U16 q4, q4, q7                   // .............*.................
        // vmla.S16 q5, q1, r12                  // .....................*.........
        // vsub.U16 q7, q3, q5                   // .......................*.......
        // vmul.S16 q6, q0, r6                   // ............*..................
        // vadd.U16 q3, q3, q5                   // .........................*.....
        // vqrdmulh.S16 q1, q0, r7               // ......*........................
        // vstrw.U32 q3, [r0], #16               // ..........................*....
        // vmla.S16 q6, q1, r12                  // ..............*................
        // vstrw.U32 q7, [r0, #(2*64 - 16)]      // ........................*......
        // vsub.U16 q7, q4, q6                   // ....................*..........
        // vstrw.U32 q7, [r1, #(2*64 - 16)]      // ......................*........
        // vadd.U16 q4, q4, q6                   // .................*.............
        // vstrw.U32 q4, [r1], #16               // ...................*...........


        // Layers 3,4,5

        restore in, STACK0
        mov lr, #4
        .p2align 2
.p2align 2
layer345_loop:
                                                          // Instructions:    89
                                                          // Expected cycles: 89
                                                          // Expected IPC:    1.00
                                                          //
                                                          // Wall time:     17.47s
                                                          // User time:     17.47s
                                                          //
                                                          // ----------------------------------- cycle (expected) ----------------------------------->
                                                          // 0                        25                       50                       75
                                                          // |------------------------|------------------------|------------------------|-------------
        ldrd r9, r6, [r11], #(7*8)                        // *........................................................................................
        vldrw.32 q5, [r0, #64]                            // .*.......................................................................................
        vmul.S16 q7, q5, r9                               // ..*......................................................................................
        vldrw.32 q4, [r0]                                 // ...*.....................................................................................
        vqrdmulh.S16 q5, q5, r6                           // ....*....................................................................................
        vldrw.32 q2, [r0, #16]                            // .....*...................................................................................
        vmla.S16 q7, q5, r12                              // ......*..................................................................................
        vldrw.32 q0, [r0, #80]                            // .......*.................................................................................
        vsub.U16 q6, q4, q7                               // ........*................................................................................
        vmul.S16 q5, q0, r9                               // .........*...............................................................................
        vadd.U16 q1, q4, q7                               // ..........*..............................................................................
        vqrdmulh.S16 q0, q0, r6                           // ...........*.............................................................................
        qsave QSTACK4, q6                                 // ............*............................................................................
        vmla.S16 q5, q0, r12                              // .............*...........................................................................
        vldrw.32 q6, [r0, #96]                            // ..............*..........................................................................
        vmul.S16 q0, q6, r9                               // ...............*.........................................................................
        vsub.U16 q4, q2, q5                               // ................*........................................................................
        vqrdmulh.S16 q6, q6, r6                           // .................*.......................................................................
        vadd.U16 q7, q2, q5                               // ..................*......................................................................
        vmla.S16 q0, q6, r12                              // ...................*.....................................................................
        vldrw.32 q5, [r0, #112]                           // ....................*....................................................................
        vmul.S16 q6, q5, r9                               // .....................*...................................................................
        qsave QSTACK5, q4                                 // ......................*..................................................................
        vqrdmulh.S16 q5, q5, r6                           // .......................*.................................................................
        vldrw.32 q2, [r0, #32]                            // ........................*................................................................
        vmla.S16 q6, q5, r12                              // .........................*...............................................................
        ldrd r8, r6, [r11, #((-7 + POS_ROOT_1)*8)]        // ..........................*..............................................................
        vadd.U16 q5, q2, q0                               // ...........................*.............................................................
        vmul.S16 q4, q5, r8                               // ............................*............................................................
        vsub.U16 q3, q2, q0                               // .............................*...........................................................
        vqrdmulh.S16 q5, q5, r6                           // ..............................*..........................................................
        vldrw.32 q2, [r0, #48]                            // ...............................*.........................................................
        vmla.S16 q4, q5, r12                              // ................................*........................................................
        vadd.U16 q5, q2, q6                               // .................................*.......................................................
        vmul.S16 q0, q5, r8                               // ..................................*......................................................
        qsave QSTACK6, q3                                 // ...................................*.....................................................
        vqrdmulh.S16 q5, q5, r6                           // ....................................*....................................................
        vsub.U16 q3, q2, q6                               // .....................................*...................................................
        vmla.S16 q0, q5, r12                              // ......................................*..................................................
        ldrd r8, r6, [r11, #((-7 + POS_ROOT_2)*8)]        // .......................................*.................................................
        vadd.U16 q5, q7, q0                               // ........................................*................................................
        vmul.S16 q6, q5, r8                               // .........................................*...............................................
        vsub.U16 q2, q1, q4                               // ..........................................*..............................................
        vqrdmulh.S16 q5, q5, r6                           // ...........................................*.............................................
        vadd.U16 q1, q1, q4                               // ............................................*............................................
        vmla.S16 q6, q5, r12                              // .............................................*...........................................
        ldrd r8, r6, [r11, #((-7 + POS_ROOT_3)*8)]        // ..............................................*..........................................
        vsub.U16 q5, q7, q0                               // ...............................................*.........................................
        vmul.S16 q0, q5, r8                               // ................................................*........................................
        vsub.U16 q4, q1, q6                               // .................................................*.......................................
        vqrdmulh.S16 q5, q5, r6                           // ..................................................*......................................
        vadd.U16 q6, q1, q6                               // ...................................................*.....................................
        vmla.S16 q0, q5, r12                              // ....................................................*....................................
        vstrw.U32 q6, [r0], #128                          // .....................................................*...................................
        vsub.U16 q6, q2, q0                               // ......................................................*..................................
        vstrw.U32 q4, [r0, #(-128+16)]                    // .......................................................*.................................
        vadd.U16 q0, q2, q0                               // ........................................................*................................
        ldrd r8, r6, [r11, #((-7 + POS_ROOT_4)*8)]        // .........................................................*...............................
        qrestore q5, QSTACK6                              // ..........................................................*..............................
        vmul.S16 q4, q5, r8                               // ...........................................................*.............................
        vstrw.U32 q0, [r0, #(-128+32)]                    // ............................................................*............................
        vqrdmulh.S16 q5, q5, r6                           // .............................................................*...........................
        vstrw.U32 q6, [r0, #(-128+48)]                    // ..............................................................*..........................
        vmla.S16 q4, q5, r12                              // ...............................................................*.........................
        qrestore q1, QSTACK4                              // ................................................................*........................
        vmul.S16 q0, q3, r8                               // .................................................................*.......................
        qrestore q2, QSTACK5                              // ..................................................................*......................
        vqrdmulh.S16 q5, q3, r6                           // ...................................................................*.....................
        vsub.U16 q3, q1, q4                               // ....................................................................*....................
        vmla.S16 q0, q5, r12                              // .....................................................................*...................
        ldrd r8, r6, [r11, #((-7 + POS_ROOT_5)*8)]        // ......................................................................*..................
        vadd.U16 q5, q2, q0                               // .......................................................................*.................
        vmul.S16 q6, q5, r8                               // ........................................................................*................
        vadd.U16 q1, q1, q4                               // .........................................................................*...............
        vqrdmulh.S16 q5, q5, r6                           // ..........................................................................*..............
        vsub.U16 q4, q2, q0                               // ...........................................................................*.............
        vmla.S16 q6, q5, r12                              // ............................................................................*............
        ldrd r8, r6, [r11, #((-7 + POS_ROOT_6)*8)]        // .............................................................................*...........
        vsub.U16 q2, q1, q6                               // ..............................................................................*..........
        vmul.S16 q0, q4, r8                               // ...............................................................................*.........
        vadd.U16 q6, q1, q6                               // ................................................................................*........
        vqrdmulh.S16 q5, q4, r6                           // .................................................................................*.......
        vstrw.U32 q6, [r0, #(-128+64)]                    // ..................................................................................*......
        vmla.S16 q0, q5, r12                              // ...................................................................................*.....
        vstrw.U32 q2, [r0, #(-128+80)]                    // ....................................................................................*....
        vadd.U16 q7, q3, q0                               // .....................................................................................*...
        vstrw.U32 q7, [r0, #(-128+96)]                    // ......................................................................................*..
        vsub.U16 q5, q3, q0                               // .......................................................................................*.
        vstrw.U32 q5, [r0, #(-128+112)]                   // ........................................................................................*

                                                           // ----------------------------------- cycle (expected) ----------------------------------->
                                                           // 0                        25                       50                       75
                                                           // |------------------------|------------------------|------------------------|-------------
        // ldrd r3, r4, [r11], #(7*8)                      // *........................................................................................
        // vldrw.32 q0, [r0]                               // ...*.....................................................................................
        // vldrw.32 q1, [r0, #64]                          // .*.......................................................................................
        // vmul.s16       q7,  q1, r3                      // ..*......................................................................................
        // vqrdmulh.s16   q1,  q1, r4                      // ....*....................................................................................
        // vmla.s16       q7,  q1, r12                     // ......*..................................................................................
        // vsub.u16       q1,    q0, q7                    // ........*................................................................................
        // vadd.u16       q0,    q0, q7                    // ..........*..............................................................................
        // qsave QSTACK4, q1                               // ............*............................................................................
        // vldrw.32 q1, [r0, #16]                          // .....*...................................................................................
        // vldrw.32 q2, [r0, #80]                          // .......*.................................................................................
        // vmul.s16       q7,  q2, r3                      // .........*...............................................................................
        // vqrdmulh.s16   q2,  q2, r4                      // ...........*.............................................................................
        // vmla.s16       q7,  q2, r12                     // .............*...........................................................................
        // vsub.u16       q2,    q1, q7                    // ................*........................................................................
        // vadd.u16       q1,    q1, q7                    // ..................*......................................................................
        // qsave QSTACK5, q2                               // ......................*..................................................................
        // vldrw.32 q2, [r0, #32]                          // ........................*................................................................
        // vldrw.32 q3, [r0, #96]                          // ..............*..........................................................................
        // vmul.s16       q7,  q3, r3                      // ...............*.........................................................................
        // vqrdmulh.s16   q3,  q3, r4                      // .................*.......................................................................
        // vmla.s16       q7,  q3, r12                     // ...................*.....................................................................
        // vsub.u16       q3,    q2, q7                    // .............................*...........................................................
        // vadd.u16       q2,    q2, q7                    // ...........................*.............................................................
        // qsave QSTACK6, q3                               // ...................................*.....................................................
        // vldrw.32 q3, [r0, #48]                          // ...............................*.........................................................
        // vldrw.32 q4, [r0, #112]                         // ....................*....................................................................
        // vmul.s16       q7,  q4, r3                      // .....................*...................................................................
        // vqrdmulh.s16   q4,  q4, r4                      // .......................*.................................................................
        // vmla.s16       q7,  q4, r12                     // .........................*...............................................................
        // vsub.u16       q4,    q3, q7                    // .....................................*...................................................
        // vadd.u16       q3,    q3, q7                    // .................................*.......................................................
        // ldrd r3, r4, [r11, #((-7 + POS_ROOT_1)*8)]      // ..........................*..............................................................
        // vmul.s16       q7,  q2, r3                      // ............................*............................................................
        // vqrdmulh.s16   q2,  q2, r4                      // ..............................*..........................................................
        // vmla.s16       q7,  q2, r12                     // ................................*........................................................
        // vsub.u16       q2,    q0, q7                    // ..........................................*..............................................
        // vadd.u16       q0,    q0, q7                    // ............................................*............................................
        // vmul.s16       q7,  q3, r3                      // ..................................*......................................................
        // vqrdmulh.s16   q3,  q3, r4                      // ....................................*....................................................
        // vmla.s16       q7,  q3, r12                     // ......................................*..................................................
        // vsub.u16       q3,    q1, q7                    // ...............................................*.........................................
        // vadd.u16       q1,    q1, q7                    // ........................................*................................................
        // ldrd r3, r4, [r11, #((-7 + POS_ROOT_2)*8)]      // .......................................*.................................................
        // vmul.s16       q7,  q1, r3                      // .........................................*...............................................
        // vqrdmulh.s16   q1,  q1, r4                      // ...........................................*.............................................
        // vmla.s16       q7,  q1, r12                     // .............................................*...........................................
        // vsub.u16       q1,    q0, q7                    // .................................................*.......................................
        // vadd.u16       q0,    q0, q7                    // ...................................................*.....................................
        // ldrd r3, r4, [r11, #((-7 + POS_ROOT_3)*8)]      // ..............................................*..........................................
        // vmul.s16       q7,  q3, r3                      // ................................................*........................................
        // vqrdmulh.s16   q3,  q3, r4                      // ..................................................*......................................
        // vmla.s16       q7,  q3, r12                     // ....................................................*....................................
        // vsub.u16       q3,    q2, q7                    // ......................................................*..................................
        // vadd.u16       q2,    q2, q7                    // ........................................................*................................
        // vstrw.u32 q0, [r0], #128                        // .....................................................*...................................
        // vstrw.u32 q1, [r0, #(-128+16)]                  // .......................................................*.................................
        // vstrw.u32 q2, [r0, #(-128+32)]                  // ............................................................*............................
        // vstrw.u32 q3, [r0, #(-128+48)]                  // ..............................................................*..........................
        // qrestore q1, QSTACK4                            // ................................................................*........................
        // qrestore q2, QSTACK5                            // ..................................................................*......................
        // qrestore q3, QSTACK6                            // ..........................................................*..............................
        // ldrd r3, r4, [r11, #((-7 + POS_ROOT_4)*8)]      // .........................................................*...............................
        // vmul.s16       q7,  q3, r3                      // ...........................................................*.............................
        // vqrdmulh.s16   q3,  q3, r4                      // .............................................................*...........................
        // vmla.s16       q7,  q3, r12                     // ...............................................................*.........................
        // vsub.u16       q3,    q1, q7                    // ....................................................................*....................
        // vadd.u16       q1,    q1, q7                    // .........................................................................*...............
        // vmul.s16       q7,  q4, r3                      // .................................................................*.......................
        // vqrdmulh.s16   q4,  q4, r4                      // ...................................................................*.....................
        // vmla.s16       q7,  q4, r12                     // .....................................................................*...................
        // vsub.u16       q4,    q2, q7                    // ...........................................................................*.............
        // vadd.u16       q2,    q2, q7                    // .......................................................................*.................
        // ldrd r3, r4, [r11, #((-7 + POS_ROOT_5)*8)]      // ......................................................................*..................
        // vmul.s16       q7,  q2, r3                      // ........................................................................*................
        // vqrdmulh.s16   q2,  q2, r4                      // ..........................................................................*..............
        // vmla.s16       q7,  q2, r12                     // ............................................................................*............
        // vsub.u16       q2,    q1, q7                    // ..............................................................................*..........
        // vadd.u16       q1,    q1, q7                    // ................................................................................*........
        // ldrd r3, r4, [r11, #((-7 + POS_ROOT_6)*8)]      // .............................................................................*...........
        // vmul.s16       q7,  q4, r3                      // ...............................................................................*.........
        // vqrdmulh.s16   q4,  q4, r4                      // .................................................................................*.......
        // vmla.s16       q7,  q4, r12                     // ...................................................................................*.....
        // vsub.u16       q4,    q3, q7                    // .......................................................................................*.
        // vadd.u16       q3,    q3, q7                    // .....................................................................................*...
        // vstrw.u32 q1, [r0, #(-128+64)]                  // ..................................................................................*......
        // vstrw.u32 q2, [r0, #(-128+80)]                  // ....................................................................................*....
        // vstrw.u32 q3, [r0, #(-128+96)]                  // ......................................................................................*..
        // vstrw.u32 q4, [r0, #(-128+112)]                 // ........................................................................................*

        le lr, layer345_loop

        // Layer 67

        // Use a different base register to facilitate Helight being able to
        // overlap the first iteration of L67 with the last iteration of L345.
        restore inp, STACK0
        mov lr, #8
        .p2align 2
                                                // Instructions:    5
                                                // Expected cycles: 11
                                                // Expected IPC:    0.45
                                                //
                                                // Wall time:     0.00s
                                                // User time:     0.00s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vld40.32 {q2, q3, q4, q5}, [r1]         // *.............................
        vld41.32 {q2, q3, q4, q5}, [r1]         // ..*...........................
        vldrh.16 q7, [r11, #(+16-96)]           // ....*.........................
        vld42.32 {q2, q3, q4, q5}, [r1]         // ......*.......................
        vld43.32 {q2, q3, q4, q5}, [r1]!        // ..........*...................

                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        // vld40.32 {q2, q3, q4, q5}, [r1]       // *..............................
        // vld41.32 {q2, q3, q4, q5}, [r1]       // ..*............................
        // vld42.32 {q2, q3, q4, q5}, [r1]       // ......*........................
        // vldrh.16 q7, [r11, #(+16-96)]         // ....*..........................
        // vld43.32 {q2, q3, q4, q5}, [r1]!      // ..........*....................

        sub lr, lr, #1
.p2align 2
layer67_loop:
                                                // Instructions:    34
                                                // Expected cycles: 35
                                                // Expected IPC:    0.97
                                                //
                                                // Wall time:     22.50s
                                                // User time:     22.50s
                                                //
                                                // -------- cycle (expected) -------->
                                                // 0                        25
                                                // |------------------------|---------
        vqrdmulh.S16 q0, q5, q7                 // *..................................
        vldrh.16 q1, [r11], #+96                // .*.................................
        vmul.S16 q6, q5, q1                     // ..*................................
        vmla.S16 q6, q0, r12                    // ....*..............................
        vldrh.16 q0, [r11, #(48 - 96)]          // .....*.............................
        vmul.S16 q1, q4, q1                     // ......*............................
        vadd.U16 q5, q3, q6                     // .......*...........................
        vqrdmulh.S16 q7, q4, q7                 // ........*..........................
        vsub.U16 q6, q3, q6                     // .........*.........................
        vmla.S16 q1, q7, r12                    // ..........*........................
        vldrh.16 q7, [r11, #(32 - 96)]          // ...........*.......................
        vmul.S16 q7, q5, q7                     // ............*......................
        vadd.U16 q3, q2, q1                     // .............*.....................
        vqrdmulh.S16 q5, q5, q0                 // ..............*....................
        vsub.U16 q0, q2, q1                     // ...............*...................
        vmla.S16 q7, q5, r12                    // ................*..................
        vldrh.16 q1, [r11, #(80-96)]            // .................*.................
        vsub.U16 q2, q3, q7                     // ..................*................
        vstrw.U32 q2, [r1, #-48]                // ...................*...............
        vadd.U16 q7, q3, q7                     // ....................*..............
        vld40.32 {q2, q3, q4, q5}, [r1]         // .....................e.............
        vqrdmulh.S16 q1, q6, q1                 // ......................*............
        vld41.32 {q2, q3, q4, q5}, [r1]         // .......................e...........
        vstrw.U32 q7, [r1, #-64]                // ........................*..........
        vldrh.16 q7, [r11, #(64-96)]            // .........................*.........
        vmul.S16 q6, q6, q7                     // ..........................*........
        vld42.32 {q2, q3, q4, q5}, [r1]         // ...........................e.......
        vmla.S16 q6, q1, r12                    // ............................*......
        vldrh.16 q7, [r11, #(+16-96)]           // .............................e.....
        vadd.U16 q1, q0, q6                     // ..............................*....
        vld43.32 {q2, q3, q4, q5}, [r1]!        // ...............................e...
        vstrw.U32 q1, [r1, #-32]                // ................................*..
        vsub.U16 q1, q0, q6                     // .................................*.
        vstrw.U32 q1, [r1, #-16]                // ..................................*

                                                  // --------------- cycle (expected) --------------->
                                                  // 0                        25
                                                  // |------------------------|-----------------------
        // vld40.32 {q0, q1, q2, q3}, [r1]        // e.............'....................~.............
        // vld41.32 {q0, q1, q2, q3}, [r1]        // ..e...........'......................~...........
        // vld42.32 {q0, q1, q2, q3}, [r1]        // ......e.......'..........................~.......
        // vld43.32 {q0, q1, q2, q3}, [r1]!       // ..........e...'..............................~...
        // vldrh.16 q5,    [r11], #+96            // ..............'*.................................
        // vldrh.16 q6, [r11, #(+16-96)]          // ........e.....'............................~.....
        // vmul.s16       q7,  q2, q5             // ..............'.....*............................
        // vqrdmulh.s16   q2,  q2, q6             // ..............'.......*..........................
        // vmla.s16       q7,  q2, r12            // ..............'.........*........................
        // vsub.u16       q2,    q0, q7           // ..............'..............*...................
        // vadd.u16       q0,    q0, q7           // ..............'............*.....................
        // vmul.s16       q7,  q3, q5             // ..............'.*................................
        // vqrdmulh.s16   q3,  q3, q6             // ..............*..................................
        // vmla.s16       q7,  q3, r12            // ..............'...*..............................
        // vsub.u16       q3,    q1, q7           // ..............'........*.........................
        // vadd.u16       q1,    q1, q7           // ..............'......*...........................
        // vldrh.16 q5,    [r11, #(32 - 96)]      // ..............'..........*.......................
        // vldrh.16 q6, [r11, #(48 - 96)]         // ..............'....*.............................
        // vmul.s16       q7,  q1, q5             // ..............'...........*......................
        // vqrdmulh.s16   q1,  q1, q6             // ..............'.............*....................
        // vmla.s16       q7,  q1, r12            // ..............'...............*..................
        // vsub.u16       q1,    q0, q7           // ..............'.................*................
        // vadd.u16       q0,    q0, q7           // ..............'...................*..............
        // vldrh.16 q5,    [r11, #(64-96)]        // ....~.........'........................*.........
        // vldrh.16 q6, [r11, #(80-96)]           // ..............'................*.................
        // vmul.s16       q7,  q3, q5             // .....~........'.........................*........
        // vqrdmulh.s16   q3,  q3, q6             // .~............'.....................*............
        // vmla.s16       q7,  q3, r12            // .......~......'...........................*......
        // vsub.u16       q3,    q2, q7           // ............~.'................................*.
        // vadd.u16       q2,    q2, q7           // .........~....'.............................*....
        // vstrw.u32 q0, [r1, #-64]               // ...~..........'.......................*..........
        // vstrw.u32 q1, [r1, #-48]               // ..............'..................*...............
        // vstrw.u32 q2, [r1, #-32]               // ...........~..'...............................*..
        // vstrw.u32 q3, [r1, #-16]               // .............~'.................................*

        le lr, layer67_loop
                                              // Instructions:    29
                                              // Expected cycles: 30
                                              // Expected IPC:    0.97
                                              //
                                              // Wall time:     0.29s
                                              // User time:     0.29s
                                              //
                                              // ----- cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|----
        vqrdmulh.S16 q1, q5, q7               // *.............................
        vldrh.16 q0, [r11], #+96              // .*............................
        vmul.S16 q5, q5, q0                   // ..*...........................
        vmla.S16 q5, q1, r12                  // ....*.........................
        vldrh.16 q1, [r11, #(32 - 96)]        // .....*........................
        vadd.U16 q6, q3, q5                   // ......*.......................
        vmul.S16 q1, q6, q1                   // .......*......................
        vsub.U16 q5, q3, q5                   // ........*.....................
        vqrdmulh.S16 q7, q4, q7               // .........*....................
        vldrh.16 q3, [r11, #(64-96)]          // ..........*...................
        vmul.S16 q0, q4, q0                   // ...........*..................
        vldrh.16 q4, [r11, #(48 - 96)]        // ............*.................
        vmla.S16 q0, q7, r12                  // .............*................
        vldrh.16 q7, [r11, #(80-96)]          // ..............*...............
        vqrdmulh.S16 q4, q6, q4               // ...............*..............
        vsub.U16 q6, q2, q0                   // ................*.............
        vmla.S16 q1, q4, r12                  // .................*............
        vadd.U16 q4, q2, q0                   // ..................*...........
        vmul.S16 q0, q5, q3                   // ...................*..........
        vsub.U16 q2, q4, q1                   // ....................*.........
        vstrw.U32 q2, [r1, #-48]              // .....................*........
        vqrdmulh.S16 q2, q5, q7               // ......................*.......
        vadd.U16 q7, q4, q1                   // .......................*......
        vmla.S16 q0, q2, r12                  // ........................*.....
        vstrw.U32 q7, [r1, #-64]              // .........................*....
        vadd.U16 q4, q6, q0                   // ..........................*...
        vstrw.U32 q4, [r1, #-32]              // ...........................*..
        vsub.U16 q5, q6, q0                   // ............................*.
        vstrw.U32 q5, [r1, #-16]              // .............................*

                                               // ------ cycle (expected) ------>
                                               // 0                        25
                                               // |------------------------|-----
        // vqrdmulh.S16 q0, q5, q7             // *..............................
        // vldrh.16 q1, [r11], #+96            // .*.............................
        // vmul.S16 q6, q5, q1                 // ..*............................
        // vmla.S16 q6, q0, r12                // ....*..........................
        // vldrh.16 q0, [r11, #(48 - 96)]      // ............*..................
        // vmul.S16 q1, q4, q1                 // ...........*...................
        // vadd.U16 q5, q3, q6                 // ......*........................
        // vqrdmulh.S16 q7, q4, q7             // .........*.....................
        // vsub.U16 q6, q3, q6                 // ........*......................
        // vmla.S16 q1, q7, r12                // .............*.................
        // vldrh.16 q7, [r11, #(32 - 96)]      // .....*.........................
        // vmul.S16 q7, q5, q7                 // .......*.......................
        // vadd.U16 q3, q2, q1                 // ..................*............
        // vqrdmulh.S16 q5, q5, q0             // ...............*...............
        // vsub.U16 q0, q2, q1                 // ................*..............
        // vmla.S16 q7, q5, r12                // .................*.............
        // vldrh.16 q1, [r11, #(80-96)]        // ..............*................
        // vsub.U16 q2, q3, q7                 // ....................*..........
        // vstrw.U32 q2, [r1, #-48]            // .....................*.........
        // vadd.U16 q7, q3, q7                 // .......................*.......
        // vqrdmulh.S16 q1, q6, q1             // ......................*........
        // vstrw.U32 q7, [r1, #-64]            // .........................*.....
        // vldrh.16 q7, [r11, #(64-96)]        // ..........*....................
        // vmul.S16 q6, q6, q7                 // ...................*...........
        // vmla.S16 q6, q1, r12                // ........................*......
        // vadd.U16 q1, q0, q6                 // ..........................*....
        // vstrw.U32 q1, [r1, #-32]            // ...........................*...
        // vsub.U16 q1, q0, q6                 // ............................*..
        // vstrw.U32 q1, [r1, #-16]            // .............................*.


        add sp, sp, #STACK_SIZE

        align_stack_undo
        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
