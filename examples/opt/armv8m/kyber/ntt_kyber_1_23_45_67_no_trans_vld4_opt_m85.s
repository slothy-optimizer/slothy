
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots:
#include "ntt_kyber_1_23_45_67_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s16       \dst,  \src, \const
        vqrdmulh.s16   \src,  \src, \const_twisted
        vmla.s16       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_twisted
        mulmod tmp, \b, \root, \root_twisted
        vsub.u16       \b,    \a, tmp
        vadd.u16       \a,    \a, tmp
.endm

.macro load_first_root root0, root0_twisted
        ldrd root0, root0_twisted, [root_ptr], #+8
.endm

.macro load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted
        ldrd root0, root0_twisted, [root_ptr], #+24
        ldrd root1, root1_twisted, [root_ptr, #(-16)]
        ldrd root2, root2_twisted, [root_ptr, #(-8)]
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_kyber_1_23_45_67_no_trans_vld4_opt_m85, %function
.global ntt_kyber_1_23_45_67_no_trans_vld4_opt_m85
ntt_kyber_1_23_45_67_no_trans_vld4_opt_m85:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus  .req r12
        root_ptr .req r11

        .equ modulus_const, -3329
        movw modulus, #:lower16:modulus_const
        ldr  root_ptr, roots_addr

        in_low       .req r0
        in_high      .req r1

        add in_high, in_low, #(4*64)

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        // Layers 1

        load_first_root root0, root0_twisted

        mov lr, #16
        vldrw.u32 q6, [r1]              // *.
        vqrdmulh.s16 q3, q6, r3         // .*
        
        // original source code
        // vldrw.u32 q6, [r1]           // *. 
        // vqrdmulh.s16 q3, q6, r3      // .* 
        
        sub lr, lr, #1
.p2align 2
layer1_loop:
        vmul.s16 q4, q6, r2              // ..*......
        vldrw.u32 q5, [r0]               // *........
        vmla.s16 q4, q3, r12             // ....*....
        vldrw.u32 q6, [r1, #16]          // .e.......
        vsub.u16 q7, q5, q4              // .....*...
        vstrw.u32 q7, [r1] , #16         // ........*
        vadd.u16 q7, q5, q4              // ......*..
        vqrdmulh.s16 q3, q6, r3          // ...e.....
        vstrw.u32 q7, [r0] , #16         // .......*.
        
        // original source code
        // vldrw.u32 q0, [r0]            // .......*....... 
        // vldrw.u32 q1, [r1]            // e.............. 
        // vmul.s16 q4, q1, r2           // ......*........ 
        // vqrdmulh.s16 q1, q1, r3       // ....e.......... 
        // vmla.s16 q4, q1, r12          // ........*...... 
        // vsub.u16 q1, q0, q4           // ..........*.... 
        // vadd.u16 q0, q0, q4           // ............*.. 
        // vstrw.u32 q0, [r0] , #16      // ..............* 
        // vstrw.u32 q1, [r1] , #16      // ...........*... 
        
        le lr, layer1_loop
        vmul.s16 q6, q6, r2              // *......
        // gap                           // .......
        vmla.s16 q6, q3, r12             // ..*....
        vldrw.u32 q3, [r0]               // .*.....
        vsub.u16 q7, q3, q6              // ...*...
        vstrw.u32 q7, [r1] , #16         // ....*..
        vadd.u16 q7, q3, q6              // .....*.
        vstrw.u32 q7, [r0] , #16         // ......*
        
        // original source code
        // vmul.s16 q4, q6, r2           // *...... 
        // vldrw.u32 q5, [r0]            // ..*.... 
        // vmla.s16 q4, q3, r12          // .*..... 
        // vsub.u16 q7, q5, q4           // ...*... 
        // vstrw.u32 q7, [r1] , #16      // ....*.. 
        // vadd.u16 q7, q5, q4           // .....*. 
        // vstrw.u32 q7, [r0] , #16      // ......* 
        
        .unreq in_high
        .unreq in_low

        in .req r0
        sub in, in, #(4*64)

        // Layers 2,3

        count .req r1
        mov count, #2

out_start:
        load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted

        mov lr, #4
        vldrw.u32 q7, [r0, #128]          // *.
        vqrdmulh.s16 q6, q7, r3           // .*
        
        // original source code
        // vldrw.u32 q7, [r0, #128]       // *. 
        // vqrdmulh.s16 q6, q7, r3        // .* 
        
        sub lr, lr, #1
.p2align 2
layer23_loop:
        vmul.s16 q1, q7, r2               // ....*.......................
        vldrw.u32 q5, [r0, #192]          // ...*........................
        vmul.s16 q2, q5, r2               // .........*..................
        vldrw.u32 q4, [r0]                // *...........................
        vqrdmulh.s16 q5, q5, r3           // ..........*.................
        vldrw.u32 q7, [r0, #144]          // ..e.........................
        vmla.s16 q2, q5, r12              // ...........*................
        vldrw.u32 q0, [r0, #64]           // .*..........................
        vmla.s16 q1, q6, r12              // ......*.....................
        vsub.u16 q6, q0, q2               // ............*...............
        vmul.s16 q5, q6, r6               // ...................*........
        vsub.u16 q3, q4, q1               // .......*....................
        vqrdmulh.s16 q6, q6, r7           // ....................*.......
        vadd.u16 q1, q4, q1               // ........*...................
        vmla.s16 q5, q6, r12              // .....................*......
        vadd.u16 q0, q0, q2               // .............*..............
        vqrdmulh.s16 q2, q0, r5           // ...............*............
        vadd.u16 q4, q3, q5               // .......................*....
        vqrdmulh.s16 q6, q7, r3           // .....e......................
        vsub.u16 q5, q3, q5               // ......................*.....
        vmul.s16 q3, q0, r4               // ..............*.............
        vstrw.u32 q4, [r0, #128]          // ..........................*.
        vmla.s16 q3, q2, r12              // ................*...........
        vstrw.u32 q5, [r0, #192]          // ...........................*
        vsub.u16 q5, q1, q3               // .................*..........
        vstrw.u32 q5, [r0, #64]           // .........................*..
        vadd.u16 q3, q1, q3               // ..................*.........
        vstrw.u32 q3, [r0] , #16          // ........................*...
        
        // original source code
        // vldrw.u32 q0, [r0]             // ..........................*........................ 
        // vldrw.u32 q1, [r0, #64]        // ..............................*.................... 
        // vldrw.u32 q2, [r0, #128]       // e.................................................. 
        // vldrw.u32 q3, [r0, #192]       // ........................*.......................... 
        // vmul.s16 q4, q2, r2            // .......................*........................... 
        // vqrdmulh.s16 q2, q2, r3        // .............e..................................... 
        // vmla.s16 q4, q2, r12           // ...............................*................... 
        // vsub.u16 q2, q0, q4            // ..................................*................ 
        // vadd.u16 q0, q0, q4            // ....................................*.............. 
        // vmul.s16 q4, q3, r2            // .........................*......................... 
        // vqrdmulh.s16 q3, q3, r3        // ...........................*....................... 
        // vmla.s16 q4, q3, r12           // .............................*..................... 
        // vsub.u16 q3, q1, q4            // ................................*.................. 
        // vadd.u16 q1, q1, q4            // ......................................*............ 
        // vmul.s16 q4, q1, r4            // ...........................................*....... 
        // vqrdmulh.s16 q1, q1, r5        // .......................................*........... 
        // vmla.s16 q4, q1, r12           // .............................................*..... 
        // vsub.u16 q1, q0, q4            // ...............................................*... 
        // vadd.u16 q0, q0, q4            // .................................................*. 
        // vmul.s16 q4, q3, r6            // .................................*................. 
        // vqrdmulh.s16 q3, q3, r7        // ...................................*............... 
        // vmla.s16 q4, q3, r12           // .....................................*............. 
        // vsub.u16 q3, q2, q4            // ..........................................*........ 
        // vadd.u16 q2, q2, q4            // ........................................*.......... 
        // vstrw.u32 q0, [r0] , #16       // ..................................................* 
        // vstrw.u32 q1, [r0, #48]        // ................................................*.. 
        // vstrw.u32 q2, [r0, #112]       // ............................................*...... 
        // vstrw.u32 q3, [r0, #176]       // ..............................................*.... 
        
        le lr, layer23_loop
        vmul.s16 q1, q7, r2               // *.........................
        vldrw.u32 q0, [r0, #192]          // .*........................
        vmul.s16 q5, q0, r2               // ..*.......................
        vldrw.u32 q4, [r0, #64]           // ......*...................
        vqrdmulh.s16 q2, q0, r3           // ....*.....................
        vldrw.u32 q3, [r0]                // ...*......................
        vmla.s16 q5, q2, r12              // .....*....................
        // gap                            // ..........................
        vmla.s16 q1, q6, r12              // .......*..................
        vadd.u16 q0, q4, q5               // ..............*...........
        vqrdmulh.s16 q6, q0, r5           // ...............*..........
        vadd.u16 q7, q3, q1               // ............*.............
        vmul.s16 q2, q0, r4               // ..................*.......
        vsub.u16 q0, q4, q5               // ........*.................
        vmla.s16 q2, q6, r12              // ....................*.....
        vsub.u16 q6, q3, q1               // ..........*...............
        vqrdmulh.s16 q5, q0, r7           // ...........*..............
        vsub.u16 q4, q7, q2               // ......................*...
        vstrw.u32 q4, [r0, #64]           // .......................*..
        vmul.s16 q0, q0, r6               // .........*................
        vadd.u16 q3, q7, q2               // ........................*.
        vmla.s16 q0, q5, r12              // .............*............
        vstrw.u32 q3, [r0] , #16          // .........................*
        vadd.u16 q2, q6, q0               // ................*.........
        vstrw.u32 q2, [r0, #112]          // ...................*......
        vsub.u16 q1, q6, q0               // .................*........
        vstrw.u32 q1, [r0, #176]          // .....................*....
        
        // original source code
        // vmul.s16 q1, q7, r2            // *......................... 
        // vldrw.u32 q5, [r0, #192]       // .*........................ 
        // vmul.s16 q2, q5, r2            // ..*....................... 
        // vldrw.u32 q4, [r0]             // .....*.................... 
        // vqrdmulh.s16 q5, q5, r3        // ....*..................... 
        // vmla.s16 q2, q5, r12           // ......*................... 
        // vldrw.u32 q0, [r0, #64]        // ...*...................... 
        // vmla.s16 q1, q6, r12           // .......*.................. 
        // vsub.u16 q6, q0, q2            // ............*............. 
        // vmul.s16 q5, q6, r6            // ..................*....... 
        // vsub.u16 q3, q4, q1            // ..............*........... 
        // vqrdmulh.s16 q6, q6, r7        // ...............*.......... 
        // vadd.u16 q1, q4, q1            // ..........*............... 
        // vmla.s16 q5, q6, r12           // ....................*..... 
        // vadd.u16 q0, q0, q2            // ........*................. 
        // vqrdmulh.s16 q2, q0, r5        // .........*................ 
        // vadd.u16 q4, q3, q5            // ......................*... 
        // vsub.u16 q5, q3, q5            // ........................*. 
        // vmul.s16 q3, q0, r4            // ...........*.............. 
        // vstrw.u32 q4, [r0, #128]       // .......................*.. 
        // vmla.s16 q3, q2, r12           // .............*............ 
        // vstrw.u32 q5, [r0, #192]       // .........................* 
        // vsub.u16 q5, q1, q3            // ................*......... 
        // vstrw.u32 q5, [r0, #64]        // .................*........ 
        // vadd.u16 q3, q1, q3            // ...................*...... 
        // vstrw.u32 q3, [r0] , #16       // .....................*.... 
        

        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*128)

        // Layers 4,5

        mov lr, #8
.p2align 2
layer45_loop:
        ldrd r4, r5, [r11] , #24          // *..............................
        vldrw.u32 q7, [r0, #32]           // .....*.........................
        vmul.s16 q3, q7, r4               // .......*.......................
        vldrw.u32 q6, [r0, #48]           // ......*........................
        vmul.s16 q4, q6, r4               // ............*..................
        ldrd r4, r6, [r11, #-16]          // .*.............................
        vqrdmulh.s16 q7, q7, r5           // ........*......................
        ldrd r10, r3, [r11, #-8]          // ..*............................
        vqrdmulh.s16 q6, q6, r5           // .............*.................
        vldrw.u32 q5, [r0]                // ...*...........................
        vmla.s16 q4, q6, r12              // ..............*................
        vldrw.u32 q6, [r0, #16]           // ....*..........................
        vmla.s16 q3, q7, r12              // .........*.....................
        vadd.u16 q7, q6, q4               // ................*..............
        vmul.s16 q1, q7, r4               // .................*.............
        vsub.u16 q2, q5, q3               // ..........*....................
        vqrdmulh.s16 q7, q7, r6           // ..................*............
        vadd.u16 q3, q5, q3               // ...........*...................
        vmla.s16 q1, q7, r12              // ...................*...........
        vsub.u16 q7, q6, q4               // ...............*...............
        vmul.s16 q6, q7, r10              // ......................*........
        vsub.u16 q4, q3, q1               // ....................*..........
        vqrdmulh.s16 q7, q7, r3           // .......................*.......
        vadd.u16 q3, q3, q1               // .....................*.........
        vstrw.u32 q4, [r0, #16]           // ............................*..
        vmla.s16 q6, q7, r12              // ........................*......
        vstrw.u32 q3, [r0] , #64          // ...........................*...
        vsub.u16 q7, q2, q6               // .........................*.....
        vstrw.u32 q7, [r0, #-16]          // ..............................*
        vadd.u16 q7, q2, q6               // ..........................*....
        vstrw.u32 q7, [r0, #-32]          // .............................*.
        
        // original source code
        // ldrd r2, r3, [r11] , #24       // *.............................. 
        // ldrd r4, r5, [r11, #-16]       // .....*......................... 
        // ldrd r6, r7, [r11, #-8]        // .......*....................... 
        // vldrw.u32 q0, [r0]             // .........*..................... 
        // vldrw.u32 q1, [r0, #16]        // ...........*................... 
        // vldrw.u32 q2, [r0, #32]        // .*............................. 
        // vldrw.u32 q3, [r0, #48]        // ...*........................... 
        // vmul.s16 q4, q2, r2            // ..*............................ 
        // vqrdmulh.s16 q2, q2, r3        // ......*........................ 
        // vmla.s16 q4, q2, r12           // ............*.................. 
        // vsub.u16 q2, q0, q4            // ...............*............... 
        // vadd.u16 q0, q0, q4            // .................*............. 
        // vmul.s16 q4, q3, r2            // ....*.......................... 
        // vqrdmulh.s16 q3, q3, r3        // ........*...................... 
        // vmla.s16 q4, q3, r12           // ..........*.................... 
        // vsub.u16 q3, q1, q4            // ...................*........... 
        // vadd.u16 q1, q1, q4            // .............*................. 
        // vmul.s16 q4, q1, r4            // ..............*................ 
        // vqrdmulh.s16 q1, q1, r5        // ................*.............. 
        // vmla.s16 q4, q1, r12           // ..................*............ 
        // vsub.u16 q1, q0, q4            // .....................*......... 
        // vadd.u16 q0, q0, q4            // .......................*....... 
        // vmul.s16 q4, q3, r6            // ....................*.......... 
        // vqrdmulh.s16 q3, q3, r7        // ......................*........ 
        // vmla.s16 q4, q3, r12           // .........................*..... 
        // vsub.u16 q3, q2, q4            // ...........................*... 
        // vadd.u16 q2, q2, q4            // .............................*. 
        // vstrw.u32 q0, [r0] , #64       // ..........................*.... 
        // vstrw.u32 q1, [r0, #-48]       // ........................*...... 
        // vstrw.u32 q2, [r0, #-32]       // ..............................* 
        // vstrw.u32 q3, [r0, #-16]       // ............................*.. 
        
        le lr, layer45_loop

        sub in, in, #(4*128)

        // Layers 6,7

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6

        mov lr, #8
        vld40.u32 {q3,q4,q5,q6}, [r0]          // *.....
        // gap                                 // ......
        vld41.u32 {q3,q4,q5,q6}, [r0]          // .*....
        // gap                                 // ......
        vld42.u32 {q3,q4,q5,q6}, [r0]          // ..*...
        // gap                                 // ......
        vld43.u32 {q3,q4,q5,q6}, [r0]!         // ...*..
        // gap                                 // ......
        vldrh.u16 q2, [r11, #16]               // ....*.
        // gap                                 // ......
        vqrdmulh.s16 q1, q6, q2                // .....*
        
        // original source code
        // vld40.u32 {q3,q4,q5,q6}, [r0]       // *..... 
        // vld41.u32 {q3,q4,q5,q6}, [r0]       // .*.... 
        // vld42.u32 {q3,q4,q5,q6}, [r0]       // ..*... 
        // vld43.u32 {q3,q4,q5,q6}, [r0]!      // ...*.. 
        // vldrh.u16 q2, [r11, #16]            // ....*. 
        // vqrdmulh.s16 q1, q6, q2             // .....* 
        
        sub lr, lr, #1
.p2align 2
layer67_loop:
        vqrdmulh.s16 q2, q5, q2                // .......*..........................
        vldrh.u16 q0, [r11] , #96              // ....*.............................
        vmul.s16 q7, q5, q0                    // ......*...........................
        vldrh.u16 q5, [r11, #-16]              // ........................*.........
        vmul.s16 q6, q6, q0                    // ...........*......................
        vldrh.u16 q0, [r11, #-32]              // .......................*..........
        vmla.s16 q6, q1, r12                   // .............*....................
        vldrh.u16 q1, [r11, #-48]              // .................*................
        vmla.s16 q7, q2, r12                   // ........*.........................
        vadd.u16 q2, q4, q6                    // ...............*..................
        vqrdmulh.s16 q1, q2, q1                // ...................*..............
        vsub.u16 q4, q4, q6                    // ..............*...................
        vmul.s16 q6, q4, q0                    // .........................*........
        vadd.u16 q0, q3, q7                    // ..........*.......................
        vqrdmulh.s16 q4, q4, q5                // ..........................*.......
        vsub.u16 q5, q3, q7                    // .........*........................
        vmla.s16 q6, q4, r12                   // ...........................*......
        vldrh.u16 q7, [r11, #-64]              // ................*.................
        vsub.u16 q3, q5, q6                    // ............................*.....
        vstrw.u32 q3, [r0, #-16]               // .................................*
        vadd.u16 q4, q5, q6                    // .............................*....
        vstrw.u32 q4, [r0, #-32]               // ................................*.
        vld40.u32 {q3,q4,q5,q6}, [r0]          // e.................................
        vmul.s16 q7, q2, q7                    // ..................*...............
        vld41.u32 {q3,q4,q5,q6}, [r0]          // .e................................
        vmla.s16 q7, q1, r12                   // ....................*.............
        vld42.u32 {q3,q4,q5,q6}, [r0]          // ..e...............................
        vsub.u16 q2, q0, q7                    // .....................*............
        vld43.u32 {q3,q4,q5,q6}, [r0]!         // ...e..............................
        vstrw.u32 q2, [r0, #-112]              // ...............................*..
        vadd.u16 q7, q0, q7                    // ......................*...........
        vldrh.u16 q2, [r11, #16]               // .....e............................
        vqrdmulh.s16 q1, q6, q2                // ............e.....................
        vstrw.u32 q7, [r0, #-128]              // ..............................*...
        
        // original source code
        // vld40.u32 {q0,q1,q2,q3}, [r0]       // e............................................. 
        // vld41.u32 {q0,q1,q2,q3}, [r0]       // ..e........................................... 
        // vld42.u32 {q0,q1,q2,q3}, [r0]       // ....e......................................... 
        // vld43.u32 {q0,q1,q2,q3}, [r0]!      // ......e....................................... 
        // vldrh.u16 q5, [r11] , #96           // .............*................................ 
        // vldrh.u16 q6, [r11, #-80]           // .........e.................................... 
        // vmul.s16 q4, q2, q5                 // ..............*............................... 
        // vqrdmulh.s16 q2, q2, q6             // ............*................................. 
        // vmla.s16 q4, q2, r12                // ....................*......................... 
        // vsub.u16 q2, q0, q4                 // ...........................*.................. 
        // vadd.u16 q0, q0, q4                 // .........................*.................... 
        // vmul.s16 q4, q3, q5                 // ................*............................. 
        // vqrdmulh.s16 q3, q3, q6             // ..........e................................... 
        // vmla.s16 q4, q3, r12                // ..................*........................... 
        // vsub.u16 q3, q1, q4                 // .......................*...................... 
        // vadd.u16 q1, q1, q4                 // .....................*........................ 
        // vldrh.u16 q5, [r11, #-64]           // .............................*................ 
        // vldrh.u16 q6, [r11, #-48]           // ...................*.......................... 
        // vmul.s16 q4, q1, q5                 // ...................................*.......... 
        // vqrdmulh.s16 q1, q1, q6             // ......................*....................... 
        // vmla.s16 q4, q1, r12                // .....................................*........ 
        // vsub.u16 q1, q0, q4                 // .......................................*...... 
        // vadd.u16 q0, q0, q4                 // ..........................................*... 
        // vldrh.u16 q5, [r11, #-32]           // .................*............................ 
        // vldrh.u16 q6, [r11, #-16]           // ...............*.............................. 
        // vmul.s16 q4, q3, q5                 // ........................*..................... 
        // vqrdmulh.s16 q3, q3, q6             // ..........................*................... 
        // vmla.s16 q4, q3, r12                // ............................*................. 
        // vsub.u16 q3, q2, q4                 // ..............................*............... 
        // vadd.u16 q2, q2, q4                 // ................................*............. 
        // vstrw.u32 q0, [r0, #-64]            // .............................................* 
        // vstrw.u32 q1, [r0, #-48]            // .........................................*.... 
        // vstrw.u32 q2, [r0, #-32]            // .................................*............ 
        // vstrw.u32 q3, [r0, #-16]            // ...............................*.............. 
        
        le lr, layer67_loop
        vqrdmulh.s16 q7, q5, q2            // *...........................
        vldrh.u16 q2, [r11] , #96          // .*..........................
        vmul.s16 q0, q6, q2                // ....*.......................
        vldrh.u16 q6, [r11, #-16]          // ...*........................
        vmla.s16 q0, q1, r12               // ......*.....................
        vldrh.u16 q1, [r11, #-32]          // .....*......................
        vmul.s16 q2, q5, q2                // ..*.........................
        vsub.u16 q5, q4, q0                // ...........*................
        vmla.s16 q2, q7, r12               // ........*...................
        vadd.u16 q7, q4, q0                // .........*..................
        vmul.s16 q0, q5, q1                // ............*...............
        vadd.u16 q4, q3, q2                // .............*..............
        vqrdmulh.s16 q1, q5, q6            // ..............*.............
        vsub.u16 q6, q3, q2                // ...............*............
        vmla.s16 q0, q1, r12               // ................*...........
        vldrh.u16 q1, [r11, #-48]          // .......*....................
        vsub.u16 q3, q6, q0                // ..................*.........
        vstrw.u32 q3, [r0, #-16]           // ...................*........
        vqrdmulh.s16 q3, q7, q1            // ..........*.................
        vldrh.u16 q2, [r11, #-64]          // .................*..........
        vmul.s16 q1, q7, q2                // ......................*.....
        vadd.u16 q5, q6, q0                // ....................*.......
        vmla.s16 q1, q3, r12               // .......................*....
        vstrw.u32 q5, [r0, #-32]           // .....................*......
        vsub.u16 q3, q4, q1                // ........................*...
        vstrw.u32 q3, [r0, #-48]           // .........................*..
        vadd.u16 q2, q4, q1                // ..........................*.
        vstrw.u32 q2, [r0, #-64]           // ...........................*
        
        // original source code
        // vqrdmulh.s16 q2, q5, q2         // *........................... 
        // vldrh.u16 q0, [r11] , #96       // .*.......................... 
        // vmul.s16 q7, q5, q0             // ......*..................... 
        // vldrh.u16 q5, [r11, #-16]       // ...*........................ 
        // vmul.s16 q6, q6, q0             // ..*......................... 
        // vldrh.u16 q0, [r11, #-32]       // .....*...................... 
        // vmla.s16 q6, q1, r12            // ....*....................... 
        // vldrh.u16 q1, [r11, #-48]       // ...............*............ 
        // vmla.s16 q7, q2, r12            // ........*................... 
        // vadd.u16 q2, q4, q6             // .........*.................. 
        // vqrdmulh.s16 q1, q2, q1         // ..................*......... 
        // vsub.u16 q4, q4, q6             // .......*.................... 
        // vmul.s16 q6, q4, q0             // ..........*................. 
        // vadd.u16 q0, q3, q7             // ...........*................ 
        // vqrdmulh.s16 q4, q4, q5         // ............*............... 
        // vsub.u16 q5, q3, q7             // .............*.............. 
        // vmla.s16 q6, q4, r12            // ..............*............. 
        // vldrh.u16 q7, [r11, #-64]       // ...................*........ 
        // vsub.u16 q3, q5, q6             // ................*........... 
        // vstrw.u32 q3, [r0, #-16]        // .................*.......... 
        // vadd.u16 q4, q5, q6             // .....................*...... 
        // vstrw.u32 q4, [r0, #-32]        // .......................*.... 
        // vmul.s16 q7, q2, q7             // ....................*....... 
        // vmla.s16 q7, q1, r12            // ......................*..... 
        // vsub.u16 q2, q0, q7             // ........................*... 
        // vstrw.u32 q2, [r0, #-48]        // .........................*.. 
        // vadd.u16 q7, q0, q7             // ..........................*. 
        // vstrw.u32 q7, [r0, #-64]        // ...........................* 
        

        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr