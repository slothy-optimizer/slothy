///
/// Copyright (c) 2022 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots:
#include "ntt_dilithium_12_34_56_78_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s32       \dst,  \src, \const
        vqrdmulh.s32   \src,  \src, \const_twisted
        vmla.s32       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_twisted
        mulmod tmp, \b, \root, \root_twisted
        vsub.u32       \b,    \a, tmp
        vadd.u32       \a,    \a, tmp
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_dilithium_12_34_56_78, %function
.global ntt_dilithium_12_34_56_78
ntt_dilithium_12_34_56_78:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus  .req r12
        root_ptr .req r11

        .equ modulus_const, -8380417
        movw modulus, #:lower16:modulus_const
        movt modulus, #:upper16:modulus_const
        ldr  root_ptr, roots_addr

        in_low       .req r0
        in_high      .req r1

        add in_high, in_low, #(4*128)

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        // Layers 1-2
        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #16
                                           // Instructions:    5
                                           // Expected cycles: 6
                                           // Expected IPC:    0.83
                                           //
                                           // Wall time:     0.00s
                                           // User time:     0.00s
                                           //
                                           // ----- cycle (expected) ------>
                                           // 0                        25
                                           // |------------------------|----
        vldrw.U32 q0, [r1, #(4*64)]        // *.............................
        vqrdmulh.S32 q1, q0, r3            // .*............................
        vmul.S32 q0, q0, r2                // ...*..........................
        vldrw.U32 q3, [r0, #(4*64)]        // ....*.........................
        vmla.S32 q0, q1, r12               // .....*........................

                                            // ------ cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|-----
        // vldrw.U32 q7, [r1, #(4*64)]      // *..............................
        // vldrw.U32 q3, [r0, #(4*64)]      // ....*..........................
        // vmul.S32 q0, q7, r2              // ...*...........................
        // vqrdmulh.S32 q7, q7, r3          // .*.............................
        // vmla.S32 q0, q7, r12             // .....*.........................

        sub lr, lr, #1
.p2align 2
layer12_loop:
                                                // Instructions:    28
                                                // Expected cycles: 28
                                                // Expected IPC:    1.00
                                                //
                                                // Wall time:     2.98s
                                                // User time:     2.98s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vsub.U32 q4, q3, q0                     // *.............................
        vmul.S32 q5, q4, r6                     // .*............................
        vldrw.U32 q6, [r1]                      // ..*...........................
        vqrdmulh.S32 q2, q6, r3                 // ...*..........................
        vldrw.U32 q7, [r1, #(4*64)]             // ....e.........................
        vmul.S32 q1, q6, r2                     // .....*........................
        vadd.U32 q3, q3, q0                     // ......*.......................
        vmla.S32 q1, q2, r12                    // .......*......................
        vldrw.U32 q6, [r0]                      // ........*.....................
        vmul.S32 q0, q3, r4                     // .........*....................
        vadd.U32 q2, q6, q1                     // ..........*...................
        vqrdmulh.S32 q3, q3, r5                 // ...........*..................
        vsub.U32 q6, q6, q1                     // ............*.................
        vmla.S32 q0, q3, r12                    // .............*................
        vldrw.U32 q3, [r0, #(4*64)]             // ..............e...............
        vqrdmulh.S32 q4, q4, r7                 // ...............*..............
        vadd.U32 q1, q2, q0                     // ................*.............
        vmla.S32 q5, q4, r12                    // .................*............
        vsub.U32 q4, q2, q0                     // ..................*...........
        vmul.S32 q0, q7, r2                     // ...................e..........
        vstrw.U32 q1, [r0], #16                 // ....................*.........
        vsub.U32 q2, q6, q5                     // .....................*........
        vqrdmulh.S32 q7, q7, r3                 // ......................e.......
        vstrw.U32 q4, [r0, #(4*64 - 16)]        // .......................*......
        vadd.U32 q6, q6, q5                     // ........................*.....
        vstrw.U32 q2, [r1, #(4*64-16)]          // .........................*....
        vmla.S32 q0, q7, r12                    // ..........................e...
        vstrw.U32 q6, [r1], #16                 // ...........................*..

                                                 // ---------------- cycle (expected) ----------------->
                                                 // 0                        25                       50
                                                 // |------------------------|------------------------|-
        // vldrw.u32 q0, [r0]                    // ....~...................'.......*...................
        // vldrw.u32 q1, [r0, #(4*64)]           // ..........e.............'.............~.............
        // vldrw.u32 q2, [r1]                    // ........................'.*.........................
        // vldrw.u32 q3, [r1, #(4*64)]           // e.......................'...~.......................
        // vmul.s32       q4,  q2, r2            // .~......................'....*......................
        // vqrdmulh.s32   q2,  q2, r3            // ........................'..*........................
        // vmla.s32       q4,  q2, r12           // ...~....................'......*....................
        // vsub.u32       q2,    q0, q4          // ........~...............'...........*...............
        // vadd.u32       q0,    q0, q4          // ......~.................'.........*.................
        // vmul.s32       q4,  q3, r2            // ...............e........'..................~........
        // vqrdmulh.s32   q3,  q3, r3            // ..................e.....'.....................~.....
        // vmla.s32       q4,  q3, r12           // ......................e.'.........................~.
        // vsub.u32       q3,    q1, q4          // ........................*...........................
        // vadd.u32       q1,    q1, q4          // ..~.....................'.....*.....................
        // vmul.s32       q4,  q1, r4            // .....~..................'........*..................
        // vqrdmulh.s32   q1,  q1, r5            // .......~................'..........*................
        // vmla.s32       q4,  q1, r12           // .........~..............'............*..............
        // vsub.u32       q1,    q0, q4          // ..............~.........'.................*.........
        // vadd.u32       q0,    q0, q4          // ............~...........'...............*...........
        // vmul.s32       q4,  q3, r6            // ........................'*..........................
        // vqrdmulh.s32   q3,  q3, r7            // ...........~............'..............*............
        // vmla.s32       q4,  q3, r12           // .............~..........'................*..........
        // vsub.u32       q3,    q2, q4          // .................~......'....................*......
        // vadd.u32       q2,    q2, q4          // ....................~...'.......................*...
        // vstrw.u32 q0, [r0], #16               // ................~.......'...................*.......
        // vstrw.u32 q1, [r0, #(4*64 - 16)]      // ...................~....'......................*....
        // vstrw.u32 q2, [r1], #16               // .......................~'..........................*
        // vstrw.u32 q3, [r1, #(4*64-16)]        // .....................~..'........................*..

        le lr, layer12_loop
                                                // Instructions:    23
                                                // Expected cycles: 24
                                                // Expected IPC:    0.96
                                                //
                                                // Wall time:     0.12s
                                                // User time:     0.12s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vsub.U32 q2, q3, q0                     // *.............................
        vqrdmulh.S32 q6, q2, r7                 // .*............................
        vldrw.U32 q5, [r1]                      // ..*...........................
        vmul.S32 q1, q5, r2                     // ...*..........................
        vldrw.U32 q4, [r0]                      // ....*.........................
        vqrdmulh.S32 q5, q5, r3                 // .....*........................
        vadd.U32 q7, q3, q0                     // ......*.......................
        vmla.S32 q1, q5, r12                    // .......*......................
        vmul.S32 q3, q2, r6                     // .........*....................
        vsub.U32 q2, q4, q1                     // ..........*...................
        vmla.S32 q3, q6, r12                    // ...........*..................
        vadd.U32 q4, q4, q1                     // ............*.................
        vmul.S32 q1, q7, r4                     // .............*................
        vadd.U32 q0, q2, q3                     // ..............*...............
        vqrdmulh.S32 q5, q7, r5                 // ...............*..............
        vstrw.U32 q0, [r1], #16                 // ................*.............
        vsub.U32 q0, q2, q3                     // .................*............
        vmla.S32 q1, q5, r12                    // ..................*...........
        vstrw.U32 q0, [r1, #(4*64-16)]          // ...................*..........
        vadd.U32 q6, q4, q1                     // ....................*.........
        vstrw.U32 q6, [r0], #16                 // .....................*........
        vsub.U32 q0, q4, q1                     // ......................*.......
        vstrw.U32 q0, [r0, #(4*64 - 16)]        // .......................*......

                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        // vsub.U32 q4, q3, q0                   // *..............................
        // vmul.S32 q5, q4, r6                   // .........*.....................
        // vldrw.U32 q6, [r1]                    // ..*............................
        // vqrdmulh.S32 q2, q6, r3               // .....*.........................
        // vmul.S32 q1, q6, r2                   // ...*...........................
        // vadd.U32 q3, q3, q0                   // ......*........................
        // vmla.S32 q1, q2, r12                  // .......*.......................
        // vldrw.U32 q6, [r0]                    // ....*..........................
        // vmul.S32 q0, q3, r4                   // .............*.................
        // vadd.U32 q2, q6, q1                   // ............*..................
        // vqrdmulh.S32 q3, q3, r5               // ...............*...............
        // vsub.U32 q6, q6, q1                   // ..........*....................
        // vmla.S32 q0, q3, r12                  // ..................*............
        // vqrdmulh.S32 q4, q4, r7               // .*.............................
        // vadd.U32 q1, q2, q0                   // ....................*..........
        // vmla.S32 q5, q4, r12                  // ...........*...................
        // vsub.U32 q4, q2, q0                   // ......................*........
        // vstrw.U32 q1, [r0], #16               // .....................*.........
        // vsub.U32 q2, q6, q5                   // .................*.............
        // vstrw.U32 q4, [r0, #(4*64 - 16)]      // .......................*.......
        // vadd.U32 q6, q6, q5                   // ..............*................
        // vstrw.U32 q2, [r1, #(4*64-16)]        // ...................*...........
        // vstrw.U32 q6, [r1], #16               // ................*..............


        .unreq in_high
        .unreq in_low
        in .req r0

        // Layers 3,4
        sub in, in, #(64*4)

        // 4 butterfly blocks per root config, 4 root configs
        // loop over root configs

        count .req r1
        mov count, #4

out_start:
        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #4
                                             // Instructions:    5
                                             // Expected cycles: 6
                                             // Expected IPC:    0.83
                                             //
                                             // Wall time:     0.00s
                                             // User time:     0.00s
                                             //
                                             // ----- cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|----
        vldrw.U32 q0, [r0, #(4*3*16)]        // *.............................
        vqrdmulh.S32 q1, q0, r3              // .*............................
        vmul.S32 q0, q0, r2                  // ...*..........................
        vldrw.U32 q3, [r0, #(4*1*16)]        // ....*.........................
        vmla.S32 q0, q1, r12                 // .....*........................

                                              // ------ cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|-----
        // vldrw.U32 q0, [r0, #(4*3*16)]      // *..............................
        // vqrdmulh.S32 q5, q0, r3            // .*.............................
        // vmul.S32 q0, q0, r2                // ...*...........................
        // vldrw.U32 q3, [r0, #(4*1*16)]      // ....*..........................
        // vmla.S32 q0, q5, r12               // .....*.........................

        sub lr, lr, #1
.p2align 2
layer34_loop:
                                                  // Instructions:    28
                                                  // Expected cycles: 28
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     2.56s
                                                  // User time:     2.56s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vadd.U32 q6, q3, q0                       // *.............................
        vqrdmulh.S32 q4, q6, r5                   // .*............................
        vldrw.U32 q2, [r0, #(4*2*16)]             // ..*...........................
        vmul.S32 q1, q2, r2                       // ...*..........................
        vsub.U32 q7, q3, q0                       // ....*.........................
        vqrdmulh.S32 q3, q2, r3                   // .....*........................
        vldrw.U32 q0, [r0, #(4*3*16)]             // ......e.......................
        vmla.S32 q1, q3, r12                      // .......*......................
        vldrw.U32 q5, [r0]                        // ........*.....................
        vmul.S32 q6, q6, r4                       // .........*....................
        vadd.U32 q3, q5, q1                       // ..........*...................
        vmla.S32 q6, q4, r12                      // ...........*..................
        vsub.U32 q2, q5, q1                       // ............*.................
        vqrdmulh.S32 q5, q0, r3                   // .............e................
        vadd.U32 q4, q3, q6                       // ..............*...............
        vmul.S32 q0, q0, r2                       // ...............e..............
        vsub.U32 q1, q3, q6                       // ................*.............
        vmul.S32 q6, q7, r6                       // .................*............
        vldrw.U32 q3, [r0, #(4*1*16)]             // ..................e...........
        vqrdmulh.S32 q7, q7, r7                   // ...................*..........
        vstrw.U32 q4, [r0], #16                   // ....................*.........
        vmla.S32 q6, q7, r12                      // .....................*........
        vstrw.U32 q1, [r0, #(4*1*16 - 16)]        // ......................*.......
        vmla.S32 q0, q5, r12                      // .......................e......
        vadd.U32 q4, q2, q6                       // ........................*.....
        vstrw.U32 q4, [r0, #(4*2*16 - 16)]        // .........................*....
        vsub.U32 q6, q2, q6                       // ..........................*...
        vstrw.U32 q6, [r0, #(4*3*16 - 16)]        // ...........................*..

                                                   // --------------- cycle (expected) ---------------->
                                                   // 0                        25
                                                   // |------------------------|------------------------
        // vldrw.u32 q0, [r0]                      // ..~...................'.......*...................
        // vldrw.u32 q1, [r0, #(4*1*16)]           // ............e.........'.................~.........
        // vldrw.u32 q2, [r0, #(4*2*16)]           // ......................'.*.........................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // e.....................'.....~.....................
        // vmul.s32       q4,  q2, r2              // ......................'..*........................
        // vqrdmulh.s32   q2,  q2, r3              // ......................'....*......................
        // vmla.s32       q4,  q2, r12             // .~....................'......*....................
        // vsub.u32       q2,    q0, q4            // ......~...............'...........*...............
        // vadd.u32       q0,    q0, q4            // ....~.................'.........*.................
        // vmul.s32       q4,  q3, r2              // .........e............'..............~............
        // vqrdmulh.s32   q3,  q3, r3              // .......e..............'............~..............
        // vmla.s32       q4,  q3, r12             // .................e....'......................~....
        // vsub.u32       q3,    q1, q4            // ......................'...*.......................
        // vadd.u32       q1,    q1, q4            // ......................*...........................
        // vmul.s32       q4,  q1, r4              // ...~..................'........*..................
        // vqrdmulh.s32   q1,  q1, r5              // ......................'*..........................
        // vmla.s32       q4,  q1, r12             // .....~................'..........*................
        // vsub.u32       q1,    q0, q4            // ..........~...........'...............*...........
        // vadd.u32       q0,    q0, q4            // ........~.............'.............*.............
        // vmul.s32       q4,  q3, r6              // ...........~..........'................*..........
        // vqrdmulh.s32   q3,  q3, r7              // .............~........'..................*........
        // vmla.s32       q4,  q3, r12             // ...............~......'....................*......
        // vsub.u32       q3,    q2, q4            // ....................~.'.........................*.
        // vadd.u32       q2,    q2, q4            // ..................~...'.......................*...
        // vstrw.u32 q0, [r0], #16                 // ..............~.......'...................*.......
        // vstrw.u32 q1, [r0, #(4*1*16 - 16)]      // ................~.....'.....................*.....
        // vstrw.u32 q2, [r0, #(4*2*16 - 16)]      // ...................~..'........................*..
        // vstrw.u32 q3, [r0, #(4*3*16 - 16)]      // .....................~'..........................*

        le lr, layer34_loop
                                                  // Instructions:    23
                                                  // Expected cycles: 24
                                                  // Expected IPC:    0.96
                                                  //
                                                  // Wall time:     0.12s
                                                  // User time:     0.12s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vldrw.U32 q4, [r0, #(4*2*16)]             // *.............................
        vqrdmulh.S32 q7, q4, r3                   // .*............................
        vadd.U32 q5, q3, q0                       // ..*...........................
        vqrdmulh.S32 q2, q5, r5                   // ...*..........................
        vmul.S32 q1, q4, r2                       // .....*........................
        vsub.U32 q0, q3, q0                       // ......*.......................
        vmla.S32 q1, q7, r12                      // .......*......................
        vldrw.U32 q7, [r0]                        // ........*.....................
        vmul.S32 q6, q5, r4                       // .........*....................
        vadd.U32 q4, q7, q1                       // ..........*...................
        vmla.S32 q6, q2, r12                      // ...........*..................
        vsub.U32 q7, q7, q1                       // ............*.................
        vmul.S32 q1, q0, r6                       // .............*................
        vsub.U32 q5, q4, q6                       // ..............*...............
        vqrdmulh.S32 q3, q0, r7                   // ...............*..............
        vadd.U32 q2, q4, q6                       // ................*.............
        vstrw.U32 q2, [r0], #16                   // .................*............
        vmla.S32 q1, q3, r12                      // ..................*...........
        vstrw.U32 q5, [r0, #(4*1*16 - 16)]        // ...................*..........
        vsub.U32 q6, q7, q1                       // ....................*.........
        vstrw.U32 q6, [r0, #(4*3*16 - 16)]        // .....................*........
        vadd.U32 q2, q7, q1                       // ......................*.......
        vstrw.U32 q2, [r0, #(4*2*16 - 16)]        // .......................*......

                                                   // ------ cycle (expected) ------>
                                                   // 0                        25
                                                   // |------------------------|-----
        // vadd.U32 q6, q3, q0                     // ..*............................
        // vqrdmulh.S32 q4, q6, r5                 // ...*...........................
        // vldrw.U32 q2, [r0, #(4*2*16)]           // *..............................
        // vmul.S32 q1, q2, r2                     // .....*.........................
        // vsub.U32 q7, q3, q0                     // ......*........................
        // vqrdmulh.S32 q3, q2, r3                 // .*.............................
        // vmla.S32 q1, q3, r12                    // .......*.......................
        // vldrw.U32 q5, [r0]                      // ........*......................
        // vmul.S32 q6, q6, r4                     // .........*.....................
        // vadd.U32 q3, q5, q1                     // ..........*....................
        // vmla.S32 q6, q4, r12                    // ...........*...................
        // vsub.U32 q2, q5, q1                     // ............*..................
        // vadd.U32 q4, q3, q6                     // ................*..............
        // vsub.U32 q1, q3, q6                     // ..............*................
        // vmul.S32 q6, q7, r6                     // .............*.................
        // vqrdmulh.S32 q7, q7, r7                 // ...............*...............
        // vstrw.U32 q4, [r0], #16                 // .................*.............
        // vmla.S32 q6, q7, r12                    // ..................*............
        // vstrw.U32 q1, [r0, #(4*1*16 - 16)]      // ...................*...........
        // vadd.U32 q4, q2, q6                     // ......................*........
        // vstrw.U32 q4, [r0, #(4*2*16 - 16)]      // .......................*.......
        // vsub.U32 q6, q2, q6                     // ....................*..........
        // vstrw.U32 q6, [r0, #(4*3*16 - 16)]      // .....................*.........


        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        // Layers 5,6
        sub in, in, #(4*256)

        mov lr, #16
                                            // Instructions:    7
                                            // Expected cycles: 7
                                            // Expected IPC:    1.00
                                            //
                                            // Wall time:     0.01s
                                            // User time:     0.01s
                                            //
                                            // ----- cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|----
        ldrd r6, r1, [r11], #+24            // *.............................
        vldrw.U32 q0, [r0, #(4*3*4)]        // .*............................
        vqrdmulh.S32 q1, q0, r1             // ..*...........................
        vldrw.U32 q2, [r0, #(4*1*4)]        // ...*..........................
        vmul.S32 q0, q0, r6                 // ....*.........................
        vldrw.U32 q4, [r0, #(4*2*4)]        // .....*........................
        vmla.S32 q0, q1, r12                // ......*.......................

                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        // ldrd r6, r1, [r11], #+24          // *..............................
        // vldrw.U32 q0, [r0, #(4*3*4)]      // .*.............................
        // vqrdmulh.S32 q2, q0, r1           // ..*............................
        // vmul.S32 q0, q0, r6               // ....*..........................
        // vmla.S32 q0, q2, r12              // ......*........................
        // vldrw.U32 q2, [r0, #(4*1*4)]      // ...*...........................
        // vldrw.U32 q4, [r0, #(4*2*4)]      // .....*.........................

        sub lr, lr, #1
.p2align 2
layer56_loop:
                                             // Instructions:    31
                                             // Expected cycles: 31
                                             // Expected IPC:    1.00
                                             //
                                             // Wall time:     3.63s
                                             // User time:     3.63s
                                             //
                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        vadd.U32 q7, q2, q0                  // *..............................
        vmul.S32 q1, q4, r6                  // .*.............................
        vsub.U32 q3, q2, q0                  // ..*............................
        vqrdmulh.S32 q2, q4, r1              // ...*...........................
        ldrd r2, r8, [r11, #(-16)]           // ....*..........................
        vmla.S32 q1, q2, r12                 // .....*.........................
        ldrd r6, r1, [r11], #+24             // ......e........................
        vmul.S32 q6, q7, r2                  // .......*.......................
        vldrw.U32 q0, [r0, #(4*3*4)]         // ........e......................
        vqrdmulh.S32 q2, q0, r1              // .........e.....................
        vldrw.U32 q5, [r0]                   // ..........*....................
        vmul.S32 q0, q0, r6                  // ...........e...................
        vadd.U32 q4, q5, q1                  // ............*..................
        vmla.S32 q0, q2, r12                 // .............e.................
        vsub.U32 q1, q5, q1                  // ..............*................
        vqrdmulh.S32 q5, q7, r8              // ...............*...............
        vldrw.U32 q2, [r0, #(4*1*4)]         // ................e..............
        vmla.S32 q6, q5, r12                 // .................*.............
        ldrd r9, r4, [r11, #(-8)]            // ..................*............
        vadd.U32 q5, q4, q6                  // ...................*...........
        vstrw.U32 q5, [r0], #64              // ....................*..........
        vmul.S32 q5, q3, r9                  // .....................*.........
        vsub.U32 q6, q4, q6                  // ......................*........
        vqrdmulh.S32 q3, q3, r4              // .......................*.......
        vldrw.U32 q4, [r0, #(4*2*4)]         // ........................e......
        vmla.S32 q5, q3, r12                 // .........................*.....
        vstrw.U32 q6, [r0, #(-64+16)]        // ..........................*....
        vadd.U32 q3, q1, q5                  // ...........................*...
        vstrw.U32 q3, [r0, #(-64+32)]        // ............................*..
        vsub.U32 q6, q1, q5                  // .............................*.
        vstrw.U32 q6, [r0, #(-64+48)]        // ..............................*

                                              // ------------------ cycle (expected) ------------------->
                                              // 0                        25                       50
                                              // |------------------------|------------------------|-----
        // ldrd r2, r3, [r11], #+24           // e........................'.....~........................
        // ldrd r4, r5, [r11, #(-16)]         // .........................'...*..........................
        // ldrd r6, r7, [r11, #(-8)]          // ............~............'.................*............
        // vldrw.u32 q0, [r0]                 // ....~....................'.........*....................
        // vldrw.u32 q1, [r0, #(4*1*4)]       // ..........e..............'...............~..............
        // vldrw.u32 q2, [r0, #(4*2*4)]       // ..................e......'.......................~......
        // vldrw.u32 q3, [r0, #(4*3*4)]       // ..e......................'.......~......................
        // vmul.s32       q4,  q2, r2         // .........................'*.............................
        // vqrdmulh.s32   q2,  q2, r3         // .........................'..*...........................
        // vmla.s32       q4,  q2, r12        // .........................'....*.........................
        // vsub.u32       q2,    q0, q4       // ........~................'.............*................
        // vadd.u32       q0,    q0, q4       // ......~..................'...........*..................
        // vmul.s32       q4,  q3, r2         // .....e...................'..........~...................
        // vqrdmulh.s32   q3,  q3, r3         // ...e.....................'........~.....................
        // vmla.s32       q4,  q3, r12        // .......e.................'............~.................
        // vsub.u32       q3,    q1, q4       // .........................'.*............................
        // vadd.u32       q1,    q1, q4       // .........................*..............................
        // vmul.s32       q4,  q1, r4         // .~.......................'......*.......................
        // vqrdmulh.s32   q1,  q1, r5         // .........~...............'..............*...............
        // vmla.s32       q4,  q1, r12        // ...........~.............'................*.............
        // vsub.u32       q1,    q0, q4       // ................~........'.....................*........
        // vadd.u32       q0,    q0, q4       // .............~...........'..................*...........
        // vmul.s32       q4,  q3, r6         // ...............~.........'....................*.........
        // vqrdmulh.s32   q3,  q3, r7         // .................~.......'......................*.......
        // vmla.s32       q4,  q3, r12        // ...................~.....'........................*.....
        // vsub.u32       q3,    q2, q4       // .......................~.'............................*.
        // vadd.u32       q2,    q2, q4       // .....................~...'..........................*...
        // vstrw.u32 q0, [r0], #64            // ..............~..........'...................*..........
        // vstrw.u32 q1, [r0, #(-64+16)]      // ....................~....'.........................*....
        // vstrw.u32 q2, [r0, #(-64+32)]      // ......................~..'...........................*..
        // vstrw.u32 q3, [r0, #(-64+48)]      // ........................~'.............................*

        le lr, layer56_loop
        layer56_loop_end:
                                                 // Instructions:    33
                                                 // Expected cycles: 36
                                                 // Expected IPC:    0.92
                                                 //
                                                 // Wall time:     0.33s
                                                 // User time:     0.33s
                                                 //
                                                 // -------- cycle (expected) --------->
                                                 // 0                        25
                                                 // |------------------------|----------
        vqrdmulh.S32 q3, q4, r1                  // *...................................
        ldrd r8, r9, [r11, #(-8)]                // .*..................................
        vmul.S32 q4, q4, r6                      // ..*.................................
        ldrd r10, r2, [r11, #(-16)]              // ...*................................
        vldrw.U32 q6, [r0]                       // ....*...............................
        vadd.U32 q1, q2, q0                      // .....*..............................
        vmla.S32 q4, q3, r12                     // ......*.............................
        vsub.U32 q2, q2, q0                      // .......*............................
        vqrdmulh.S32 q3, q1, r2                  // ........*...........................
        vsub.U32 q7, q6, q4                      // .........*..........................
        vmul.S32 q1, q1, r10                     // ..........*.........................
        vadd.U32 q4, q6, q4                      // ...........*........................
        vmla.S32 q1, q3, r12                     // ............*.......................
        vldrw.U32 q0, [r11, #(+16-96)]           // .............*......................
        vqrdmulh.S32 q3, q2, r9                  // ..............*.....................
        vsub.U32 q6, q4, q1                      // ...............*....................
        vmul.S32 q5, q2, r8                      // ................*...................
        vstrw.U32 q6, [r0, #(-64+16)]            // .................*..................
        vadd.U32 q6, q4, q1                      // ..................*.................
        vmla.S32 q5, q3, r12                     // ...................*................
        vstrw.U32 q6, [r0], #64                  // ....................*...............
        vsub.U32 q4, q7, q5                      // .....................*..............
        vstrw.U32 q4, [r0, #(-64+48)]            // ......................*.............
        vadd.U32 q6, q7, q5                      // .......................*............
        vstrw.U32 q6, [r0, #(-64+32)]            // ........................*...........
        mov r14, #16                             // .........................*..........
        sub r0, r0, #(4*256)                     // ..........................*.........
        vld40.U32 {q2, q3, q4, q5}, [r0]         // ...........................*........
        sub r14, r14, #1                         // ............................*.......
        vld41.U32 {q2, q3, q4, q5}, [r0]         // .............................*......
        vld42.U32 {q2, q3, q4, q5}, [r0]         // ...............................*....
        vld43.U32 {q2, q3, q4, q5}, [r0]!        // .................................*..
        vqrdmulh.S32 q6, q5, q0                  // ...................................*

                                                  // -------- cycle (expected) --------->
                                                  // 0                        25
                                                  // |------------------------|----------
        // vadd.U32 q7, q2, q0                    // .....*..............................
        // vmul.S32 q1, q4, r6                    // ..*.................................
        // vsub.U32 q3, q2, q0                    // .......*............................
        // vqrdmulh.S32 q2, q4, r1                // *...................................
        // ldrd r2, r8, [r11, #(-16)]             // ...*................................
        // vmla.S32 q1, q2, r12                   // ......*.............................
        // vmul.S32 q6, q7, r2                    // ..........*.........................
        // vldrw.U32 q5, [r0]                     // ....*...............................
        // vadd.U32 q4, q5, q1                    // ...........*........................
        // vsub.U32 q1, q5, q1                    // .........*..........................
        // vqrdmulh.S32 q5, q7, r8                // ........*...........................
        // vmla.S32 q6, q5, r12                   // ............*.......................
        // ldrd r9, r4, [r11, #(-8)]              // .*..................................
        // vadd.U32 q5, q4, q6                    // ..................*.................
        // vstrw.U32 q5, [r0], #64                // ....................*...............
        // vmul.S32 q5, q3, r9                    // ................*...................
        // vsub.U32 q6, q4, q6                    // ...............*....................
        // vqrdmulh.S32 q3, q3, r4                // ..............*.....................
        // vmla.S32 q5, q3, r12                   // ...................*................
        // vstrw.U32 q6, [r0, #(-64+16)]          // .................*..................
        // vadd.U32 q3, q1, q5                    // .......................*............
        // vstrw.U32 q3, [r0, #(-64+32)]          // ........................*...........
        // vsub.U32 q6, q1, q5                    // .....................*..............
        // vstrw.U32 q6, [r0, #(-64+48)]          // ......................*.............
        // sub r0, r0, #(4*256)                   // ..........................*.........
        // mov r14, #16                           // .........................*..........
        // vld40.U32 {q2, q3, q4, q5}, [r0]       // ...........................*........
        // vld41.U32 {q2, q3, q4, q5}, [r0]       // .............................*......
        // vld42.U32 {q2, q3, q4, q5}, [r0]       // ...............................*....
        // vld43.U32 {q2, q3, q4, q5}, [r0]!      // .................................*..
        // vldrw.U32 q0, [r11, #(+16-96)]         // .............*......................
        // vqrdmulh.S32 q6, q5, q0                // ...................................*
        // sub r14, r14, #1                       // ............................*.......

        layer78_loop:

                                                 // Instructions:    34
                                                 // Expected cycles: 34
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     14.98s
                                                 // User time:     14.98s
                                                 //
                                                 // ------- cycle (expected) -------->
                                                 // 0                        25
                                                 // |------------------------|--------
        vqrdmulh.S32 q7, q4, q0                  // *.................................
        vldrw.U32 q1, [r11], #+96                // .*................................
        vmul.S32 q4, q4, q1                      // ..*...............................
        vldrw.U32 q0, [r11, #(64-96)]            // ...*..............................
        vmul.S32 q1, q5, q1                      // ....*.............................
        vldrw.U32 q5, [r11, #(32 - 96)]          // .....*............................
        vmla.S32 q1, q6, r12                     // ......*...........................
        vldrw.U32 q6, [r11, #(48 - 96)]          // .......*..........................
        vmla.S32 q4, q7, r12                     // ........*.........................
        vadd.U32 q7, q3, q1                      // .........*........................
        vqrdmulh.S32 q6, q7, q6                  // ..........*.......................
        vsub.U32 q1, q3, q1                      // ...........*......................
        vmul.S32 q5, q7, q5                      // ............*.....................
        vadd.U32 q3, q2, q4                      // .............*....................
        vmla.S32 q5, q6, r12                     // ..............*...................
        vldrw.U32 q7, [r11, #(80-96)]            // ...............*..................
        vadd.U32 q6, q3, q5                      // ................*.................
        vstrw.32 q6, [r0, #( 0 - 64)]            // .................*................
        vsub.U32 q6, q3, q5                      // ..................*...............
        vstrw.32 q6, [r0, #(16 - 64)]            // ...................*..............
        vsub.U32 q6, q2, q4                      // ....................*.............
        vld40.U32 {q2, q3, q4, q5}, [r0]         // .....................e............
        vmul.S32 q0, q1, q0                      // ......................*...........
        vld41.U32 {q2, q3, q4, q5}, [r0]         // .......................e..........
        vqrdmulh.S32 q1, q1, q7                  // ........................*.........
        vld42.U32 {q2, q3, q4, q5}, [r0]         // .........................e........
        vmla.S32 q0, q1, r12                     // ..........................*.......
        vld43.U32 {q2, q3, q4, q5}, [r0]!        // ...........................e......
        vsub.U32 q7, q6, q0                      // ............................*.....
        vstrw.32 q7, [r0, #(48 - 64)]            // .............................*....
        vadd.U32 q1, q6, q0                      // ..............................*...
        vldrw.U32 q0, [r11, #(+16-96)]           // ...............................e..
        vqrdmulh.S32 q6, q5, q0                  // ................................e.
        vstrw.32 q1, [r0, #(32 - 64)]            // .................................*

                                                        // -------------- cycle (expected) -------------->
                                                        // 0                        25
                                                        // |------------------------|---------------------
        // vld40.u32 {q0, q1, q2, q3}, [r0]             // e............'....................~............
        // vld41.u32 {q0, q1, q2, q3}, [r0]             // ..e..........'......................~..........
        // vld42.u32 {q0, q1, q2, q3}, [r0]             // ....e........'........................~........
        // vld43.u32 {q0, q1, q2, q3}, [r0]!            // ......e......'..........................~......
        // vldrw.u32 q5,         [r11], #+96            // .............'*................................
        // vldrw.u32 q6, [r11, #(+16-96)]               // ..........e..'..............................~..
        // vmul.s32       q4,  q2, q5                   // .............'.*...............................
        // vqrdmulh.s32   q2,  q2, q6                   // .............*.................................
        // vmla.s32       q4,  q2, r12                  // .............'.......*.........................
        // vsub.u32       q2,    q0, q4                 // .............'...................*.............
        // vadd.u32       q0,    q0, q4                 // .............'............*....................
        // vmul.s32       q4,  q3, q5                   // .............'...*.............................
        // vqrdmulh.s32   q3,  q3, q6                   // ...........e.'...............................~.
        // vmla.s32       q4,  q3, r12                  // .............'.....*...........................
        // vsub.u32       q3,    q1, q4                 // .............'..........*......................
        // vadd.u32       q1,    q1, q4                 // .............'........*........................
        // vldrw.u32 q5,         [r11, #(32 - 96)]      // .............'....*............................
        // vldrw.u32 q6, [r11, #(48 - 96)]              // .............'......*..........................
        // vmul.s32       q4,  q1, q5                   // .............'...........*.....................
        // vqrdmulh.s32   q1,  q1, q6                   // .............'.........*.......................
        // vmla.s32       q4,  q1, r12                  // .............'.............*...................
        // vsub.u32       q1,    q0, q4                 // .............'.................*...............
        // vadd.u32       q0,    q0, q4                 // .............'...............*.................
        // vldrw.u32 q5,         [r11, #(64-96)]        // .............'..*..............................
        // vldrw.u32 q6, [r11, #(80-96)]                // .............'..............*..................
        // vmul.s32       q4,  q3, q5                   // .~...........'.....................*...........
        // vqrdmulh.s32   q3,  q3, q6                   // ...~.........'.......................*.........
        // vmla.s32       q4,  q3, r12                  // .....~.......'.........................*.......
        // vsub.u32       q3,    q2, q4                 // .......~.....'...........................*.....
        // vadd.u32       q2,    q2, q4                 // .........~...'.............................*...
        // vstrw.32 q0, [r0, #( 0 - 64)]                // .............'................*................
        // vstrw.32 q1, [r0, #(16 - 64)]                // .............'..................*..............
        // vstrw.32 q2, [r0, #(32 - 64)]                // ............~'................................*
        // vstrw.32 q3, [r0, #(48 - 64)]                // ........~....'............................*....

        le lr, layer78_loop
                                               // Instructions:    28
                                               // Expected cycles: 28
                                               // Expected IPC:    1.00
                                               //
                                               // Wall time:     0.25s
                                               // User time:     0.25s
                                               //
                                               // ----- cycle (expected) ------>
                                               // 0                        25
                                               // |------------------------|----
        vqrdmulh.S32 q7, q4, q0                // *.............................
        vldrw.U32 q1, [r11], #+96              // .*............................
        vmul.S32 q5, q5, q1                    // ..*...........................
        vldrw.U32 q0, [r11, #(80-96)]          // ...*..........................
        vmla.S32 q5, q6, r12                   // ....*.........................
        vldrw.U32 q6, [r11, #(48 - 96)]        // .....*........................
        vmul.S32 q1, q4, q1                    // ......*.......................
        vsub.U32 q4, q3, q5                    // .......*......................
        vqrdmulh.S32 q0, q4, q0                // ........*.....................
        vadd.U32 q3, q3, q5                    // .........*....................
        vmla.S32 q1, q7, r12                   // ..........*...................
        vldrw.U32 q5, [r11, #(64-96)]          // ...........*..................
        vmul.S32 q5, q4, q5                    // ............*.................
        vsub.U32 q7, q2, q1                    // .............*................
        vmla.S32 q5, q0, r12                   // ..............*...............
        vldrw.U32 q4, [r11, #(32 - 96)]        // ...............*..............
        vadd.U32 q0, q7, q5                    // ................*.............
        vqrdmulh.S32 q6, q3, q6                // .................*............
        vstrw.32 q0, [r0, #(32 - 64)]          // ..................*...........
        vsub.U32 q0, q7, q5                    // ...................*..........
        vmul.S32 q4, q3, q4                    // ....................*.........
        vadd.U32 q5, q2, q1                    // .....................*........
        vmla.S32 q4, q6, r12                   // ......................*.......
        vstrw.32 q0, [r0, #(48 - 64)]          // .......................*......
        vsub.U32 q3, q5, q4                    // ........................*.....
        vstrw.32 q3, [r0, #(16 - 64)]          // .........................*....
        vadd.U32 q0, q5, q4                    // ..........................*...
        vstrw.32 q0, [r0, #( 0 - 64)]          // ...........................*..

                                                // ------ cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|-----
        // vqrdmulh.S32 q7, q4, q0              // *..............................
        // vldrw.U32 q1, [r11], #+96            // .*.............................
        // vmul.S32 q4, q4, q1                  // ......*........................
        // vldrw.U32 q0, [r11, #(64-96)]        // ...........*...................
        // vmul.S32 q1, q5, q1                  // ..*............................
        // vldrw.U32 q5, [r11, #(32 - 96)]      // ...............*...............
        // vmla.S32 q1, q6, r12                 // ....*..........................
        // vldrw.U32 q6, [r11, #(48 - 96)]      // .....*.........................
        // vmla.S32 q4, q7, r12                 // ..........*....................
        // vadd.U32 q7, q3, q1                  // .........*.....................
        // vqrdmulh.S32 q6, q7, q6              // .................*.............
        // vsub.U32 q1, q3, q1                  // .......*.......................
        // vmul.S32 q5, q7, q5                  // ....................*..........
        // vadd.U32 q3, q2, q4                  // .....................*.........
        // vmla.S32 q5, q6, r12                 // ......................*........
        // vldrw.U32 q7, [r11, #(80-96)]        // ...*...........................
        // vadd.U32 q6, q3, q5                  // ..........................*....
        // vstrw.32 q6, [r0, #( 0 - 64)]        // ...........................*...
        // vsub.U32 q6, q3, q5                  // ........................*......
        // vstrw.32 q6, [r0, #(16 - 64)]        // .........................*.....
        // vsub.U32 q6, q2, q4                  // .............*.................
        // vmul.S32 q0, q1, q0                  // ............*..................
        // vqrdmulh.S32 q1, q1, q7              // ........*......................
        // vmla.S32 q0, q1, r12                 // ..............*................
        // vsub.U32 q7, q6, q0                  // ...................*...........
        // vstrw.32 q7, [r0, #(48 - 64)]        // .......................*.......
        // vadd.U32 q1, q6, q0                  // ................*..............
        // vstrw.32 q1, [r0, #(32 - 64)]        // ..................*............


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
