
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots:
#include "ntt_n256_l8_s32_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s32       \dst,  \src, \const
        vqrdmulh.s32   \src,  \src, \const_twisted
        vmla.s32       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_twisted
        mulmod tmp, \b, \root, \root_twisted
        vsub.u32       \b,    \a, tmp
        vadd.u32       \a,    \a, tmp
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_n256_u32_33556993_28678040_complete_manual, %function
.global ntt_n256_u32_33556993_28678040_complete_manual
ntt_n256_u32_33556993_28678040_complete_manual:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus  .req r12
        root_ptr .req r11

        .equ modulus_const, -33556993
        movw modulus, #:lower16:modulus_const
        movt modulus, #:upper16:modulus_const
        ldr  root_ptr, roots_addr

        in_low       .req r0
        in_high      .req r1

        add in_high, in_low, #(4*128)

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        // Layers 1-2

        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #16
                                           // Instructions:    5
                                           // Expected cycles: 6
                                           // Expected IPC:    0.83
                                           //
                                           // Wall time:     0.00s
                                           // User time:     0.00s
                                           //
                                           // ----- cycle (expected) ------>
                                           // 0                        25
                                           // |------------------------|----
        vldrw.U32 q0, [r1, #(4*64)]        // *.............................
        vqrdmulh.S32 q1, q0, r3            // .*............................
        vmul.S32 q0, q0, r2                // ...*..........................
        vldrw.U32 q2, [r0, #(4*64)]        // ....*.........................
        vmla.S32 q0, q1, r12               // .....*........................

                                            // ------ cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|-----
        // vldrw.U32 q0, [r1, #(4*64)]      // *..............................
        // vqrdmulh.S32 q7, q0, r3          // .*.............................
        // vldrw.U32 q2, [r0, #(4*64)]      // ....*..........................
        // vmul.S32 q0, q0, r2              // ...*...........................
        // vmla.S32 q0, q7, r12             // .....*.........................

        sub lr, lr, #1
.p2align 2
layer12_loop:
                                                // Instructions:    28
                                                // Expected cycles: 28
                                                // Expected IPC:    1.00
                                                //
                                                // Wall time:     2.77s
                                                // User time:     2.77s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vadd.U32 q4, q2, q0                     // *.............................
        vmul.S32 q6, q4, r4                     // .*............................
        vldrw.U32 q5, [r1]                      // ..*...........................
        vqrdmulh.S32 q7, q5, r3                 // ...*..........................
        vldrw.U32 q3, [r0]                      // ....*.........................
        vmul.S32 q1, q5, r2                     // .....*........................
        vsub.U32 q5, q2, q0                     // ......*.......................
        vmla.S32 q1, q7, r12                    // .......*......................
        vldrw.U32 q0, [r1, #(4*64)]             // ........e.....................
        vqrdmulh.S32 q7, q4, r5                 // .........*....................
        vsub.U32 q4, q3, q1                     // ..........*...................
        vmla.S32 q6, q7, r12                    // ...........*..................
        vadd.U32 q7, q3, q1                     // ............*.................
        vqrdmulh.S32 q1, q5, r7                 // .............*................
        vsub.U32 q3, q7, q6                     // ..............*...............
        vmul.S32 q5, q5, r6                     // ...............*..............
        vadd.U32 q6, q7, q6                     // ................*.............
        vqrdmulh.S32 q7, q0, r3                 // .................e............
        vldrw.U32 q2, [r0, #(4*64)]             // ..................e...........
        vmul.S32 q0, q0, r2                     // ...................e..........
        vstrw.U32 q6, [r0], #16                 // ....................*.........
        vmla.S32 q5, q1, r12                    // .....................*........
        vstrw.U32 q3, [r0, #(4*64 - 16)]        // ......................*.......
        vmla.S32 q0, q7, r12                    // .......................e......
        vsub.U32 q6, q4, q5                     // ........................*.....
        vstrw.U32 q6, [r1, #(4*64-16)]          // .........................*....
        vadd.U32 q7, q4, q5                     // ..........................*...
        vstrw.U32 q7, [r1], #16                 // ...........................*..

                                                 // -------------- cycle (expected) --------------->
                                                 // 0                        25
                                                 // |------------------------|----------------------
        // vldrw.u32 q0, [r0]                    // ....................'...*.......................
        // vldrw.u32 q1, [r0, #(4*64)]           // ..........e.........'.................~.........
        // vldrw.u32 q2, [r1]                    // ....................'.*.........................
        // vldrw.u32 q3, [r1, #(4*64)]           // e...................'.......~...................
        // vmul.s32       q4,  q2, r2            // ....................'....*......................
        // vqrdmulh.s32   q2,  q2, r3            // ....................'..*........................
        // vmla.s32       q4,  q2, r12           // ....................'......*....................
        // vsub.u32       q2,    q0, q4          // ..~.................'.........*.................
        // vadd.u32       q0,    q0, q4          // ....~...............'...........*...............
        // vmul.s32       q4,  q3, r2            // ...........e........'..................~........
        // vqrdmulh.s32   q3,  q3, r3            // .........e..........'................~..........
        // vmla.s32       q4,  q3, r12           // ...............e....'......................~....
        // vsub.u32       q3,    q1, q4          // ....................'.....*.....................
        // vadd.u32       q1,    q1, q4          // ....................*...........................
        // vmul.s32       q4,  q1, r4            // ....................'*..........................
        // vqrdmulh.s32   q1,  q1, r5            // .~..................'........*..................
        // vmla.s32       q4,  q1, r12           // ...~................'..........*................
        // vsub.u32       q1,    q0, q4          // ......~.............'.............*.............
        // vadd.u32       q0,    q0, q4          // ........~...........'...............*...........
        // vmul.s32       q4,  q3, r6            // .......~............'..............*............
        // vqrdmulh.s32   q3,  q3, r7            // .....~..............'............*..............
        // vmla.s32       q4,  q3, r12           // .............~......'....................*......
        // vsub.u32       q3,    q2, q4          // ................~...'.......................*...
        // vadd.u32       q2,    q2, q4          // ..................~.'.........................*.
        // vstrw.u32 q0, [r0], #16               // ............~.......'...................*.......
        // vstrw.u32 q1, [r0, #(4*64 - 16)]      // ..............~.....'.....................*.....
        // vstrw.u32 q2, [r1], #16               // ...................~'..........................*
        // vstrw.u32 q3, [r1, #(4*64-16)]        // .................~..'........................*..

        le lr, layer12_loop
                                                // Instructions:    23
                                                // Expected cycles: 24
                                                // Expected IPC:    0.96
                                                //
                                                // Wall time:     0.12s
                                                // User time:     0.12s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vldrw.U32 q5, [r1]                      // *.............................
        vmul.S32 q6, q5, r2                     // .*............................
        vldrw.U32 q3, [r0]                      // ..*...........................
        vqrdmulh.S32 q5, q5, r3                 // ...*..........................
        vsub.U32 q7, q2, q0                     // ....*.........................
        vmla.S32 q6, q5, r12                    // .....*........................
        vadd.U32 q5, q2, q0                     // ......*.......................
        vqrdmulh.S32 q0, q5, r5                 // .......*......................
        vadd.U32 q4, q3, q6                     // ........*.....................
        vmul.S32 q2, q5, r4                     // .........*....................
        vsub.U32 q3, q3, q6                     // ..........*...................
        vmla.S32 q2, q0, r12                    // ...........*..................
        vqrdmulh.S32 q5, q7, r7                 // .............*................
        vsub.U32 q0, q4, q2                     // ..............*...............
        vmul.S32 q6, q7, r6                     // ...............*..............
        vstrw.U32 q0, [r0, #(4*64 - 16)]        // ................*.............
        vmla.S32 q6, q5, r12                    // .................*............
        vadd.U32 q5, q4, q2                     // ..................*...........
        vstrw.U32 q5, [r0], #16                 // ...................*..........
        vsub.U32 q0, q3, q6                     // ....................*.........
        vstrw.U32 q0, [r1, #(4*64-16)]          // .....................*........
        vadd.U32 q6, q3, q6                     // ......................*.......
        vstrw.U32 q6, [r1], #16                 // .......................*......

                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        // vadd.U32 q4, q2, q0                   // ......*........................
        // vmul.S32 q6, q4, r4                   // .........*.....................
        // vldrw.U32 q5, [r1]                    // *..............................
        // vqrdmulh.S32 q7, q5, r3               // ...*...........................
        // vldrw.U32 q3, [r0]                    // ..*............................
        // vmul.S32 q1, q5, r2                   // .*.............................
        // vsub.U32 q5, q2, q0                   // ....*..........................
        // vmla.S32 q1, q7, r12                  // .....*.........................
        // vqrdmulh.S32 q7, q4, r5               // .......*.......................
        // vsub.U32 q4, q3, q1                   // ..........*....................
        // vmla.S32 q6, q7, r12                  // ...........*...................
        // vadd.U32 q7, q3, q1                   // ........*......................
        // vqrdmulh.S32 q1, q5, r7               // .............*.................
        // vsub.U32 q3, q7, q6                   // ..............*................
        // vmul.S32 q5, q5, r6                   // ...............*...............
        // vadd.U32 q6, q7, q6                   // ..................*............
        // vstrw.U32 q6, [r0], #16               // ...................*...........
        // vmla.S32 q5, q1, r12                  // .................*.............
        // vstrw.U32 q3, [r0, #(4*64 - 16)]      // ................*..............
        // vsub.U32 q6, q4, q5                   // ....................*..........
        // vstrw.U32 q6, [r1, #(4*64-16)]        // .....................*.........
        // vadd.U32 q7, q4, q5                   // ......................*........
        // vstrw.U32 q7, [r1], #16               // .......................*.......

        .unreq in_high
        .unreq in_low

        in .req r0
        sub in, in, #(64*4)

        // Layers 3,4

        // 4 butterfly blocks per root config, 4 root configs
        // loop over root configs

        count .req r1
        mov count, #4

out_start:
        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #4
                                             // Instructions:    5
                                             // Expected cycles: 6
                                             // Expected IPC:    0.83
                                             //
                                             // Wall time:     0.00s
                                             // User time:     0.00s
                                             //
                                             // ----- cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|----
        vldrw.U32 q0, [r0, #(4*3*16)]        // *.............................
        vqrdmulh.S32 q1, q0, r3              // .*............................
        vmul.S32 q0, q0, r2                  // ...*..........................
        vldrw.U32 q3, [r0, #(4*1*16)]        // ....*.........................
        vmla.S32 q0, q1, r12                 // .....*........................

                                              // ------ cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|-----
        // vldrw.U32 q3, [r0, #(4*3*16)]      // *..............................
        // vmul.S32 q0, q3, r2                // ...*...........................
        // vqrdmulh.S32 q2, q3, r3            // .*.............................
        // vldrw.U32 q3, [r0, #(4*1*16)]      // ....*..........................
        // vmla.S32 q0, q2, r12               // .....*.........................

        sub lr, lr, #1
.p2align 2
layer34_loop:
                                                  // Instructions:    28
                                                  // Expected cycles: 28
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     3.19s
                                                  // User time:     3.19s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vsub.U32 q4, q3, q0                       // *.............................
        vqrdmulh.S32 q2, q4, r7                   // .*............................
        vldrw.U32 q1, [r0, #(4*2*16)]             // ..*...........................
        vqrdmulh.S32 q7, q1, r3                   // ...*..........................
        vadd.U32 q6, q3, q0                       // ....*.........................
        vmul.S32 q1, q1, r2                       // .....*........................
        vldrw.U32 q3, [r0, #(4*3*16)]             // ......e.......................
        vmla.S32 q1, q7, r12                      // .......*......................
        vldrw.U32 q5, [r0]                        // ........*.....................
        vmul.S32 q0, q4, r6                       // .........*....................
        vsub.U32 q4, q5, q1                       // ..........*...................
        vmla.S32 q0, q2, r12                      // ...........*..................
        vadd.U32 q1, q5, q1                       // ............*.................
        vqrdmulh.S32 q2, q6, r5                   // .............*................
        vadd.U32 q7, q4, q0                       // ..............*...............
        vstrw.U32 q7, [r0, #(4*2*16 - 16)]        // ...............*..............
        vmul.S32 q6, q6, r4                       // ................*.............
        vsub.U32 q7, q4, q0                       // .................*............
        vmla.S32 q6, q2, r12                      // ..................*...........
        vstrw.U32 q7, [r0, #(4*3*16 - 16)]        // ...................*..........
        vmul.S32 q0, q3, r2                       // ....................e.........
        vsub.U32 q7, q1, q6                       // .....................*........
        vqrdmulh.S32 q2, q3, r3                   // ......................e.......
        vldrw.U32 q3, [r0, #(4*1*16)]             // .......................e......
        vmla.S32 q0, q2, r12                      // ........................e.....
        vstrw.U32 q7, [r0, #(4*1*16 - 16)]        // .........................*....
        vadd.U32 q5, q1, q6                       // ..........................*...
        vstrw.U32 q5, [r0], #16                   // ...........................*..

                                                   // --------------- cycle (expected) ---------------->
                                                   // 0                        25
                                                   // |------------------------|------------------------
        // vldrw.u32 q0, [r0]                      // ..~...................'.......*...................
        // vldrw.u32 q1, [r0, #(4*1*16)]           // .................e....'......................~....
        // vldrw.u32 q2, [r0, #(4*2*16)]           // ......................'.*.........................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // e.....................'.....~.....................
        // vmul.s32       q4,  q2, r2              // ......................'....*......................
        // vqrdmulh.s32   q2,  q2, r3              // ......................'..*........................
        // vmla.s32       q4,  q2, r12             // .~....................'......*....................
        // vsub.u32       q2,    q0, q4            // ....~.................'.........*.................
        // vadd.u32       q0,    q0, q4            // ......~...............'...........*...............
        // vmul.s32       q4,  q3, r2              // ..............e.......'...................~.......
        // vqrdmulh.s32   q3,  q3, r3              // ................e.....'.....................~.....
        // vmla.s32       q4,  q3, r12             // ..................e...'.......................~...
        // vsub.u32       q3,    q1, q4            // ......................*...........................
        // vadd.u32       q1,    q1, q4            // ......................'...*.......................
        // vmul.s32       q4,  q1, r4              // ..........~...........'...............*...........
        // vqrdmulh.s32   q1,  q1, r5              // .......~..............'............*..............
        // vmla.s32       q4,  q1, r12             // ............~.........'.................*.........
        // vsub.u32       q1,    q0, q4            // ...............~......'....................*......
        // vadd.u32       q0,    q0, q4            // ....................~.'.........................*.
        // vmul.s32       q4,  q3, r6              // ...~..................'........*..................
        // vqrdmulh.s32   q3,  q3, r7              // ......................'*..........................
        // vmla.s32       q4,  q3, r12             // .....~................'..........*................
        // vsub.u32       q3,    q2, q4            // ...........~..........'................*..........
        // vadd.u32       q2,    q2, q4            // ........~.............'.............*.............
        // vstrw.u32 q0, [r0], #16                 // .....................~'..........................*
        // vstrw.u32 q1, [r0, #(4*1*16 - 16)]      // ...................~..'........................*..
        // vstrw.u32 q2, [r0, #(4*2*16 - 16)]      // .........~............'..............*............
        // vstrw.u32 q3, [r0, #(4*3*16 - 16)]      // .............~........'..................*........

        le lr, layer34_loop
                                                  // Instructions:    23
                                                  // Expected cycles: 24
                                                  // Expected IPC:    0.96
                                                  //
                                                  // Wall time:     0.12s
                                                  // User time:     0.12s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vadd.U32 q7, q3, q0                       // *.............................
        vmul.S32 q6, q7, r4                       // .*............................
        vldrw.U32 q2, [r0, #(4*2*16)]             // ..*...........................
        vmul.S32 q1, q2, r2                       // ...*..........................
        vldrw.U32 q4, [r0]                        // ....*.........................
        vqrdmulh.S32 q5, q2, r3                   // .....*........................
        vsub.U32 q2, q3, q0                       // ......*.......................
        vmla.S32 q1, q5, r12                      // .......*......................
        vqrdmulh.S32 q0, q7, r5                   // .........*....................
        vadd.U32 q5, q4, q1                       // ..........*...................
        vmla.S32 q6, q0, r12                      // ...........*..................
        vsub.U32 q4, q4, q1                       // ............*.................
        vqrdmulh.S32 q1, q2, r7                   // .............*................
        vadd.U32 q0, q5, q6                       // ..............*...............
        vmul.S32 q3, q2, r6                       // ...............*..............
        vstrw.U32 q0, [r0], #16                   // ................*.............
        vsub.U32 q0, q5, q6                       // .................*............
        vmla.S32 q3, q1, r12                      // ..................*...........
        vstrw.U32 q0, [r0, #(4*1*16 - 16)]        // ...................*..........
        vadd.U32 q0, q4, q3                       // ....................*.........
        vstrw.U32 q0, [r0, #(4*2*16 - 16)]        // .....................*........
        vsub.U32 q0, q4, q3                       // ......................*.......
        vstrw.U32 q0, [r0, #(4*3*16 - 16)]        // .......................*......

                                                   // ------ cycle (expected) ------>
                                                   // 0                        25
                                                   // |------------------------|-----
        // vsub.U32 q4, q3, q0                     // ......*........................
        // vqrdmulh.S32 q2, q4, r7                 // .............*.................
        // vldrw.U32 q1, [r0, #(4*2*16)]           // ..*............................
        // vqrdmulh.S32 q7, q1, r3                 // .....*.........................
        // vadd.U32 q6, q3, q0                     // *..............................
        // vmul.S32 q1, q1, r2                     // ...*...........................
        // vmla.S32 q1, q7, r12                    // .......*.......................
        // vldrw.U32 q5, [r0]                      // ....*..........................
        // vmul.S32 q0, q4, r6                     // ...............*...............
        // vsub.U32 q4, q5, q1                     // ............*..................
        // vmla.S32 q0, q2, r12                    // ..................*............
        // vadd.U32 q1, q5, q1                     // ..........*....................
        // vqrdmulh.S32 q2, q6, r5                 // .........*.....................
        // vadd.U32 q7, q4, q0                     // ....................*..........
        // vstrw.U32 q7, [r0, #(4*2*16 - 16)]      // .....................*.........
        // vmul.S32 q6, q6, r4                     // .*.............................
        // vsub.U32 q7, q4, q0                     // ......................*........
        // vmla.S32 q6, q2, r12                    // ...........*...................
        // vstrw.U32 q7, [r0, #(4*3*16 - 16)]      // .......................*.......
        // vsub.U32 q7, q1, q6                     // .................*.............
        // vstrw.U32 q7, [r0, #(4*1*16 - 16)]      // ...................*...........
        // vadd.U32 q5, q1, q6                     // ..............*................
        // vstrw.U32 q5, [r0], #16                 // ................*..............


        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*256)

        // Layers 5,6

        // 1 butterfly blocks per root config, 16 root configs
        // loop over root configs

        mov lr, #16
                                            // Instructions:    7
                                            // Expected cycles: 7
                                            // Expected IPC:    1.00
                                            //
                                            // Wall time:     0.00s
                                            // User time:     0.00s
                                            //
                                            // ----- cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|----
        ldrd r6, r1, [r11], #+24            // *.............................
        vldrw.U32 q2, [r0, #(4*3*4)]        // .*............................
        vqrdmulh.S32 q1, q2, r1             // ..*...........................
        ldrd r9, r4, [r11, #(-8)]           // ...*..........................
        vmul.S32 q7, q2, r6                 // ....*.........................
        vldrw.U32 q2, [r0, #(4*1*4)]        // .....*........................
        vmla.S32 q7, q1, r12                // ......*.......................

                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        // vldrw.U32 q7, [r0, #(4*3*4)]      // .*.............................
        // vldrw.U32 q2, [r0, #(4*1*4)]      // .....*.........................
        // ldrd r9, r4, [r11, #(-8)]         // ...*...........................
        // ldrd r6, r1, [r11], #+24          // *..............................
        // vqrdmulh.S32 q0, q7, r1           // ..*............................
        // vmul.S32 q7, q7, r6               // ....*..........................
        // vmla.S32 q7, q0, r12              // ......*........................

        sub lr, lr, #1
.p2align 2
layer56_loop:
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     3.12s
                                                 // User time:     3.12s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vsub.U32 q1, q2, q7                      // *..............................
        vqrdmulh.S32 q6, q1, r4                  // .*.............................
        vldrw.U32 q0, [r0, #(4*2*4)]             // ..*............................
        vqrdmulh.S32 q3, q0, r1                  // ...*...........................
        vadd.U32 q4, q2, q7                      // ....*..........................
        vmul.S32 q0, q0, r6                      // .....*.........................
        ldrd r2, r8, [r11, #(-16)]               // ......*........................
        vmla.S32 q0, q3, r12                     // .......*.......................
        vldrw.U32 q2, [r0]                       // ........*......................
        vmul.S32 q3, q1, r9                      // .........*.....................
        vadd.U32 q1, q2, q0                      // ..........*....................
        vmla.S32 q3, q6, r12                     // ...........*...................
        vsub.U32 q6, q2, q0                      // ............*..................
        vmul.S32 q0, q4, r2                      // .............*.................
        vadd.U32 q5, q6, q3                      // ..............*................
        vqrdmulh.S32 q7, q4, r8                  // ...............*...............
        vsub.U32 q6, q6, q3                      // ................*..............
        vmla.S32 q0, q7, r12                     // .................*.............
        vldrw.U32 q7, [r0, #(4*3*4)]             // ..................e............
        vadd.U32 q3, q1, q0                      // ...................*...........
        vldrw.U32 q2, [r0, #(4*1*4)]             // ....................e..........
        vsub.U32 q4, q1, q0                      // .....................*.........
        ldrd r9, r4, [r11, #(-8)]                // ......................e........
        ldrd r6, r1, [r11], #+24                 // .......................e.......
        vst40.U32 {q3, q4, q5, q6}, [r0]         // ........................*......
        vqrdmulh.S32 q0, q7, r1                  // .........................e.....
        vst41.U32 {q3, q4, q5, q6}, [r0]         // ..........................*....
        vmul.S32 q7, q7, r6                      // ...........................e...
        vst42.U32 {q3, q4, q5, q6}, [r0]         // ............................*..
        vmla.S32 q7, q0, r12                     // .............................e.
        vst43.U32 {q3, q4, q5, q6}, [r0]!        // ..............................*

                                                  // ------------ cycle (expected) ------------->
                                                  // 0                        25
                                                  // |------------------------|------------------
        // ldrd r2, r3, [r11], #+24               // .....e.......'......................~.......
        // ldrd r4, r5, [r11, #(-16)]             // .............'.....*........................
        // ldrd r6, r7, [r11, #(-8)]              // ....e........'.....................~........
        // vldrw.u32 q0, [r0]                     // .............'.......*......................
        // vldrw.u32 q1, [r0, #(4*1*4)]           // ..e..........'...................~..........
        // vldrw.u32 q2, [r0, #(4*2*4)]           // .............'.*............................
        // vldrw.u32 q3, [r0, #(4*3*4)]           // e............'.................~............
        // vmul.s32       q4,  q2, r2             // .............'....*.........................
        // vqrdmulh.s32   q2,  q2, r3             // .............'..*...........................
        // vmla.s32       q4,  q2, r12            // .............'......*.......................
        // vsub.u32       q2,    q0, q4           // .............'...........*..................
        // vadd.u32       q0,    q0, q4           // .............'.........*....................
        // vmul.s32       q4,  q3, r2             // .........e...'..........................~...
        // vqrdmulh.s32   q3,  q3, r3             // .......e.....'........................~.....
        // vmla.s32       q4,  q3, r12            // ...........e.'............................~.
        // vsub.u32       q3,    q1, q4           // .............*..............................
        // vadd.u32       q1,    q1, q4           // .............'...*..........................
        // vmul.s32       q4,  q1, r4             // .............'............*.................
        // vqrdmulh.s32   q1,  q1, r5             // .............'..............*...............
        // vmla.s32       q4,  q1, r12            // .............'................*.............
        // vsub.u32       q1,    q0, q4           // ...~.........'....................*.........
        // vadd.u32       q0,    q0, q4           // .~...........'..................*...........
        // vmul.s32       q4,  q3, r6             // .............'........*.....................
        // vqrdmulh.s32   q3,  q3, r7             // .............'*.............................
        // vmla.s32       q4,  q3, r12            // .............'..........*...................
        // vsub.u32       q3,    q2, q4           // .............'...............*..............
        // vadd.u32       q2,    q2, q4           // .............'.............*................
        // vst40.u32 {q0, q1, q2, q3}, [r0]       // ......~......'.......................*......
        // vst41.u32 {q0, q1, q2, q3}, [r0]       // ........~....'.........................*....
        // vst42.u32 {q0, q1, q2, q3}, [r0]       // ..........~..'...........................*..
        // vst43.u32 {q0, q1, q2, q3}, [r0]!      // ............~'.............................*

        le lr, layer56_loop
                                                 // Instructions:    24
                                                 // Expected cycles: 31
                                                 // Expected IPC:    0.77
                                                 //
                                                 // Wall time:     0.11s
                                                 // User time:     0.11s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vldrw.U32 q0, [r0, #(4*2*4)]             // *..............................
        vmul.S32 q1, q0, r6                      // .*.............................
        vsub.U32 q3, q2, q7                      // ..*............................
        vqrdmulh.S32 q5, q0, r1                  // ...*...........................
        ldrd r6, r2, [r11, #(-16)]               // ....*..........................
        vmla.S32 q1, q5, r12                     // .....*.........................
        vadd.U32 q0, q2, q7                      // ......*........................
        vqrdmulh.S32 q6, q3, r4                  // .......*.......................
        vldrw.U32 q2, [r0]                       // ........*......................
        vmul.S32 q5, q3, r9                      // .........*.....................
        vsub.U32 q4, q2, q1                      // ..........*....................
        vmla.S32 q5, q6, r12                     // ...........*...................
        vadd.U32 q1, q2, q1                      // ............*..................
        vmul.S32 q7, q0, r6                      // .............*.................
        vadd.U32 q3, q4, q5                      // ..............*................
        vqrdmulh.S32 q0, q0, r2                  // ...............*...............
        vsub.U32 q4, q4, q5                      // ................*..............
        vmla.S32 q7, q0, r12                     // .................*.............
        vsub.U32 q2, q1, q7                      // ...................*...........
        vadd.U32 q1, q1, q7                      // .....................*.........
        vst40.U32 {q1, q2, q3, q4}, [r0]         // ........................*......
        vst41.U32 {q1, q2, q3, q4}, [r0]         // ..........................*....
        vst42.U32 {q1, q2, q3, q4}, [r0]         // ............................*..
        vst43.U32 {q1, q2, q3, q4}, [r0]!        // ..............................*

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vsub.U32 q1, q2, q7                    // ..*............................
        // vqrdmulh.S32 q6, q1, r4                // .......*.......................
        // vldrw.U32 q0, [r0, #(4*2*4)]           // *..............................
        // vqrdmulh.S32 q3, q0, r1                // ...*...........................
        // vadd.U32 q4, q2, q7                    // ......*........................
        // vmul.S32 q0, q0, r6                    // .*.............................
        // ldrd r2, r8, [r11, #(-16)]             // ....*..........................
        // vmla.S32 q0, q3, r12                   // .....*.........................
        // vldrw.U32 q2, [r0]                     // ........*......................
        // vmul.S32 q3, q1, r9                    // .........*.....................
        // vadd.U32 q1, q2, q0                    // ............*..................
        // vmla.S32 q3, q6, r12                   // ...........*...................
        // vsub.U32 q6, q2, q0                    // ..........*....................
        // vmul.S32 q0, q4, r2                    // .............*.................
        // vadd.U32 q5, q6, q3                    // ..............*................
        // vqrdmulh.S32 q7, q4, r8                // ...............*...............
        // vsub.U32 q6, q6, q3                    // ................*..............
        // vmla.S32 q0, q7, r12                   // .................*.............
        // vadd.U32 q3, q1, q0                    // .....................*.........
        // vsub.U32 q4, q1, q0                    // ...................*...........
        // vst40.U32 {q3, q4, q5, q6}, [r0]       // ........................*......
        // vst41.U32 {q3, q4, q5, q6}, [r0]       // ..........................*....
        // vst42.U32 {q3, q4, q5, q6}, [r0]       // ............................*..
        // vst43.U32 {q3, q4, q5, q6}, [r0]!      // ..............................*


        sub in, in, #(4*256)

        // Layers 7,8

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6

        mov lr, #16
                                              // Instructions:    8
                                              // Expected cycles: 9
                                              // Expected IPC:    0.89
                                              //
                                              // Wall time:     0.01s
                                              // User time:     0.01s
                                              //
                                              // ----- cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|----
        vldrw.U32 q4, [r11], #+96             // *.............................
        vldrw.U32 q6, [r0, #-16]              // ..*...........................
        vmul.S32 q0, q6, q4                   // ...*..........................
        vldrw.U32 q2, [r11, #(+16-96)]        // ....*.........................
        vqrdmulh.S32 q5, q6, q2               // .....*........................
        vldrw.U32 q6, [r0, #-32]              // ......*.......................
        vmla.S32 q0, q5, r12                  // .......*......................
        vldrw.U32 q5, [r0, #-48]              // ........*.....................

                                               // ------ cycle (expected) ------>
                                               // 0                        25
                                               // |------------------------|-----
        // vldrw.U32 q4, [r11], #+96           // *..............................
        // vldrw.U32 q5, [r0, #-48]            // ........*......................
        // vldrw.U32 q2, [r11, #(+16-96)]      // ....*..........................
        // vldrw.U32 q6, [r0, #-16]            // ..*............................
        // vqrdmulh.S32 q7, q6, q2             // .....*.........................
        // vmul.S32 q0, q6, q4                 // ...*...........................
        // vldrw.U32 q6, [r0, #-32]            // ......*........................
        // vmla.S32 q0, q7, r12                // .......*.......................

        sub lr, lr, #1
.p2align 2
layer78_loop:
                                                 // Instructions:    34
                                                 // Expected cycles: 34
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     13.34s
                                                 // User time:     13.34s
                                                 //
                                                 // ------- cycle (expected) -------->
                                                 // 0                        25
                                                 // |------------------------|--------
        vmul.S32 q1, q6, q4                      // *.................................
        vadd.U32 q3, q5, q0                      // .*................................
        vldrw.U32 q4, [r11], #+96                // ..e...............................
        vqrdmulh.S32 q7, q6, q2                  // ...*..............................
        vldrw.U32 q6, [r11, #(48 - 96)]          // ....*.............................
        vqrdmulh.S32 q2, q3, q6                  // .....*............................
        vldrw.U32 q6, [r0], #64                  // ......*...........................
        vmla.S32 q1, q7, r12                     // .......*..........................
        vsub.U32 q7, q5, q0                      // ........*.........................
        vldrw.U32 q0, [r11, #(32 - 96)]          // .........*........................
        vmul.S32 q0, q3, q0                      // ..........*.......................
        vldrw.U32 q5, [r0, #-48]                 // ...........e......................
        vsub.U32 q3, q6, q1                      // ............*.....................
        vmla.S32 q0, q2, r12                     // .............*....................
        vadd.U32 q1, q6, q1                      // ..............*...................
        vldrw.U32 q6, [r11, #(80-96)]            // ...............*..................
        vsub.U32 q2, q1, q0                      // ................*.................
        vstrw.U32 q2, [r0, #(4*1*4 - 64)]        // .................*................
        vqrdmulh.S32 q6, q7, q6                  // ..................*...............
        vadd.U32 q1, q1, q0                      // ...................*..............
        vldrw.U32 q2, [r11, #(64-96)]            // ....................*.............
        vmul.S32 q0, q7, q2                      // .....................*............
        vldrw.U32 q2, [r11, #(+16-96)]           // ......................e...........
        vmla.S32 q0, q6, r12                     // .......................*..........
        vldrw.U32 q6, [r0, #-16]                 // ........................e.........
        vqrdmulh.S32 q7, q6, q2                  // .........................e........
        vstrw.U32 q1, [r0, #(4*0*4 - 64)]        // ..........................*.......
        vadd.U32 q1, q3, q0                      // ...........................*......
        vstrw.U32 q1, [r0, #(4*2*4 - 64)]        // ............................*.....
        vsub.U32 q3, q3, q0                      // .............................*....
        vmul.S32 q0, q6, q4                      // ..............................e...
        vldrw.U32 q6, [r0, #-32]                 // ...............................e..
        vmla.S32 q0, q7, r12                     // ................................e.
        vstrw.U32 q3, [r0, #(4*3*4 - 64)]        // .................................*

                                                        // ----------------------- cycle (expected) ------------------------>
                                                        // 0                        25                       50
                                                        // |------------------------|------------------------|---------------
        // vldrw.u32 q0, [r0], #64                      // ....~...........................'.....*...........................
        // vldrw.u32 q1, [r0, #-48]                     // .........e......................'..........~......................
        // vldrw.u32 q2, [r0, #-32]                     // .............................e..'..............................~..
        // vldrw.u32 q3, [r0, #-16]                     // ......................e.........'.......................~.........
        // vldrw.u32 q5,         [r11], #+96            // e...............................'.~...............................
        // vldrw.u32 q6, [r11, #(+16-96)]               // ....................e...........'.....................~...........
        // vmul.s32       q4,  q2, q5                   // ................................*.................................
        // vqrdmulh.s32   q2,  q2, q6                   // .~..............................'..*..............................
        // vmla.s32       q4,  q2, r12                  // .....~..........................'......*..........................
        // vsub.u32       q2,    q0, q4                 // ..........~.....................'...........*.....................
        // vadd.u32       q0,    q0, q4                 // ............~...................'.............*...................
        // vmul.s32       q4,  q3, q5                   // ............................e...'.............................~...
        // vqrdmulh.s32   q3,  q3, q6                   // .......................e........'........................~........
        // vmla.s32       q4,  q3, r12                  // ..............................e.'...............................~.
        // vsub.u32       q3,    q1, q4                 // ......~.........................'.......*.........................
        // vadd.u32       q1,    q1, q4                 // ................................'*................................
        // vldrw.u32 q5,         [r11, #(32 - 96)]      // .......~........................'........*........................
        // vldrw.u32 q6, [r11, #(48 - 96)]              // ..~.............................'...*.............................
        // vmul.s32       q4,  q1, q5                   // ........~.......................'.........*.......................
        // vqrdmulh.s32   q1,  q1, q6                   // ...~............................'....*............................
        // vmla.s32       q4,  q1, r12                  // ...........~....................'............*....................
        // vsub.u32       q1,    q0, q4                 // ..............~.................'...............*.................
        // vadd.u32       q0,    q0, q4                 // .................~..............'..................*..............
        // vldrw.u32 q5,         [r11, #(64-96)]        // ..................~.............'...................*.............
        // vldrw.u32 q6, [r11, #(80-96)]                // .............~..................'..............*..................
        // vmul.s32       q4,  q3, q5                   // ...................~............'....................*............
        // vqrdmulh.s32   q3,  q3, q6                   // ................~...............'.................*...............
        // vmla.s32       q4,  q3, r12                  // .....................~..........'......................*..........
        // vsub.u32       q3,    q2, q4                 // ...........................~....'............................*....
        // vadd.u32       q2,    q2, q4                 // .........................~......'..........................*......
        // vstrw.u32 q0, [r0, #(4*0*4 - 64)]            // ........................~.......'.........................*.......
        // vstrw.u32 q1, [r0, #(4*1*4 - 64)]            // ...............~................'................*................
        // vstrw.u32 q2, [r0, #(4*2*4 - 64)]            // ..........................~.....'...........................*.....
        // vstrw.u32 q3, [r0, #(4*3*4 - 64)]            // ...............................~'................................*

        le lr, layer78_loop
                                                 // Instructions:    26
                                                 // Expected cycles: 26
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     0.22s
                                                 // User time:     0.22s
                                                 //
                                                 // ----- cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|----
        vmul.S32 q4, q6, q4                      // *.............................
        vldrw.U32 q7, [r11, #(80-96)]            // .*............................
        vqrdmulh.S32 q2, q6, q2                  // ..*...........................
        vsub.U32 q3, q5, q0                      // ...*..........................
        vqrdmulh.S32 q7, q3, q7                  // ....*.........................
        vldrw.U32 q1, [r11, #(64-96)]            // .....*........................
        vadd.U32 q6, q5, q0                      // ......*.......................
        vmul.S32 q5, q3, q1                      // .......*......................
        vldrw.U32 q3, [r11, #(32 - 96)]          // ........*.....................
        vmla.S32 q5, q7, r12                     // .........*....................
        vldrw.U32 q7, [r0], #64                  // ..........*...................
        vmla.S32 q4, q2, r12                     // ...........*..................
        vldrw.U32 q1, [r11, #(48 - 96)]          // ............*.................
        vsub.U32 q0, q7, q4                      // .............*................
        vqrdmulh.S32 q2, q6, q1                  // ..............*...............
        vsub.U32 q1, q0, q5                      // ...............*..............
        vmul.S32 q3, q6, q3                      // ................*.............
        vadd.U32 q5, q0, q5                      // .................*............
        vstrw.U32 q1, [r0, #(4*3*4 - 64)]        // ..................*...........
        vadd.U32 q6, q7, q4                      // ...................*..........
        vmla.S32 q3, q2, r12                     // ....................*.........
        vstrw.U32 q5, [r0, #(4*2*4 - 64)]        // .....................*........
        vadd.U32 q1, q6, q3                      // ......................*.......
        vstrw.U32 q1, [r0, #(4*0*4 - 64)]        // .......................*......
        vsub.U32 q5, q6, q3                      // ........................*.....
        vstrw.U32 q5, [r0, #(4*1*4 - 64)]        // .........................*....

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vmul.S32 q1, q6, q4                    // *..............................
        // vadd.U32 q3, q5, q0                    // ......*........................
        // vqrdmulh.S32 q7, q6, q2                // ..*............................
        // vldrw.U32 q6, [r11, #(48 - 96)]        // ............*..................
        // vqrdmulh.S32 q2, q3, q6                // ..............*................
        // vldrw.U32 q6, [r0], #64                // ..........*....................
        // vmla.S32 q1, q7, r12                   // ...........*...................
        // vsub.U32 q7, q5, q0                    // ...*...........................
        // vldrw.U32 q0, [r11, #(32 - 96)]        // ........*......................
        // vmul.S32 q0, q3, q0                    // ................*..............
        // vsub.U32 q3, q6, q1                    // .............*.................
        // vmla.S32 q0, q2, r12                   // ....................*..........
        // vadd.U32 q1, q6, q1                    // ...................*...........
        // vldrw.U32 q6, [r11, #(80-96)]          // .*.............................
        // vsub.U32 q2, q1, q0                    // ........................*......
        // vstrw.U32 q2, [r0, #(4*1*4 - 64)]      // .........................*.....
        // vqrdmulh.S32 q6, q7, q6                // ....*..........................
        // vadd.U32 q1, q1, q0                    // ......................*........
        // vldrw.U32 q2, [r11, #(64-96)]          // .....*.........................
        // vmul.S32 q0, q7, q2                    // .......*.......................
        // vmla.S32 q0, q6, r12                   // .........*.....................
        // vstrw.U32 q1, [r0, #(4*0*4 - 64)]      // .......................*.......
        // vadd.U32 q1, q3, q0                    // .................*.............
        // vstrw.U32 q1, [r0, #(4*2*4 - 64)]      // .....................*.........
        // vsub.U32 q3, q3, q0                    // ...............*...............
        // vstrw.U32 q3, [r0, #(4*3*4 - 64)]      // ..................*............


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
