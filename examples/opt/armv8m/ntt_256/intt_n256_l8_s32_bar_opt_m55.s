
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

#define INVNTT_REDUCE_AFTER_L56
#define INVNTT_REDUCE_AFTER_L34

.data
roots_inv:
#include "intt_n256_l8_s32_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s32       \dst,  \src, \const
        vqrdmulh.s32   \src,  \src, \const_twisted
        vmla.s32       \dst,  \src, modulus
.endm

.macro gs_butterfly a, b, root, root_twisted
        vsub.u32       tmp, \a,  \b
        vadd.u32       \a,  \a,  \b
        mulmod         \b,  tmp, \root, \root_twisted
.endm

.align 4
roots_addr: .word roots_inv
.syntax unified
.type invntt_n256_u32_33556993_28678040_complete_manual, %function
.global invntt_n256_u32_33556993_28678040_complete_manual
invntt_n256_u32_33556993_28678040_complete_manual:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus     .req r12
        root_ptr    .req r11

        .equ modulus_const, -33556993
        movw modulus, #:lower16:modulus_const
        movt modulus, #:upper16:modulus_const
        ldr  root_ptr, roots_addr

        in .req r0

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6


        tmp .req q4

        // Layers 7,8

        mov lr, #16
                                            // Instructions:    3
                                            // Expected cycles: 5
                                            // Expected IPC:    0.60
                                            //
                                            // Wall time:     0.00s
                                            // User time:     0.00s
                                            //
                                            // ----- cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|----
        vldrw.U32 q2, [r0, #(4*4*1)]        // *.............................
        vldrw.U32 q1, [r11, #(32)]          // ..*...........................
        vldrw.U32 q3, [r0]                  // ....*.........................

                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        // vldrw.U32 q3, [r0]                // ....*..........................
        // vldrw.U32 q1, [r11, #(32)]        // ..*............................
        // vldrw.U32 q2, [r0, #(4*4*1)]      // *..............................

        sub lr, lr, #1
.p2align 2
layer78_loop:
                                                 // Instructions:    34
                                                 // Expected cycles: 34
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     4.36s
                                                 // User time:     4.36s
                                                 //
                                                 // ------- cycle (expected) -------->
                                                 // 0                        25
                                                 // |------------------------|--------
        vsub.U32 q6, q3, q2                      // *.................................
        vmul.S32 q1, q6, q1                      // .*................................
        vldrw.U32 q5, [r11, #(32+16)]            // ..*...............................
        vqrdmulh.S32 q6, q6, q5                  // ...*..............................
        vldrw.U32 q7, [r0, #(4*4*3)]             // ....*.............................
        vmla.S32 q1, q6, r12                     // .....*............................
        vldrw.U32 q4, [r0, #(4*4*2)]             // ......*...........................
        vadd.U32 q6, q3, q2                      // .......*..........................
        vldrw.U32 q2, [r11, #(64+16)]            // ........*.........................
        vsub.U32 q3, q4, q7                      // .........*........................
        vqrdmulh.S32 q5, q3, q2                  // ..........*.......................
        vldrw.U32 q2, [r11, #(64)]               // ...........*......................
        vmul.S32 q0, q3, q2                      // ............*.....................
        vadd.U32 q7, q4, q7                      // .............*....................
        vmla.S32 q0, q5, r12                     // ..............*...................
        vldrw.U32 q2, [r11, #(16 - 3*32)]        // ...............*..................
        vadd.U32 q4, q1, q0                      // ................*.................
        vstrw.U32 q4, [r0, #(4*4*1 - 64)]        // .................*................
        vsub.U32 q1, q1, q0                      // ..................*...............
        vqrdmulh.S32 q4, q1, q2                  // ...................*..............
        vldrw.U32 q5, [r11], #(3*32)             // ....................*.............
        vmul.S32 q0, q1, q5                      // .....................*............
        vldrw.U32 q3, [r0]                       // ......................e...........
        vmla.S32 q0, q4, r12                     // .......................*..........
        vstrw.U32 q0, [r0, #(4*4*3 - 64)]        // ........................*.........
        vsub.U32 q4, q6, q7                      // .........................*........
        vmul.S32 q0, q4, q5                      // ..........................*.......
        vldrw.U32 q1, [r11, #(32)]               // ...........................e......
        vqrdmulh.S32 q4, q4, q2                  // ............................*.....
        vldrw.U32 q2, [r0, #(4*4*1)]             // .............................e....
        vmla.S32 q0, q4, r12                     // ..............................*...
        vstrw.U32 q0, [r0, #(4*4*2 - 64)]        // ...............................*..
        vadd.U32 q4, q6, q7                      // ................................*.
        vstrw.U32 q4, [r0], #64                  // .................................*

                                                     // ------------- cycle (expected) -------------->
                                                     // 0                        25
                                                     // |------------------------|--------------------
        // vldrw.u32 q0, [r0]                        // e...........'.....................~...........
        // vldrw.u32 q1, [r0, #(4*4*1)]              // .......e....'............................~....
        // vldrw.u32 q2, [r0, #(4*4*2)]              // ............'.....*...........................
        // vldrw.u32 q3, [r0, #(4*4*3)]              // ............'...*.............................
        // vldrw.u32 q5,         [r11, #(32)]        // .....e......'..........................~......
        // vldrw.u32 q6, [r11, #(32+16)]             // ............'.*...............................
        // vsub.u32       q4, q0,  q1                // ............*.................................
        // vadd.u32       q0,  q0,  q1               // ............'......*..........................
        // vmul.s32       q1,  q4, q5                // ............'*................................
        // vqrdmulh.s32   q4,  q4, q6                // ............'..*..............................
        // vmla.s32       q1,  q4, r12               // ............'....*............................
        // vldrw.u32 q5,         [r11, #(64)]        // ............'..........*......................
        // vldrw.u32 q6, [r11, #(64+16)]             // ............'.......*.........................
        // vsub.u32       q4, q2,  q3                // ............'........*........................
        // vadd.u32       q2,  q2,  q3               // ............'............*....................
        // vmul.s32       q3,  q4, q5                // ............'...........*.....................
        // vqrdmulh.s32   q4,  q4, q6                // ............'.........*.......................
        // vmla.s32       q3,  q4, r12               // ............'.............*...................
        // vldrw.u32 q5,         [r11], #(3*32)      // ............'...................*.............
        // vldrw.u32 q6, [r11, #(16 - 3*32)]         // ............'..............*..................
        // vsub.u32       q4, q0,  q2                // ...~........'........................*........
        // vadd.u32       q0,  q0,  q2               // ..........~.'...............................*.
        // vmul.s32       q2,  q4, q5                // ....~.......'.........................*.......
        // vqrdmulh.s32   q4,  q4, q6                // ......~.....'...........................*.....
        // vmla.s32       q2,  q4, r12               // ........~...'.............................*...
        // vsub.u32       q4, q1,  q3                // ............'.................*...............
        // vadd.u32       q1,  q1,  q3               // ............'...............*.................
        // vmul.s32       q3,  q4, q5                // ............'....................*............
        // vqrdmulh.s32   q4,  q4, q6                // ............'..................*..............
        // vmla.s32       q3,  q4, r12               // .~..........'......................*..........
        // vstrw.u32 q0, [r0], #64                   // ...........~'................................*
        // vstrw.u32 q1, [r0, #(4*4*1 - 64)]         // ............'................*................
        // vstrw.u32 q2, [r0, #(4*4*2 - 64)]         // .........~..'..............................*..
        // vstrw.u32 q3, [r0, #(4*4*3 - 64)]         // ..~.........'.......................*.........

        le lr, layer78_loop
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     0.39s
                                                 // User time:     0.39s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vadd.U32 q4, q3, q2                      // *..............................
        vldrw.U32 q7, [r0, #(4*4*3)]             // .*.............................
        vsub.U32 q6, q3, q2                      // ..*............................
        vldrw.U32 q2, [r0, #(4*4*2)]             // ...*...........................
        vadd.U32 q5, q2, q7                      // ....*..........................
        vmul.S32 q3, q6, q1                      // .....*.........................
        vadd.U32 q1, q4, q5                      // ......*........................
        vldrw.U32 q0, [r11, #(64+16)]            // .......*.......................
        vsub.U32 q7, q2, q7                      // ........*......................
        vqrdmulh.S32 q0, q7, q0                  // .........*.....................
        vldrw.U32 q2, [r11, #(64)]               // ..........*....................
        vmul.S32 q7, q7, q2                      // ...........*...................
        vsub.U32 q2, q4, q5                      // ............*..................
        vmla.S32 q7, q0, r12                     // .............*.................
        vldrw.U32 q0, [r11, #(32+16)]            // ..............*................
        vqrdmulh.S32 q4, q6, q0                  // ...............*...............
        vldrw.U32 q6, [r11, #(16 - 3*32)]        // ................*..............
        vqrdmulh.S32 q5, q2, q6                  // .................*.............
        vldrw.U32 q0, [r11], #(3*32)             // ..................*............
        vmla.S32 q3, q4, r12                     // ...................*...........
        vstrw.U32 q1, [r0], #64                  // ....................*..........
        vmul.S32 q1, q2, q0                      // .....................*.........
        vsub.U32 q2, q3, q7                      // ......................*........
        vmul.S32 q0, q2, q0                      // .......................*.......
        vadd.U32 q7, q3, q7                      // ........................*......
        vmla.S32 q1, q5, r12                     // .........................*.....
        vstrw.U32 q1, [r0, #(4*4*2 - 64)]        // ..........................*....
        vqrdmulh.S32 q6, q2, q6                  // ...........................*...
        vstrw.U32 q7, [r0, #(4*4*1 - 64)]        // ............................*..
        vmla.S32 q0, q6, r12                     // .............................*.
        vstrw.U32 q0, [r0, #(4*4*3 - 64)]        // ..............................*

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vsub.U32 q6, q3, q2                    // ..*............................
        // vmul.S32 q1, q6, q1                    // .....*.........................
        // vldrw.U32 q5, [r11, #(32+16)]          // ..............*................
        // vqrdmulh.S32 q6, q6, q5                // ...............*...............
        // vldrw.U32 q7, [r0, #(4*4*3)]           // .*.............................
        // vmla.S32 q1, q6, r12                   // ...................*...........
        // vldrw.U32 q4, [r0, #(4*4*2)]           // ...*...........................
        // vadd.U32 q6, q3, q2                    // *..............................
        // vldrw.U32 q2, [r11, #(64+16)]          // .......*.......................
        // vsub.U32 q3, q4, q7                    // ........*......................
        // vqrdmulh.S32 q5, q3, q2                // .........*.....................
        // vldrw.U32 q2, [r11, #(64)]             // ..........*....................
        // vmul.S32 q0, q3, q2                    // ...........*...................
        // vadd.U32 q7, q4, q7                    // ....*..........................
        // vmla.S32 q0, q5, r12                   // .............*.................
        // vldrw.U32 q2, [r11, #(16 - 3*32)]      // ................*..............
        // vadd.U32 q4, q1, q0                    // ........................*......
        // vstrw.U32 q4, [r0, #(4*4*1 - 64)]      // ............................*..
        // vsub.U32 q1, q1, q0                    // ......................*........
        // vqrdmulh.S32 q4, q1, q2                // ...........................*...
        // vldrw.U32 q5, [r11], #(3*32)           // ..................*............
        // vmul.S32 q0, q1, q5                    // .......................*.......
        // vmla.S32 q0, q4, r12                   // .............................*.
        // vstrw.U32 q0, [r0, #(4*4*3 - 64)]      // ..............................*
        // vsub.U32 q4, q6, q7                    // ............*..................
        // vmul.S32 q0, q4, q5                    // .....................*.........
        // vqrdmulh.S32 q4, q4, q2                // .................*.............
        // vmla.S32 q0, q4, r12                   // .........................*.....
        // vstrw.U32 q0, [r0, #(4*4*2 - 64)]      // ..........................*....
        // vadd.U32 q4, q6, q7                    // ......*........................
        // vstrw.U32 q4, [r0], #64                // ....................*..........


        sub in, in, #(4*256)

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        // Layers 5,6

        mov lr, #16
                                                // Instructions:    5
                                                // Expected cycles: 8
                                                // Expected IPC:    0.62
                                                //
                                                // Wall time:     0.01s
                                                // User time:     0.01s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vld40.U32 {q0, q1, q2, q3}, [r0]        // *.............................
        vld41.U32 {q0, q1, q2, q3}, [r0]        // ..*...........................
        vld42.U32 {q0, q1, q2, q3}, [r0]        // ....*.........................
        ldrd r9, r8, [r11, #-8]                 // ......*.......................
        vld43.U32 {q0, q1, q2, q3}, [r0]        // .......*......................

                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        // ldrd r9, r8, [r11, #-8]               // ......*........................
        // vld40.U32 {q0, q1, q2, q3}, [r0]      // *..............................
        // vld41.U32 {q0, q1, q2, q3}, [r0]      // ..*............................
        // vld42.U32 {q0, q1, q2, q3}, [r0]      // ....*..........................
        // vld43.U32 {q0, q1, q2, q3}, [r0]      // .......*.......................

        sub lr, lr, #1
.p2align 2
layer56_loop:
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     4.54s
                                                 // User time:     4.54s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vsub.U32 q4, q2, q3                      // *..............................
        vmul.S32 q6, q4, r9                      // .*.............................
        ldrd r2, r1, [r11, #-16]                 // ..*............................
        vadd.U32 q5, q2, q3                      // ...*...........................
        vqrdmulh.S32 q7, q4, r8                  // ....*..........................
        vsub.U32 q3, q0, q1                      // .....*.........................
        vqrdmulh.S32 q4, q3, r1                  // ......*........................
        vadd.U32 q2, q0, q1                      // .......*.......................
        vmul.S32 q1, q3, r2                      // ........*......................
        vadd.U32 q0, q2, q5                      // .........*.....................
        ldrd r6, r1, [r11], #24                  // ..........*....................
        vmla.S32 q6, q7, r12                     // ...........*...................
        ldrd r9, r8, [r11, #-8]                  // ............e..................
        vmla.S32 q1, q4, r12                     // .............*.................
        vsub.U32 q5, q2, q5                      // ..............*................
        vstrw.U32 q0, [r0], #(64)                // ...............*...............
        vadd.U32 q3, q1, q6                      // ................*..............
        vstrw.U32 q3, [r0, #(4*4*1 - 64)]        // .................*.............
        vsub.U32 q7, q1, q6                      // ..................*............
        vmul.S32 q4, q5, r6                      // ...................*...........
        vld40.U32 {q0, q1, q2, q3}, [r0]         // ....................e..........
        vqrdmulh.S32 q5, q5, r1                  // .....................*.........
        vld41.U32 {q0, q1, q2, q3}, [r0]         // ......................e........
        vqrdmulh.S32 q6, q7, r1                  // .......................*.......
        vld42.U32 {q0, q1, q2, q3}, [r0]         // ........................e......
        vmla.S32 q4, q5, r12                     // .........................*.....
        vld43.U32 {q0, q1, q2, q3}, [r0]         // ..........................e....
        vmul.S32 q7, q7, r6                      // ...........................*...
        vstrw.U32 q4, [r0, #(4*4*2 - 64)]        // ............................*..
        vmla.S32 q7, q6, r12                     // .............................*.
        vstrw.U32 q7, [r0, #(4*4*3 - 64)]        // ..............................*

                                                  // --------------- cycle (expected) ---------------->
                                                  // 0                        25
                                                  // |------------------------|------------------------
        // ldrd r2, r3, [r11], #24                // ...................'.........*....................
        // ldrd r4, r5, [r11, #-16]               // ...................'.*............................
        // ldrd r6, r7, [r11, #-8]                // e..................'...........~..................
        // vld40.u32 {q0, q1, q2, q3}, [r0]       // ........e..........'...................~..........
        // vld41.u32 {q0, q1, q2, q3}, [r0]       // ..........e........'.....................~........
        // vld42.u32 {q0, q1, q2, q3}, [r0]       // ............e......'.......................~......
        // vld43.u32 {q0, q1, q2, q3}, [r0]       // ..............e....'.........................~....
        // vsub.u32       q4, q0,  q1             // ...................'....*.........................
        // vadd.u32       q0,  q0,  q1            // ...................'......*.......................
        // vmul.s32       q1,  q4, r4             // ...................'.......*......................
        // vqrdmulh.s32   q4,  q4, r5             // ...................'.....*........................
        // vmla.s32       q1,  q4, r12            // .~.................'............*.................
        // vsub.u32       q4, q2,  q3             // ...................*..............................
        // vadd.u32       q2,  q2,  q3            // ...................'..*...........................
        // vmul.s32       q3,  q4, r6             // ...................'*.............................
        // vqrdmulh.s32   q4,  q4, r7             // ...................'...*..........................
        // vmla.s32       q3,  q4, r12            // ...................'..........*...................
        // vsub.u32       q4, q0,  q2             // ..~................'.............*................
        // vadd.u32       q0,  q0,  q2            // ...................'........*.....................
        // vmul.s32       q2,  q4, r2             // .......~...........'..................*...........
        // vqrdmulh.s32   q4,  q4, r3             // .........~.........'....................*.........
        // vmla.s32       q2,  q4, r12            // .............~.....'........................*.....
        // vsub.u32       q4, q1,  q3             // ......~............'.................*............
        // vadd.u32       q1,  q1,  q3            // ....~..............'...............*..............
        // vmul.s32       q3,  q4, r2             // ...............~...'..........................*...
        // vqrdmulh.s32   q4,  q4, r3             // ...........~.......'......................*.......
        // vmla.s32       q3,  q4, r12            // .................~.'............................*.
        // vstrw.u32 q0, [r0], #(64)              // ...~...............'..............*...............
        // vstrw.u32 q1, [r0, #(4*4*1 - 64)]      // .....~.............'................*.............
        // vstrw.u32 q2, [r0, #(4*4*2 - 64)]      // ................~..'...........................*..
        // vstrw.u32 q3, [r0, #(4*4*3 - 64)]      // ..................~'.............................*

        le lr, layer56_loop
                                                 // Instructions:    26
                                                 // Expected cycles: 26
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     0.20s
                                                 // User time:     0.20s
                                                 //
                                                 // ----- cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|----
        vsub.U32 q4, q2, q3                      // *.............................
        vmul.S32 q7, q4, r9                      // .*............................
        vsub.U32 q6, q0, q1                      // ..*...........................
        vqrdmulh.S32 q5, q4, r8                  // ...*..........................
        ldrd r8, r9, [r11, #-16]                 // ....*.........................
        vmla.S32 q7, q5, r12                     // .....*........................
        vadd.U32 q5, q2, q3                      // ......*.......................
        vqrdmulh.S32 q2, q6, r9                  // .......*......................
        vadd.U32 q3, q0, q1                      // ........*.....................
        vmul.S32 q6, q6, r8                      // .........*....................
        vsub.U32 q4, q3, q5                      // ..........*...................
        vmla.S32 q6, q2, r12                     // ...........*..................
        ldrd r9, r8, [r11], #24                  // ............*.................
        vsub.U32 q0, q6, q7                      // .............*................
        vmul.S32 q2, q0, r9                      // ..............*...............
        vadd.U32 q1, q3, q5                      // ...............*..............
        vqrdmulh.S32 q0, q0, r8                  // ................*.............
        vstrw.U32 q1, [r0], #(64)                // .................*............
        vmla.S32 q2, q0, r12                     // ..................*...........
        vstrw.U32 q2, [r0, #(4*4*3 - 64)]        // ...................*..........
        vqrdmulh.S32 q3, q4, r8                  // ....................*.........
        vadd.U32 q6, q6, q7                      // .....................*........
        vmul.S32 q0, q4, r9                      // ......................*.......
        vstrw.U32 q6, [r0, #(4*4*1 - 64)]        // .......................*......
        vmla.S32 q0, q3, r12                     // ........................*.....
        vstrw.U32 q0, [r0, #(4*4*2 - 64)]        // .........................*....

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vsub.U32 q4, q2, q3                    // *..............................
        // vmul.S32 q6, q4, r9                    // .*.............................
        // ldrd r2, r1, [r11, #-16]               // ....*..........................
        // vadd.U32 q5, q2, q3                    // ......*........................
        // vqrdmulh.S32 q7, q4, r8                // ...*...........................
        // vsub.U32 q3, q0, q1                    // ..*............................
        // vqrdmulh.S32 q4, q3, r1                // .......*.......................
        // vadd.U32 q2, q0, q1                    // ........*......................
        // vmul.S32 q1, q3, r2                    // .........*.....................
        // vadd.U32 q0, q2, q5                    // ...............*...............
        // ldrd r6, r1, [r11], #24                // ............*..................
        // vmla.S32 q6, q7, r12                   // .....*.........................
        // vmla.S32 q1, q4, r12                   // ...........*...................
        // vsub.U32 q5, q2, q5                    // ..........*....................
        // vstrw.U32 q0, [r0], #(64)              // .................*.............
        // vadd.U32 q3, q1, q6                    // .....................*.........
        // vstrw.U32 q3, [r0, #(4*4*1 - 64)]      // .......................*.......
        // vsub.U32 q7, q1, q6                    // .............*.................
        // vmul.S32 q4, q5, r6                    // ......................*........
        // vqrdmulh.S32 q5, q5, r1                // ....................*..........
        // vqrdmulh.S32 q6, q7, r1                // ................*..............
        // vmla.S32 q4, q5, r12                   // ........................*......
        // vmul.S32 q7, q7, r6                    // ..............*................
        // vstrw.U32 q4, [r0, #(4*4*2 - 64)]      // .........................*.....
        // vmla.S32 q7, q6, r12                   // ..................*............
        // vstrw.U32 q7, [r0, #(4*4*3 - 64)]      // ...................*...........


        sub in, in, #(4*256)

        // TEMPORARY: Barrett reduction
        barrett_const .req r1
        .equ const_barrett, 63
        movw barrett_const, #:lower16:const_barrett
        movt barrett_const, #:upper16:const_barrett
        mov lr, #64
1:
        vldrw.u32 data0, [in]
        vqrdmulh.s32 tmp, data0, barrett_const
        vmla.s32 data0, tmp, modulus
        vstrw.u32 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*256)
        .unreq barrett_const

        // Layers 3,4

        // 4 butterfly blocks per root config, 4 root configs
        // loop over root configs

        count .req r1
        mov count, #4

out_start:
        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #4
                                             // Instructions:    2
                                             // Expected cycles: 3
                                             // Expected IPC:    0.67
                                             //
                                             // Wall time:     0.00s
                                             // User time:     0.00s
                                             //
                                             // ----- cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|----
        vldrw.U32 q5, [r0, #(4*1*16)]        // *.............................
        vldrw.U32 q3, [r0]                   // ..*...........................

                                              // ------ cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|-----
        // vldrw.U32 q3, [r0]                 // ..*............................
        // vldrw.U32 q5, [r0, #(4*1*16)]      // *..............................

        sub lr, lr, #1
.p2align 2
layer34_loop:
                                                  // Instructions:    28
                                                  // Expected cycles: 28
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     2.37s
                                                  // User time:     2.37s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vsub.U32 q6, q3, q5                       // *.............................
        vqrdmulh.S32 q2, q6, r5                   // .*............................
        vadd.U32 q4, q3, q5                       // ..*...........................
        vmul.S32 q1, q6, r4                       // ...*..........................
        vldrw.U32 q0, [r0, #(4*2*16)]             // ....*.........................
        vmla.S32 q1, q2, r12                      // .....*........................
        vldrw.U32 q5, [r0, #(4*3*16)]             // ......*.......................
        vsub.U32 q7, q0, q5                       // .......*......................
        vqrdmulh.S32 q2, q7, r7                   // ........*.....................
        vadd.U32 q6, q0, q5                       // .........*....................
        vmul.S32 q0, q7, r6                       // ..........*...................
        vsub.U32 q5, q4, q6                       // ...........*..................
        vmla.S32 q0, q2, r12                      // ............*.................
        vadd.U32 q4, q4, q6                       // .............*................
        vqrdmulh.S32 q2, q5, r3                   // ..............*...............
        vsub.U32 q7, q1, q0                       // ...............*..............
        vstrw.U32 q4, [r0], #(16)                 // ................*.............
        vmul.S32 q6, q7, r2                       // .................*............
        vadd.U32 q1, q1, q0                       // ..................*...........
        vldrw.U32 q3, [r0]                        // ...................e..........
        vmul.S32 q0, q5, r2                       // ....................*.........
        vldrw.U32 q5, [r0, #(4*1*16)]             // .....................e........
        vqrdmulh.S32 q7, q7, r3                   // ......................*.......
        vstrw.U32 q1, [r0, #(4*16*1 - 16)]        // .......................*......
        vmla.S32 q6, q7, r12                      // ........................*.....
        vstrw.U32 q6, [r0, #(4*16*3 - 16)]        // .........................*....
        vmla.S32 q0, q2, r12                      // ..........................*...
        vstrw.U32 q0, [r0, #(4*16*2 - 16)]        // ...........................*..

                                                   // --------- cycle (expected) --------->
                                                   // 0                        25
                                                   // |------------------------|-----------
        // vldrw.u32 q0, [r0]                      // e........'..................~........
        // vldrw.u32 q1, [r0, #(4*1*16)]           // ..e......'....................~......
        // vldrw.u32 q2, [r0, #(4*2*16)]           // .........'...*.......................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // .........'.....*.....................
        // vsub.u32       q4, q0,  q1              // .........*...........................
        // vadd.u32       q0,  q0,  q1             // .........'.*.........................
        // vmul.s32       q1,  q4, r4              // .........'..*........................
        // vqrdmulh.s32   q4,  q4, r5              // .........'*..........................
        // vmla.s32       q1,  q4, r12             // .........'....*......................
        // vsub.u32       q4, q2,  q3              // .........'......*....................
        // vadd.u32       q2,  q2,  q3             // .........'........*..................
        // vmul.s32       q3,  q4, r6              // .........'.........*.................
        // vqrdmulh.s32   q4,  q4, r7              // .........'.......*...................
        // vmla.s32       q3,  q4, r12             // .........'...........*...............
        // vsub.u32       q4, q0,  q2              // .........'..........*................
        // vadd.u32       q0,  q0,  q2             // .........'............*..............
        // vmul.s32       q2,  q4, r2              // .~.......'...................*.......
        // vqrdmulh.s32   q4,  q4, r3              // .........'.............*.............
        // vmla.s32       q2,  q4, r12             // .......~.'.........................*.
        // vsub.u32       q4, q1,  q3              // .........'..............*............
        // vadd.u32       q1,  q1,  q3             // .........'.................*.........
        // vmul.s32       q3,  q4, r2              // .........'................*..........
        // vqrdmulh.s32   q4,  q4, r3              // ...~.....'.....................*.....
        // vmla.s32       q3,  q4, r12             // .....~...'.......................*...
        // vstrw.u32 q0, [r0], #(16)               // .........'...............*...........
        // vstrw.u32 q1, [r0, #(4*16*1 - 16)]      // ....~....'......................*....
        // vstrw.u32 q2, [r0, #(4*16*2 - 16)]      // ........~'..........................*
        // vstrw.u32 q3, [r0, #(4*16*3 - 16)]      // ......~..'........................*..

        le lr, layer34_loop
                                                  // Instructions:    26
                                                  // Expected cycles: 26
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     0.12s
                                                  // User time:     0.12s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vsub.U32 q6, q3, q5                       // *.............................
        vldrw.U32 q0, [r0, #(4*3*16)]             // .*............................
        vmul.S32 q1, q6, r4                       // ..*...........................
        vldrw.U32 q2, [r0, #(4*2*16)]             // ...*..........................
        vqrdmulh.S32 q6, q6, r5                   // ....*.........................
        vadd.U32 q4, q3, q5                       // .....*........................
        vmla.S32 q1, q6, r12                      // ......*.......................
        vsub.U32 q6, q2, q0                       // .......*......................
        vmul.S32 q5, q6, r6                       // ........*.....................
        vadd.U32 q0, q2, q0                       // .........*....................
        vqrdmulh.S32 q6, q6, r7                   // ..........*...................
        vsub.U32 q3, q4, q0                       // ...........*..................
        vmla.S32 q5, q6, r12                      // ............*.................
        vadd.U32 q2, q4, q0                       // .............*................
        vmul.S32 q0, q3, r2                       // ..............*...............
        vadd.U32 q6, q1, q5                       // ...............*..............
        vqrdmulh.S32 q3, q3, r3                   // ................*.............
        vsub.U32 q7, q1, q5                       // .................*............
        vmul.S32 q5, q7, r2                       // ..................*...........
        vstrw.U32 q2, [r0], #(16)                 // ...................*..........
        vmla.S32 q0, q3, r12                      // ....................*.........
        vstrw.U32 q0, [r0, #(4*16*2 - 16)]        // .....................*........
        vqrdmulh.S32 q0, q7, r3                   // ......................*.......
        vstrw.U32 q6, [r0, #(4*16*1 - 16)]        // .......................*......
        vmla.S32 q5, q0, r12                      // ........................*.....
        vstrw.U32 q5, [r0, #(4*16*3 - 16)]        // .........................*....

                                                   // ------ cycle (expected) ------>
                                                   // 0                        25
                                                   // |------------------------|-----
        // vsub.U32 q6, q3, q5                     // *..............................
        // vqrdmulh.S32 q2, q6, r5                 // ....*..........................
        // vadd.U32 q4, q3, q5                     // .....*.........................
        // vmul.S32 q1, q6, r4                     // ..*............................
        // vldrw.U32 q0, [r0, #(4*2*16)]           // ...*...........................
        // vmla.S32 q1, q2, r12                    // ......*........................
        // vldrw.U32 q5, [r0, #(4*3*16)]           // .*.............................
        // vsub.U32 q7, q0, q5                     // .......*.......................
        // vqrdmulh.S32 q2, q7, r7                 // ..........*....................
        // vadd.U32 q6, q0, q5                     // .........*.....................
        // vmul.S32 q0, q7, r6                     // ........*......................
        // vsub.U32 q5, q4, q6                     // ...........*...................
        // vmla.S32 q0, q2, r12                    // ............*..................
        // vadd.U32 q4, q4, q6                     // .............*.................
        // vqrdmulh.S32 q2, q5, r3                 // ................*..............
        // vsub.U32 q7, q1, q0                     // .................*.............
        // vstrw.U32 q4, [r0], #(16)               // ...................*...........
        // vmul.S32 q6, q7, r2                     // ..................*............
        // vadd.U32 q1, q1, q0                     // ...............*...............
        // vmul.S32 q0, q5, r2                     // ..............*................
        // vqrdmulh.S32 q7, q7, r3                 // ......................*........
        // vstrw.U32 q1, [r0, #(4*16*1 - 16)]      // .......................*.......
        // vmla.S32 q6, q7, r12                    // ........................*......
        // vstrw.U32 q6, [r0, #(4*16*3 - 16)]      // .........................*.....
        // vmla.S32 q0, q2, r12                    // ....................*..........
        // vstrw.U32 q0, [r0, #(4*16*2 - 16)]      // .....................*.........

        add in, in, #(4*64 - 4*16)

        subs count, count, #1
        bne out_start

        sub in, in, #(4*256)

        // TEMPORARY: Barrett reduction
        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        movt barrett_const, #:upper16:const_barrett
        mov lr, #64
1:
        vldrw.u32 data0, [in]
        vqrdmulh.s32 tmp, data0, barrett_const
        vmla.s32 data0, tmp, modulus
        vstrw.u32 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*256)
        .unreq barrett_const

        in_low       .req r0
        in_high      .req r1
        add in_high, in_low, #(4*128)

        // Layers 1,2

        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #16
                                           // Instructions:    2
                                           // Expected cycles: 3
                                           // Expected IPC:    0.67
                                           //
                                           // Wall time:     0.00s
                                           // User time:     0.00s
                                           //
                                           // ----- cycle (expected) ------>
                                           // 0                        25
                                           // |------------------------|----
        vldrw.U32 q5, [r0]                 // *.............................
        vldrw.U32 q3, [r0, #(4*64)]        // ..*...........................

                                            // ------ cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|-----
        // vldrw.U32 q3, [r0, #(4*64)]      // ..*............................
        // vldrw.U32 q5, [r0]               // *..............................

        sub lr, lr, #1
.p2align 2
layer12_loop:
                                                // Instructions:    28
                                                // Expected cycles: 28
                                                // Expected IPC:    1.00
                                                //
                                                // Wall time:     2.28s
                                                // User time:     2.28s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vsub.U32 q7, q5, q3                     // *.............................
        vqrdmulh.S32 q6, q7, r5                 // .*............................
        vldrw.U32 q4, [r1]                      // ..*...........................
        vmul.S32 q1, q7, r4                     // ...*..........................
        vldrw.U32 q7, [r1, #(4*64)]             // ....*.........................
        vmla.S32 q1, q6, r12                    // .....*........................
        vsub.U32 q6, q4, q7                     // ......*.......................
        vmul.S32 q0, q6, r6                     // .......*......................
        vadd.U32 q2, q5, q3                     // ........*.....................
        vqrdmulh.S32 q5, q6, r7                 // .........*....................
        vadd.U32 q6, q4, q7                     // ..........*...................
        vmla.S32 q0, q5, r12                    // ...........*..................
        vsub.U32 q7, q2, q6                     // ............*.................
        vqrdmulh.S32 q5, q7, r3                 // .............*................
        vadd.U32 q4, q2, q6                     // ..............*...............
        vmul.S32 q6, q7, r2                     // ...............*..............
        vstrw.U32 q4, [r0], #16                 // ................*.............
        vmla.S32 q6, q5, r12                    // .................*............
        vstrw.U32 q6, [r1], #16                 // ..................*...........
        vsub.U32 q4, q1, q0                     // ...................*..........
        vmul.S32 q6, q4, r2                     // ....................*.........
        vldrw.U32 q3, [r0, #(4*64)]             // .....................e........
        vqrdmulh.S32 q2, q4, r3                 // ......................*.......
        vldrw.U32 q5, [r0]                      // .......................e......
        vadd.U32 q4, q1, q0                     // ........................*.....
        vstrw.U32 q4, [r0, #(4*64 - 16)]        // .........................*....
        vmla.S32 q6, q2, r12                    // ..........................*...
        vstrw.U32 q6, [r1, #(4*64 - 16)]        // ...........................*..

                                                 // -------- cycle (expected) -------->
                                                 // 0                        25
                                                 // |------------------------|---------
        // vldrw.u32 q0, [r0]                    // ..e....'......................~....
        // vldrw.u32 q1, [r0,  #(4*64)]          // e......'....................~......
        // vldrw.u32 q2, [r1]                    // .......'.*.........................
        // vldrw.u32 q3, [r1, #(4*64)]           // .......'...*.......................
        // vsub.u32       q4, q0,  q1            // .......*...........................
        // vadd.u32       q0,  q0,  q1           // .......'.......*...................
        // vmul.s32       q1,  q4, r4            // .......'..*........................
        // vqrdmulh.s32   q4,  q4, r5            // .......'*..........................
        // vmla.s32       q1,  q4, r12           // .......'....*......................
        // vsub.u32       q4, q2,  q3            // .......'.....*.....................
        // vadd.u32       q2,  q2,  q3           // .......'.........*.................
        // vmul.s32       q3,  q4, r6            // .......'......*....................
        // vqrdmulh.s32   q4,  q4, r7            // .......'........*..................
        // vmla.s32       q3,  q4, r12           // .......'..........*................
        // vsub.u32       q4, q0,  q2            // .......'...........*...............
        // vadd.u32       q0,  q0,  q2           // .......'.............*.............
        // vmul.s32       q2,  q4, r2            // .......'..............*............
        // vqrdmulh.s32   q4,  q4, r3            // .......'............*..............
        // vmla.s32       q2,  q4, r12           // .......'................*..........
        // vsub.u32       q4, q1,  q3            // .......'..................*........
        // vadd.u32       q1,  q1,  q3           // ...~...'.......................*...
        // vmul.s32       q3,  q4, r2            // .......'...................*.......
        // vqrdmulh.s32   q4,  q4, r3            // .~.....'.....................*.....
        // vmla.s32       q3,  q4, r12           // .....~.'.........................*.
        // vstrw.u32 q0, [r0], #16               // .......'...............*...........
        // vstrw.u32 q1, [r0, #(4*64 - 16)]      // ....~..'........................*..
        // vstrw.u32 q2, [r1], #16               // .......'.................*.........
        // vstrw.u32 q3, [r1, #(4*64 - 16)]      // ......~'..........................*

        le lr, layer12_loop
                                                // Instructions:    26
                                                // Expected cycles: 26
                                                // Expected IPC:    1.00
                                                //
                                                // Wall time:     0.12s
                                                // User time:     0.12s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vsub.U32 q0, q5, q3                     // *.............................
        vmul.S32 q1, q0, r4                     // .*............................
        vldrw.U32 q4, [r1]                      // ..*...........................
        vqrdmulh.S32 q0, q0, r5                 // ...*..........................
        vldrw.U32 q2, [r1, #(4*64)]             // ....*.........................
        vmla.S32 q1, q0, r12                    // .....*........................
        vsub.U32 q6, q4, q2                     // ......*.......................
        vmul.S32 q0, q6, r6                     // .......*......................
        vadd.U32 q2, q4, q2                     // ........*.....................
        vqrdmulh.S32 q7, q6, r7                 // .........*....................
        vadd.U32 q5, q5, q3                     // ..........*...................
        vmla.S32 q0, q7, r12                    // ...........*..................
        vsub.U32 q3, q5, q2                     // ............*.................
        vqrdmulh.S32 q7, q3, r3                 // .............*................
        vsub.U32 q4, q1, q0                     // ..............*...............
        vqrdmulh.S32 q6, q4, r3                 // ...............*..............
        vadd.U32 q5, q5, q2                     // ................*.............
        vmul.S32 q3, q3, r2                     // .................*............
        vstrw.U32 q5, [r0], #16                 // ..................*...........
        vmla.S32 q3, q7, r12                    // ...................*..........
        vstrw.U32 q3, [r1], #16                 // ....................*.........
        vmul.S32 q3, q4, r2                     // .....................*........
        vadd.U32 q5, q1, q0                     // ......................*.......
        vstrw.U32 q5, [r0, #(4*64 - 16)]        // .......................*......
        vmla.S32 q3, q6, r12                    // ........................*.....
        vstrw.U32 q3, [r1, #(4*64 - 16)]        // .........................*....

                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        // vsub.U32 q7, q5, q3                   // *..............................
        // vqrdmulh.S32 q6, q7, r5               // ...*...........................
        // vldrw.U32 q4, [r1]                    // ..*............................
        // vmul.S32 q1, q7, r4                   // .*.............................
        // vldrw.U32 q7, [r1, #(4*64)]           // ....*..........................
        // vmla.S32 q1, q6, r12                  // .....*.........................
        // vsub.U32 q6, q4, q7                   // ......*........................
        // vmul.S32 q0, q6, r6                   // .......*.......................
        // vadd.U32 q2, q5, q3                   // ..........*....................
        // vqrdmulh.S32 q5, q6, r7               // .........*.....................
        // vadd.U32 q6, q4, q7                   // ........*......................
        // vmla.S32 q0, q5, r12                  // ...........*...................
        // vsub.U32 q7, q2, q6                   // ............*..................
        // vqrdmulh.S32 q5, q7, r3               // .............*.................
        // vadd.U32 q4, q2, q6                   // ................*..............
        // vmul.S32 q6, q7, r2                   // .................*.............
        // vstrw.U32 q4, [r0], #16               // ..................*............
        // vmla.S32 q6, q5, r12                  // ...................*...........
        // vstrw.U32 q6, [r1], #16               // ....................*..........
        // vsub.U32 q4, q1, q0                   // ..............*................
        // vmul.S32 q6, q4, r2                   // .....................*.........
        // vqrdmulh.S32 q2, q4, r3               // ...............*...............
        // vadd.U32 q4, q1, q0                   // ......................*........
        // vstrw.U32 q4, [r0, #(4*64 - 16)]      // .......................*.......
        // vmla.S32 q6, q2, r12                  // ........................*......
        // vstrw.U32 q6, [r1, #(4*64 - 16)]      // .........................*.....


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
