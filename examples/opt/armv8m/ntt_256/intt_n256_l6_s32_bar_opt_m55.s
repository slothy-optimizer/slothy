
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots_inv:
#include "intt_n256_l6_s32_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s32       \dst,  \src, \const
        vqrdmulh.s32   \src,  \src, \const_twisted
        vmla.s32       \dst,  \src, modulus
.endm

.macro gs_butterfly a, b, root, root_twisted
        vsub.u32       tmp, \a,  \b
        vadd.u32       \a,  \a,  \b
        mulmod         \b,  tmp, \root, \root_twisted
.endm

.align 4
roots_addr: .word roots_inv
.syntax unified
.type invntt_n256_u32_33556993_28678040_incomplete_manual, %function
.global invntt_n256_u32_33556993_28678040_incomplete_manual
invntt_n256_u32_33556993_28678040_incomplete_manual:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus     .req r12
        root_ptr    .req r11

        .equ modulus_const, -33556993
        movw modulus, #:lower16:modulus_const
        movt modulus, #:upper16:modulus_const
        ldr  root_ptr, roots_addr

        in .req r0

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        // Layers 5,6

        mov lr, #16
                                            // Instructions:    3
                                            // Expected cycles: 4
                                            // Expected IPC:    0.75
                                            //
                                            // Wall time:     0.00s
                                            // User time:     0.00s
                                            //
                                            // ----- cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|----
        ldrd r9, r4, [r11, #-8]             // *.............................
        vldrw.U32 q4, [r0, #(4*4*2)]        // .*............................
        vldrw.U32 q7, [r0, #(4*4*3)]        // ...*..........................

                                             // ------ cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|-----
        // vldrw.U32 q4, [r0, #(4*4*2)]      // .*.............................
        // vldrw.U32 q7, [r0, #(4*4*3)]      // ...*...........................
        // ldrd r9, r4, [r11, #-8]           // *..............................

        sub lr, lr, #1
.p2align 2
layer56_loop:
                                                 // Instructions:    31
                                                 // Expected cycles: 31
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     3.35s
                                                 // User time:     3.35s
                                                 //
                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        vsub.U32 q3, q4, q7                      // *..............................
        vmul.S32 q0, q3, r9                      // .*.............................
        vldrw.U32 q2, [r0, #(4*4*1)]             // ..*............................
        vqrdmulh.S32 q6, q3, r4                  // ...*...........................
        vldrw.U32 q3, [r0]                       // ....*..........................
        vsub.U32 q5, q3, q2                      // .....*.........................
        ldrd r6, r1, [r11], #24                  // ......*........................
        ldrd r2, r8, [r11, #-16]                 // .......*.......................
        vmla.S32 q0, q6, r12                     // ........*......................
        vadd.U32 q7, q4, q7                      // .........*.....................
        vqrdmulh.S32 q4, q5, r8                  // ..........*....................
        vadd.U32 q3, q3, q2                      // ...........*...................
        vmul.S32 q1, q5, r2                      // ............*..................
        vadd.U32 q2, q3, q7                      // .............*.................
        vmla.S32 q1, q4, r12                     // ..............*................
        vstrw.U32 q2, [r0], #(64)                // ...............*...............
        vsub.U32 q5, q1, q0                      // ................*..............
        vmul.S32 q6, q5, r6                      // .................*.............
        vadd.U32 q2, q1, q0                      // ..................*............
        vqrdmulh.S32 q4, q5, r1                  // ...................*...........
        vstrw.U32 q2, [r0, #(4*4*1 - 64)]        // ....................*..........
        vmla.S32 q6, q4, r12                     // .....................*.........
        vstrw.U32 q6, [r0, #(4*4*3 - 64)]        // ......................*........
        vsub.U32 q3, q3, q7                      // .......................*.......
        vmul.S32 q1, q3, r6                      // ........................*......
        vldrw.U32 q4, [r0, #(4*4*2)]             // .........................e.....
        vqrdmulh.S32 q2, q3, r1                  // ..........................*....
        vldrw.U32 q7, [r0, #(4*4*3)]             // ...........................e...
        vmla.S32 q1, q2, r12                     // ............................*..
        ldrd r9, r4, [r11, #-8]                  // .............................e.
        vstrw.U32 q1, [r0, #(4*4*2 - 64)]        // ..............................*

                                                  // --------- cycle (expected) --------->
                                                  // 0                        25
                                                  // |------------------------|-----------
        // ldrd r2, r3, [r11], #24                // ......'.....*........................
        // ldrd r4, r5, [r11, #-16]               // ......'......*.......................
        // ldrd r6, r7, [r11, #-8]                // ....e.'............................~.
        // vldrw.u32 q0, [r0]                     // ......'...*..........................
        // vldrw.u32 q1, [r0, #(4*4*1)]           // ......'.*............................
        // vldrw.u32 q2, [r0, #(4*4*2)]           // e.....'........................~.....
        // vldrw.u32 q3, [r0, #(4*4*3)]           // ..e...'..........................~...
        // vsub.u32       q4, q0,  q1             // ......'....*.........................
        // vadd.u32       q0,  q0,  q1            // ......'..........*...................
        // vmul.s32       q1,  q4, r4             // ......'...........*..................
        // vqrdmulh.s32   q4,  q4, r5             // ......'.........*....................
        // vmla.s32       q1,  q4, r12            // ......'.............*................
        // vsub.u32       q4, q2,  q3             // ......*..............................
        // vadd.u32       q2,  q2,  q3            // ......'........*.....................
        // vmul.s32       q3,  q4, r6             // ......'*.............................
        // vqrdmulh.s32   q4,  q4, r7             // ......'..*...........................
        // vmla.s32       q3,  q4, r12            // ......'.......*......................
        // vsub.u32       q4, q0,  q2             // ......'......................*.......
        // vadd.u32       q0,  q0,  q2            // ......'............*.................
        // vmul.s32       q2,  q4, r2             // ......'.......................*......
        // vqrdmulh.s32   q4,  q4, r3             // .~....'.........................*....
        // vmla.s32       q2,  q4, r12            // ...~..'...........................*..
        // vsub.u32       q4, q1,  q3             // ......'...............*..............
        // vadd.u32       q1,  q1,  q3            // ......'.................*............
        // vmul.s32       q3,  q4, r2             // ......'................*.............
        // vqrdmulh.s32   q4,  q4, r3             // ......'..................*...........
        // vmla.s32       q3,  q4, r12            // ......'....................*.........
        // vstrw.u32 q0, [r0], #(64)              // ......'..............*...............
        // vstrw.u32 q1, [r0, #(4*4*1 - 64)]      // ......'...................*..........
        // vstrw.u32 q2, [r0, #(4*4*2 - 64)]      // .....~'.............................*
        // vstrw.u32 q3, [r0, #(4*4*3 - 64)]      // ......'.....................*........

        le lr, layer56_loop
                                                 // Instructions:    28
                                                 // Expected cycles: 28
                                                 // Expected IPC:    1.00
                                                 //
                                                 // Wall time:     0.25s
                                                 // User time:     0.25s
                                                 //
                                                 // ----- cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|----
        vsub.U32 q0, q4, q7                      // *.............................
        vmul.S32 q1, q0, r9                      // .*............................
        ldrd r2, r1, [r11, #-16]                 // ..*...........................
        vqrdmulh.S32 q2, q0, r4                  // ...*..........................
        vldrw.U32 q3, [r0]                       // ....*.........................
        vmla.S32 q1, q2, r12                     // .....*........................
        vldrw.U32 q5, [r0, #(4*4*1)]             // ......*.......................
        vsub.U32 q6, q3, q5                      // .......*......................
        vmul.S32 q0, q6, r2                      // ........*.....................
        vadd.U32 q2, q3, q5                      // .........*....................
        vqrdmulh.S32 q5, q6, r1                  // ..........*...................
        ldrd r6, r1, [r11], #24                  // ...........*..................
        vadd.U32 q6, q4, q7                      // ............*.................
        vmla.S32 q0, q5, r12                     // .............*................
        vsub.U32 q7, q2, q6                      // ..............*...............
        vmul.S32 q5, q7, r6                      // ...............*..............
        vsub.U32 q4, q0, q1                      // ................*.............
        vqrdmulh.S32 q3, q7, r1                  // .................*............
        vadd.U32 q0, q0, q1                      // ..................*...........
        vmla.S32 q5, q3, r12                     // ...................*..........
        vadd.U32 q7, q2, q6                      // ....................*.........
        vstrw.U32 q7, [r0], #(64)                // .....................*........
        vmul.S32 q6, q4, r6                      // ......................*.......
        vstrw.U32 q0, [r0, #(4*4*1 - 64)]        // .......................*......
        vqrdmulh.S32 q0, q4, r1                  // ........................*.....
        vstrw.U32 q5, [r0, #(4*4*2 - 64)]        // .........................*....
        vmla.S32 q6, q0, r12                     // ..........................*...
        vstrw.U32 q6, [r0, #(4*4*3 - 64)]        // ...........................*..

                                                  // ------ cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|-----
        // vsub.U32 q3, q4, q7                    // *..............................
        // vmul.S32 q0, q3, r9                    // .*.............................
        // vldrw.U32 q2, [r0, #(4*4*1)]           // ......*........................
        // vqrdmulh.S32 q6, q3, r4                // ...*...........................
        // vldrw.U32 q3, [r0]                     // ....*..........................
        // vsub.U32 q5, q3, q2                    // .......*.......................
        // ldrd r6, r1, [r11], #24                // ...........*...................
        // ldrd r2, r8, [r11, #-16]               // ..*............................
        // vmla.S32 q0, q6, r12                   // .....*.........................
        // vadd.U32 q7, q4, q7                    // ............*..................
        // vqrdmulh.S32 q4, q5, r8                // ..........*....................
        // vadd.U32 q3, q3, q2                    // .........*.....................
        // vmul.S32 q1, q5, r2                    // ........*......................
        // vadd.U32 q2, q3, q7                    // ....................*..........
        // vmla.S32 q1, q4, r12                   // .............*.................
        // vstrw.U32 q2, [r0], #(64)              // .....................*.........
        // vsub.U32 q5, q1, q0                    // ................*..............
        // vmul.S32 q6, q5, r6                    // ......................*........
        // vadd.U32 q2, q1, q0                    // ..................*............
        // vqrdmulh.S32 q4, q5, r1                // ........................*......
        // vstrw.U32 q2, [r0, #(4*4*1 - 64)]      // .......................*.......
        // vmla.S32 q6, q4, r12                   // ..........................*....
        // vstrw.U32 q6, [r0, #(4*4*3 - 64)]      // ...........................*...
        // vsub.U32 q3, q3, q7                    // ..............*................
        // vmul.S32 q1, q3, r6                    // ...............*...............
        // vqrdmulh.S32 q2, q3, r1                // .................*.............
        // vmla.S32 q1, q2, r12                   // ...................*...........
        // vstrw.U32 q1, [r0, #(4*4*2 - 64)]      // .........................*.....


        sub in, in, #(4*256)

        // TEMPORARY: Barrett reduction
        barrett_const .req r1
        .equ const_barrett, 63
        movw barrett_const, #:lower16:const_barrett
        movt barrett_const, #:upper16:const_barrett
        mov lr, #64
1:
        vldrw.u32 data0, [in]
        vqrdmulh.s32 tmp, data0, barrett_const
        vmla.s32 data0, tmp, modulus
        vstrw.u32 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*256)
        .unreq barrett_const

        // Layers 3,4

        // 4 butterfly blocks per root config, 4 root configs
        // loop over root configs

        count .req r1
        mov count, #4

out_start:
        ldrd root0, root0_twisted, [root_ptr], #+24
        ldrd root1, root1_twisted, [root_ptr, #-16]
        ldrd root2, root2_twisted, [root_ptr, #-8]

        mov lr, #4
                                             // Instructions:    2
                                             // Expected cycles: 3
                                             // Expected IPC:    0.67
                                             //
                                             // Wall time:     0.00s
                                             // User time:     0.00s
                                             //
                                             // ----- cycle (expected) ------>
                                             // 0                        25
                                             // |------------------------|----
        vldrw.U32 q3, [r0, #(4*1*16)]        // *.............................
        vldrw.U32 q0, [r0]                   // ..*...........................

                                              // ------ cycle (expected) ------>
                                              // 0                        25
                                              // |------------------------|-----
        // vldrw.U32 q0, [r0]                 // ..*............................
        // vldrw.U32 q3, [r0, #(4*1*16)]      // *..............................

        sub lr, lr, #1
.p2align 2
layer34_loop:
                                                  // Instructions:    28
                                                  // Expected cycles: 28
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     2.24s
                                                  // User time:     2.24s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vsub.U32 q7, q0, q3                       // *.............................
        vqrdmulh.S32 q6, q7, r5                   // .*............................
        vldrw.U32 q4, [r0, #(4*2*16)]             // ..*...........................
        vmul.S32 q1, q7, r4                       // ...*..........................
        vldrw.U32 q7, [r0, #(4*3*16)]             // ....*.........................
        vadd.U32 q2, q4, q7                       // .....*........................
        vmla.S32 q1, q6, r12                      // ......*.......................
        vsub.U32 q6, q4, q7                       // .......*......................
        vqrdmulh.S32 q5, q6, r7                   // ........*.....................
        vadd.U32 q7, q0, q3                       // .........*....................
        vmul.S32 q0, q6, r6                       // ..........*...................
        vadd.U32 q4, q7, q2                       // ...........*..................
        vmla.S32 q0, q5, r12                      // ............*.................
        vstrw.U32 q4, [r0], #(16)                 // .............*................
        vsub.U32 q4, q1, q0                       // ..............*...............
        vqrdmulh.S32 q5, q4, r3                   // ...............*..............
        vadd.U32 q1, q1, q0                       // ................*.............
        vmul.S32 q6, q4, r2                       // .................*............
        vstrw.U32 q1, [r0, #(4*16*1 - 16)]        // ..................*...........
        vmla.S32 q6, q5, r12                      // ...................*..........
        vstrw.U32 q6, [r0, #(4*16*3 - 16)]        // ....................*.........
        vsub.U32 q7, q7, q2                       // .....................*........
        vqrdmulh.S32 q5, q7, r3                   // ......................*.......
        vldrw.U32 q0, [r0]                        // .......................e......
        vmul.S32 q6, q7, r2                       // ........................*.....
        vldrw.U32 q3, [r0, #(4*1*16)]             // .........................e....
        vmla.S32 q6, q5, r12                      // ..........................*...
        vstrw.U32 q6, [r0, #(4*16*2 - 16)]        // ...........................*..

                                                   // ------- cycle (expected) ------->
                                                   // 0                        25
                                                   // |------------------------|-------
        // vldrw.u32 q0, [r0]                      // e....'......................~....
        // vldrw.u32 q1, [r0, #(4*1*16)]           // ..e..'........................~..
        // vldrw.u32 q2, [r0, #(4*2*16)]           // .....'.*.........................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // .....'...*.......................
        // vsub.u32       q4, q0,  q1              // .....*...........................
        // vadd.u32       q0,  q0,  q1             // .....'........*..................
        // vmul.s32       q1,  q4, r4              // .....'..*........................
        // vqrdmulh.s32   q4,  q4, r5              // .....'*..........................
        // vmla.s32       q1,  q4, r12             // .....'.....*.....................
        // vsub.u32       q4, q2,  q3              // .....'......*....................
        // vadd.u32       q2,  q2,  q3             // .....'....*......................
        // vmul.s32       q3,  q4, r6              // .....'.........*.................
        // vqrdmulh.s32   q4,  q4, r7              // .....'.......*...................
        // vmla.s32       q3,  q4, r12             // .....'...........*...............
        // vsub.u32       q4, q0,  q2              // .....'....................*......
        // vadd.u32       q0,  q0,  q2             // .....'..........*................
        // vmul.s32       q2,  q4, r2              // .~...'.......................*...
        // vqrdmulh.s32   q4,  q4, r3              // .....'.....................*.....
        // vmla.s32       q2,  q4, r12             // ...~.'.........................*.
        // vsub.u32       q4, q1,  q3              // .....'.............*.............
        // vadd.u32       q1,  q1,  q3             // .....'...............*...........
        // vmul.s32       q3,  q4, r2              // .....'................*..........
        // vqrdmulh.s32   q4,  q4, r3              // .....'..............*............
        // vmla.s32       q3,  q4, r12             // .....'..................*........
        // vstrw.u32 q0, [r0], #(16)               // .....'............*..............
        // vstrw.u32 q1, [r0, #(4*16*1 - 16)]      // .....'.................*.........
        // vstrw.u32 q2, [r0, #(4*16*2 - 16)]      // ....~'..........................*
        // vstrw.u32 q3, [r0, #(4*16*3 - 16)]      // .....'...................*.......

        le lr, layer34_loop
                                                  // Instructions:    26
                                                  // Expected cycles: 26
                                                  // Expected IPC:    1.00
                                                  //
                                                  // Wall time:     0.12s
                                                  // User time:     0.12s
                                                  //
                                                  // ----- cycle (expected) ------>
                                                  // 0                        25
                                                  // |------------------------|----
        vsub.U32 q6, q0, q3                       // *.............................
        vmul.S32 q1, q6, r4                       // .*............................
        vldrw.U32 q5, [r0, #(4*2*16)]             // ..*...........................
        vqrdmulh.S32 q2, q6, r5                   // ...*..........................
        vldrw.U32 q6, [r0, #(4*3*16)]             // ....*.........................
        vmla.S32 q1, q2, r12                      // .....*........................
        vsub.U32 q2, q5, q6                       // ......*.......................
        vqrdmulh.S32 q7, q2, r7                   // .......*......................
        vadd.U32 q5, q5, q6                       // ........*.....................
        vmul.S32 q6, q2, r6                       // .........*....................
        vadd.U32 q4, q0, q3                       // ..........*...................
        vmla.S32 q6, q7, r12                      // ...........*..................
        vsub.U32 q3, q4, q5                       // ............*.................
        vmul.S32 q2, q3, r2                       // .............*................
        vsub.U32 q0, q1, q6                       // ..............*...............
        vqrdmulh.S32 q3, q3, r3                   // ...............*..............
        vadd.U32 q7, q1, q6                       // ................*.............
        vmla.S32 q2, q3, r12                      // .................*............
        vstrw.U32 q2, [r0, #(4*16*2 - 16)]        // ..................*...........
        vqrdmulh.S32 q1, q0, r3                   // ...................*..........
        vstrw.U32 q7, [r0, #(4*16*1 - 16)]        // ....................*.........
        vmul.S32 q6, q0, r2                       // .....................*........
        vadd.U32 q2, q4, q5                       // ......................*.......
        vstrw.U32 q2, [r0], #(16)                 // .......................*......
        vmla.S32 q6, q1, r12                      // ........................*.....
        vstrw.U32 q6, [r0, #(4*16*3 - 16)]        // .........................*....

                                                   // ------ cycle (expected) ------>
                                                   // 0                        25
                                                   // |------------------------|-----
        // vsub.U32 q7, q0, q3                     // *..............................
        // vqrdmulh.S32 q6, q7, r5                 // ...*...........................
        // vldrw.U32 q4, [r0, #(4*2*16)]           // ..*............................
        // vmul.S32 q1, q7, r4                     // .*.............................
        // vldrw.U32 q7, [r0, #(4*3*16)]           // ....*..........................
        // vadd.U32 q2, q4, q7                     // ........*......................
        // vmla.S32 q1, q6, r12                    // .....*.........................
        // vsub.U32 q6, q4, q7                     // ......*........................
        // vqrdmulh.S32 q5, q6, r7                 // .......*.......................
        // vadd.U32 q7, q0, q3                     // ..........*....................
        // vmul.S32 q0, q6, r6                     // .........*.....................
        // vadd.U32 q4, q7, q2                     // ......................*........
        // vmla.S32 q0, q5, r12                    // ...........*...................
        // vstrw.U32 q4, [r0], #(16)               // .......................*.......
        // vsub.U32 q4, q1, q0                     // ..............*................
        // vqrdmulh.S32 q5, q4, r3                 // ...................*...........
        // vadd.U32 q1, q1, q0                     // ................*..............
        // vmul.S32 q6, q4, r2                     // .....................*.........
        // vstrw.U32 q1, [r0, #(4*16*1 - 16)]      // ....................*..........
        // vmla.S32 q6, q5, r12                    // ........................*......
        // vstrw.U32 q6, [r0, #(4*16*3 - 16)]      // .........................*.....
        // vsub.U32 q7, q7, q2                     // ............*..................
        // vqrdmulh.S32 q5, q7, r3                 // ...............*...............
        // vmul.S32 q6, q7, r2                     // .............*.................
        // vmla.S32 q6, q5, r12                    // .................*.............
        // vstrw.U32 q6, [r0, #(4*16*2 - 16)]      // ..................*............

        add in, in, #(4*64 - 4*16)

        subs count, count, #1
        bne out_start

        sub in, in, #(4*256)

        // TEMPORARY: Barrett reduction
        barrett_const .req r1
        movw barrett_const, #:lower16:const_barrett
        movt barrett_const, #:upper16:const_barrett
        mov lr, #64
1:
        vldrw.u32 data0, [in]
        vqrdmulh.s32 tmp, data0, barrett_const
        vmla.s32 data0, tmp, modulus
        vstrw.u32 data0, [in], #16
        le lr, 1b
2:
        sub in, in, #(4*256)
        .unreq barrett_const

        in_low       .req r0
        in_high      .req r1
        add in_high, in_low, #(4*128)

        // Layers 1,2

        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #16
                                           // Instructions:    2
                                           // Expected cycles: 3
                                           // Expected IPC:    0.67
                                           //
                                           // Wall time:     0.00s
                                           // User time:     0.00s
                                           //
                                           // ----- cycle (expected) ------>
                                           // 0                        25
                                           // |------------------------|----
        vldrw.U32 q4, [r1, #(4*64)]        // *.............................
        vldrw.U32 q2, [r1]                 // ..*...........................

                                            // ------ cycle (expected) ------>
                                            // 0                        25
                                            // |------------------------|-----
        // vldrw.U32 q2, [r1]               // ..*............................
        // vldrw.U32 q4, [r1, #(4*64)]      // *..............................

        sub lr, lr, #1
.p2align 2
layer12_loop:
                                                // Instructions:    28
                                                // Expected cycles: 28
                                                // Expected IPC:    1.00
                                                //
                                                // Wall time:     2.22s
                                                // User time:     2.22s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vsub.U32 q7, q2, q4                     // *.............................
        vqrdmulh.S32 q6, q7, r7                 // .*............................
        vldrw.U32 q1, [r0, #(4*64)]             // ..*...........................
        vmul.S32 q0, q7, r6                     // ...*..........................
        vldrw.U32 q3, [r0]                      // ....*.........................
        vsub.U32 q7, q3, q1                     // .....*........................
        vqrdmulh.S32 q5, q7, r5                 // ......*.......................
        vadd.U32 q3, q3, q1                     // .......*......................
        vmul.S32 q1, q7, r4                     // ........*.....................
        vadd.U32 q7, q2, q4                     // .........*....................
        vmla.S32 q1, q5, r12                    // ..........*...................
        vldrw.U32 q2, [r1]                      // ...........e..................
        vmla.S32 q0, q6, r12                    // ............*.................
        vsub.U32 q6, q3, q7                     // .............*................
        vqrdmulh.S32 q4, q6, r3                 // ..............*...............
        vadd.U32 q5, q1, q0                     // ...............*..............
        vmul.S32 q6, q6, r2                     // ................*.............
        vstrw.U32 q5, [r0, #(4*64 - 16)]        // .................*............
        vsub.U32 q5, q1, q0                     // ..................*...........
        vmla.S32 q6, q4, r12                    // ...................*..........
        vldrw.U32 q4, [r1, #(4*64)]             // ....................e.........
        vmul.S32 q0, q5, r2                     // .....................*........
        vstrw.U32 q6, [r1], #16                 // ......................*.......
        vqrdmulh.S32 q5, q5, r3                 // .......................*......
        vadd.U32 q1, q3, q7                     // ........................*.....
        vstrw.U32 q1, [r0], #16                 // .........................*....
        vmla.S32 q0, q5, r12                    // ..........................*...
        vstrw.U32 q0, [r1, #(4*64 - 16)]        // ...........................*..

                                                 // ------------- cycle (expected) ------------->
                                                 // 0                        25
                                                 // |------------------------|-------------------
        // vldrw.u32 q0, [r0]                    // .................'...*.......................
        // vldrw.u32 q1, [r0,  #(4*64)]          // .................'.*.........................
        // vldrw.u32 q2, [r1]                    // e................'..........~................
        // vldrw.u32 q3, [r1, #(4*64)]           // .........e.......'...................~.......
        // vsub.u32       q4, q0,  q1            // .................'....*......................
        // vadd.u32       q0,  q0,  q1           // .................'......*....................
        // vmul.s32       q1,  q4, r4            // .................'.......*...................
        // vqrdmulh.s32   q4,  q4, r5            // .................'.....*.....................
        // vmla.s32       q1,  q4, r12           // .................'.........*.................
        // vsub.u32       q4, q2,  q3            // .................*...........................
        // vadd.u32       q2,  q2,  q3           // .................'........*..................
        // vmul.s32       q3,  q4, r6            // .................'..*........................
        // vqrdmulh.s32   q4,  q4, r7            // .................'*..........................
        // vmla.s32       q3,  q4, r12           // .~...............'...........*...............
        // vsub.u32       q4, q0,  q2            // ..~..............'............*..............
        // vadd.u32       q0,  q0,  q2           // .............~...'.......................*...
        // vmul.s32       q2,  q4, r2            // .....~...........'...............*...........
        // vqrdmulh.s32   q4,  q4, r3            // ...~.............'.............*.............
        // vmla.s32       q2,  q4, r12           // ........~........'..................*........
        // vsub.u32       q4, q1,  q3            // .......~.........'.................*.........
        // vadd.u32       q1,  q1,  q3           // ....~............'..............*............
        // vmul.s32       q3,  q4, r2            // ..........~......'....................*......
        // vqrdmulh.s32   q4,  q4, r3            // ............~....'......................*....
        // vmla.s32       q3,  q4, r12           // ...............~.'.........................*.
        // vstrw.u32 q0, [r0], #16               // ..............~..'........................*..
        // vstrw.u32 q1, [r0, #(4*64 - 16)]      // ......~..........'................*..........
        // vstrw.u32 q2, [r1], #16               // ...........~.....'.....................*.....
        // vstrw.u32 q3, [r1, #(4*64 - 16)]      // ................~'..........................*

        le lr, layer12_loop
                                                // Instructions:    26
                                                // Expected cycles: 26
                                                // Expected IPC:    1.00
                                                //
                                                // Wall time:     0.11s
                                                // User time:     0.11s
                                                //
                                                // ----- cycle (expected) ------>
                                                // 0                        25
                                                // |------------------------|----
        vsub.U32 q6, q2, q4                     // *.............................
        vmul.S32 q1, q6, r6                     // .*............................
        vadd.U32 q7, q2, q4                     // ..*...........................
        vqrdmulh.S32 q6, q6, r7                 // ...*..........................
        vldrw.U32 q5, [r0]                      // ....*.........................
        vmla.S32 q1, q6, r12                    // .....*........................
        vldrw.U32 q6, [r0, #(4*64)]             // ......*.......................
        vsub.U32 q3, q5, q6                     // .......*......................
        vmul.S32 q2, q3, r4                     // ........*.....................
        vadd.U32 q0, q5, q6                     // .........*....................
        vqrdmulh.S32 q6, q3, r5                 // ..........*...................
        vadd.U32 q3, q0, q7                     // ...........*..................
        vmla.S32 q2, q6, r12                    // ............*.................
        vsub.U32 q0, q0, q7                     // .............*................
        vmul.S32 q4, q0, r2                     // ..............*...............
        vsub.U32 q6, q2, q1                     // ...............*..............
        vmul.S32 q5, q6, r2                     // ................*.............
        vstrw.U32 q3, [r0], #16                 // .................*............
        vqrdmulh.S32 q3, q6, r3                 // ..................*...........
        vadd.U32 q6, q2, q1                     // ...................*..........
        vmla.S32 q5, q3, r12                    // ....................*.........
        vstrw.U32 q5, [r1, #(4*64 - 16)]        // .....................*........
        vqrdmulh.S32 q0, q0, r3                 // ......................*.......
        vstrw.U32 q6, [r0, #(4*64 - 16)]        // .......................*......
        vmla.S32 q4, q0, r12                    // ........................*.....
        vstrw.U32 q4, [r1], #16                 // .........................*....

                                                 // ------ cycle (expected) ------>
                                                 // 0                        25
                                                 // |------------------------|-----
        // vsub.U32 q7, q2, q4                   // *..............................
        // vqrdmulh.S32 q6, q7, r7               // ...*...........................
        // vldrw.U32 q1, [r0, #(4*64)]           // ......*........................
        // vmul.S32 q0, q7, r6                   // .*.............................
        // vldrw.U32 q3, [r0]                    // ....*..........................
        // vsub.U32 q7, q3, q1                   // .......*.......................
        // vqrdmulh.S32 q5, q7, r5               // ..........*....................
        // vadd.U32 q3, q3, q1                   // .........*.....................
        // vmul.S32 q1, q7, r4                   // ........*......................
        // vadd.U32 q7, q2, q4                   // ..*............................
        // vmla.S32 q1, q5, r12                  // ............*..................
        // vmla.S32 q0, q6, r12                  // .....*.........................
        // vsub.U32 q6, q3, q7                   // .............*.................
        // vqrdmulh.S32 q4, q6, r3               // ......................*........
        // vadd.U32 q5, q1, q0                   // ...................*...........
        // vmul.S32 q6, q6, r2                   // ..............*................
        // vstrw.U32 q5, [r0, #(4*64 - 16)]      // .......................*.......
        // vsub.U32 q5, q1, q0                   // ...............*...............
        // vmla.S32 q6, q4, r12                  // ........................*......
        // vmul.S32 q0, q5, r2                   // ................*..............
        // vstrw.U32 q6, [r1], #16               // .........................*.....
        // vqrdmulh.S32 q5, q5, r3               // ..................*............
        // vadd.U32 q1, q3, q7                   // ...........*...................
        // vstrw.U32 q1, [r0], #16               // .................*.............
        // vmla.S32 q0, q5, r12                  // ....................*..........
        // vstrw.U32 q0, [r1, #(4*64 - 16)]      // .....................*.........


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
