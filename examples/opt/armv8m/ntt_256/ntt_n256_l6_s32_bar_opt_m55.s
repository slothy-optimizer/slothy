
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///



///
/// This assembly code has been auto-generated.
/// Don't modify it directly.
///

.data
roots:
#include "ntt_n256_l6_s32_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s32       \dst,  \src, \const
        vqrdmulh.s32   \src,  \src, \const_twisted
        vmla.s32       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_twisted
        mulmod tmp, \b, \root, \root_twisted
        vsub.u32       \b,    \a, tmp
        vadd.u32       \a,    \a, tmp
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_n256_u32_33556993_28678040_incomplete_manual, %function
.global ntt_n256_u32_33556993_28678040_incomplete_manual
ntt_n256_u32_33556993_28678040_incomplete_manual:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus  .req r12
        root_ptr .req r11

        .equ modulus_const, -33556993
        movw modulus, #:lower16:modulus_const
        movt modulus, #:upper16:modulus_const
        ldr  root_ptr, roots_addr

        in_low       .req r0
        in_high      .req r1

        add in_high, in_low, #(4*128)

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        // Layers 1-2

        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #16
                                        // Instructions:    5
                                        // Expected cycles: 6
                                        // Expected IPC:    0.83
                                        //
                                        // Wall time:     0.00s
                                        // User time:     0.00s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vldrw.U32 q5, [r1, #256]        // *.............................
        vqrdmulh.S32 q1, q5, r3         // .*............................
        vmul.S32 q0, q5, r2             // ...*..........................
        vldrw.U32 q5, [r0, #256]        // ....*.........................
        vmla.S32 q0, q1, r12            // .....*........................

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vldrw.U32 q6, [r1, #256]      // *..............................
        // vldrw.U32 q5, [r0, #256]      // ....*..........................
        // vmul.S32 q0, q6, r2           // ...*...........................
        // vqrdmulh.S32 q7, q6, r3       // .*.............................
        // vmla.S32 q0, q7, r12          // .....*.........................

        sub lr, lr, #1
.p2align 2
layer12_loop:
                                        // Instructions:    28
                                        // Expected cycles: 28
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     2.31s
                                        // User time:     2.31s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vsub.U32 q1, q5, q0             // *.............................
        vmul.S32 q2, q1, r6             // .*............................
        vldrw.U32 q4, [r1]              // ..*...........................
        vmul.S32 q7, q4, r2             // ...*..........................
        vldrw.U32 q6, [r1, #272]        // ....e.........................
        vqrdmulh.S32 q1, q1, r7         // .....*........................
        vldrw.U32 q3, [r0]              // ......*.......................
        vqrdmulh.S32 q4, q4, r3         // .......*......................
        vadd.U32 q0, q5, q0             // ........*.....................
        vmla.S32 q7, q4, r12            // .........*....................
        vldrw.U32 q5, [r0, #272]        // ..........e...................
        vmla.S32 q2, q1, r12            // ...........*..................
        vadd.U32 q4, q3, q7             // ............*.................
        vmul.S32 q1, q0, r4             // .............*................
        vsub.U32 q3, q3, q7             // ..............*...............
        vqrdmulh.S32 q0, q0, r5         // ...............*..............
        vsub.U32 q7, q3, q2             // ................*.............
        vmla.S32 q1, q0, r12            // .................*............
        vstrw.U32 q7, [r1, #256]        // ..................*...........
        vsub.U32 q7, q4, q1             // ...................*..........
        vmul.S32 q0, q6, r2             // ....................e.........
        vadd.U32 q1, q4, q1             // .....................*........
        vstrw.U32 q7, [r0, #256]        // ......................*.......
        vqrdmulh.S32 q7, q6, r3         // .......................e......
        vstrw.U32 q1, [r0], #16         // ........................*.....
        vmla.S32 q0, q7, r12            // .........................e....
        vadd.U32 q3, q3, q2             // ..........................*...
        vstrw.U32 q3, [r1], #16         // ...........................*..

                                                 // ---------------- cycle (expected) ----------------->
                                                 // 0                        25                       50
                                                 // |------------------------|------------------------|-
        // vldrw.u32 q0, [r0]                    // ..~.....................'.....*.....................
        // vldrw.u32 q1, [r0, #(4*64)]           // ......e.................'.........~.................
        // vldrw.u32 q2, [r1]                    // ........................'.*.........................
        // vldrw.u32 q3, [r1, #(4*64)]           // e.......................'...~.......................
        // vmul.s32       q4,  q2, r2            // ........................'..*........................
        // vqrdmulh.s32   q2,  q2, r3            // ...~....................'......*....................
        // vmla.s32       q4,  q2, r12           // .....~..................'........*..................
        // vsub.u32       q2,    q0, q4          // ..........~.............'.............*.............
        // vadd.u32       q0,    q0, q4          // ........~...............'...........*...............
        // vmul.s32       q4,  q3, r2            // ................e.......'...................~.......
        // vqrdmulh.s32   q3,  q3, r3            // ...................e....'......................~....
        // vmla.s32       q4,  q3, r12           // .....................e..'........................~..
        // vsub.u32       q3,    q1, q4          // ........................*...........................
        // vadd.u32       q1,    q1, q4          // ....~...................'.......*...................
        // vmul.s32       q4,  q1, r4            // .........~..............'............*..............
        // vqrdmulh.s32   q1,  q1, r5            // ...........~............'..............*............
        // vmla.s32       q4,  q1, r12           // .............~..........'................*..........
        // vsub.u32       q1,    q0, q4          // ...............~........'..................*........
        // vadd.u32       q0,    q0, q4          // .................~......'....................*......
        // vmul.s32       q4,  q3, r6            // ........................'*..........................
        // vqrdmulh.s32   q3,  q3, r7            // .~......................'....*......................
        // vmla.s32       q4,  q3, r12           // .......~................'..........*................
        // vsub.u32       q3,    q2, q4          // ............~...........'...............*...........
        // vadd.u32       q2,    q2, q4          // ......................~.'.........................*.
        // vstrw.u32 q0, [r0], #16               // ....................~...'.......................*...
        // vstrw.u32 q1, [r0, #(4*64 - 16)]      // ..................~.....'.....................*.....
        // vstrw.u32 q2, [r1], #16               // .......................~'..........................*
        // vstrw.u32 q3, [r1, #(4*64-16)]        // ..............~.........'.................*.........

        le lr, layer12_loop
                                        // Instructions:    23
                                        // Expected cycles: 24
                                        // Expected IPC:    0.96
                                        //
                                        // Wall time:     0.15s
                                        // User time:     0.15s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vldrw.U32 q2, [r1]              // *.............................
        vmul.S32 q1, q2, r2             // .*............................
        vsub.U32 q3, q5, q0             // ..*...........................
        vqrdmulh.S32 q6, q2, r3         // ...*..........................
        vadd.U32 q4, q5, q0             // ....*.........................
        vmla.S32 q1, q6, r12            // .....*........................
        vldrw.U32 q2, [r0]              // ......*.......................
        vqrdmulh.S32 q6, q4, r5         // .......*......................
        vsub.U32 q5, q2, q1             // ........*.....................
        vmul.S32 q7, q4, r4             // .........*....................
        vadd.U32 q4, q2, q1             // ..........*...................
        vmla.S32 q7, q6, r12            // ...........*..................
        vadd.U32 q6, q4, q7             // .............*................
        vqrdmulh.S32 q1, q3, r7         // ..............*...............
        vsub.U32 q4, q4, q7             // ...............*..............
        vmul.S32 q2, q3, r6             // ................*.............
        vstrw.U32 q4, [r0, #256]        // .................*............
        vmla.S32 q2, q1, r12            // ..................*...........
        vstrw.U32 q6, [r0], #16         // ...................*..........
        vsub.U32 q4, q5, q2             // ....................*.........
        vstrw.U32 q4, [r1, #256]        // .....................*........
        vadd.U32 q5, q5, q2             // ......................*.......
        vstrw.U32 q5, [r1], #16         // .......................*......

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vsub.U32 q1, q5, q0           // ..*............................
        // vmul.S32 q2, q1, r6           // ................*..............
        // vldrw.U32 q4, [r1]            // *..............................
        // vmul.S32 q7, q4, r2           // .*.............................
        // vqrdmulh.S32 q1, q1, r7       // ..............*................
        // vldrw.U32 q3, [r0]            // ......*........................
        // vqrdmulh.S32 q4, q4, r3       // ...*...........................
        // vadd.U32 q0, q5, q0           // ....*..........................
        // vmla.S32 q7, q4, r12          // .....*.........................
        // vmla.S32 q2, q1, r12          // ..................*............
        // vadd.U32 q4, q3, q7           // ..........*....................
        // vmul.S32 q1, q0, r4           // .........*.....................
        // vsub.U32 q3, q3, q7           // ........*......................
        // vqrdmulh.S32 q0, q0, r5       // .......*.......................
        // vsub.U32 q7, q3, q2           // ....................*..........
        // vmla.S32 q1, q0, r12          // ...........*...................
        // vstrw.U32 q7, [r1, #256]      // .....................*.........
        // vsub.U32 q7, q4, q1           // ...............*...............
        // vadd.U32 q1, q4, q1           // .............*.................
        // vstrw.U32 q7, [r0, #256]      // .................*.............
        // vstrw.U32 q1, [r0], #16       // ...................*...........
        // vadd.U32 q3, q3, q2           // ......................*........
        // vstrw.U32 q3, [r1], #16       // .......................*.......

        .unreq in_high
        .unreq in_low

        in .req r0
        sub in, in, #(64*4)

        // Layers 3,4

        // 4 butterfly blocks per root config, 4 root configs
        // loop over root configs

        count .req r1
        mov count, #4

out_start:
        ldrd root0, root0_twisted, [root_ptr], #+8
        ldrd root1, root1_twisted, [root_ptr], #+8
        ldrd root2, root2_twisted, [root_ptr], #+8

        mov lr, #4
                                        // Instructions:    5
                                        // Expected cycles: 6
                                        // Expected IPC:    0.83
                                        //
                                        // Wall time:     0.00s
                                        // User time:     0.00s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vldrw.U32 q4, [r0, #192]        // *.............................
        vqrdmulh.S32 q1, q4, r3         // .*............................
        vmul.S32 q0, q4, r2             // ...*..........................
        vldrw.U32 q4, [r0, #64]         // ....*.........................
        vmla.S32 q0, q1, r12            // .....*........................

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vldrw.U32 q4, [r0, #64]       // ....*..........................
        // vldrw.U32 q3, [r0, #192]      // *..............................
        // vmul.S32 q0, q3, r2           // ...*...........................
        // vqrdmulh.S32 q5, q3, r3       // .*.............................
        // vmla.S32 q0, q5, r12          // .....*.........................

        sub lr, lr, #1
.p2align 2
layer34_loop:
                                        // Instructions:    28
                                        // Expected cycles: 28
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     2.17s
                                        // User time:     2.17s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vsub.U32 q2, q4, q0             // *.............................
        vqrdmulh.S32 q6, q2, r7         // .*............................
        vldrw.U32 q5, [r0, #128]        // ..*...........................
        vqrdmulh.S32 q1, q5, r3         // ...*..........................
        vadd.U32 q0, q4, q0             // ....*.........................
        vmul.S32 q7, q5, r2             // .....*........................
        vldrw.U32 q4, [r0, #80]         // ......e.......................
        vmla.S32 q7, q1, r12            // .......*......................
        vldrw.U32 q3, [r0]              // ........*.....................
        vqrdmulh.S32 q1, q0, r5         // .........*....................
        vsub.U32 q5, q3, q7             // ..........*...................
        vmul.S32 q0, q0, r4             // ...........*..................
        vadd.U32 q7, q3, q7             // ............*.................
        vmla.S32 q0, q1, r12            // .............*................
        vldrw.U32 q3, [r0, #208]        // ..............e...............
        vadd.U32 q1, q7, q0             // ...............*..............
        vmul.S32 q2, q2, r6             // ................*.............
        vsub.U32 q0, q7, q0             // .................*............
        vmla.S32 q2, q6, r12            // ..................*...........
        vstrw.U32 q0, [r0, #64]         // ...................*..........
        vsub.U32 q7, q5, q2             // ....................*.........
        vmul.S32 q0, q3, r2             // .....................e........
        vadd.U32 q6, q5, q2             // ......................*.......
        vstrw.U32 q1, [r0], #16         // .......................*......
        vqrdmulh.S32 q5, q3, r3         // ........................e.....
        vstrw.U32 q7, [r0, #176]        // .........................*....
        vmla.S32 q0, q5, r12            // ..........................e...
        vstrw.U32 q6, [r0, #112]        // ...........................*..

                                                   // --------------- cycle (expected) ---------------->
                                                   // 0                        25
                                                   // |------------------------|------------------------
        // vldrw.u32 q0, [r0]                      // ..~...................'.......*...................
        // vldrw.u32 q1, [r0, #(4*1*16)]           // e.....................'.....~.....................
        // vldrw.u32 q2, [r0, #(4*2*16)]           // ......................'.*.........................
        // vldrw.u32 q3, [r0, #(4*3*16)]           // ........e.............'.............~.............
        // vmul.s32       q4,  q2, r2              // ......................'....*......................
        // vqrdmulh.s32   q2,  q2, r3              // ......................'..*........................
        // vmla.s32       q4,  q2, r12             // .~....................'......*....................
        // vsub.u32       q2,    q0, q4            // ....~.................'.........*.................
        // vadd.u32       q0,    q0, q4            // ......~...............'...........*...............
        // vmul.s32       q4,  q3, r2              // ...............e......'....................~......
        // vqrdmulh.s32   q3,  q3, r3              // ..................e...'.......................~...
        // vmla.s32       q4,  q3, r12             // ....................e.'.........................~.
        // vsub.u32       q3,    q1, q4            // ......................*...........................
        // vadd.u32       q1,    q1, q4            // ......................'...*.......................
        // vmul.s32       q4,  q1, r4              // .....~................'..........*................
        // vqrdmulh.s32   q1,  q1, r5              // ...~..................'........*..................
        // vmla.s32       q4,  q1, r12             // .......~..............'............*..............
        // vsub.u32       q1,    q0, q4            // ...........~..........'................*..........
        // vadd.u32       q0,    q0, q4            // .........~............'..............*............
        // vmul.s32       q4,  q3, r6              // ..........~...........'...............*...........
        // vqrdmulh.s32   q3,  q3, r7              // ......................'*..........................
        // vmla.s32       q4,  q3, r12             // ............~.........'.................*.........
        // vsub.u32       q3,    q2, q4            // ..............~.......'...................*.......
        // vadd.u32       q2,    q2, q4            // ................~.....'.....................*.....
        // vstrw.u32 q0, [r0], #16                 // .................~....'......................*....
        // vstrw.u32 q1, [r0, #(4*1*16 - 16)]      // .............~........'..................*........
        // vstrw.u32 q2, [r0, #(4*2*16 - 16)]      // .....................~'..........................*
        // vstrw.u32 q3, [r0, #(4*3*16 - 16)]      // ...................~..'........................*..

        le lr, layer34_loop
                                        // Instructions:    23
                                        // Expected cycles: 24
                                        // Expected IPC:    0.96
                                        //
                                        // Wall time:     0.14s
                                        // User time:     0.14s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vldrw.U32 q5, [r0, #128]        // *.............................
        vqrdmulh.S32 q6, q5, r3         // .*............................
        vadd.U32 q7, q4, q0             // ..*...........................
        vmul.S32 q1, q5, r2             // ...*..........................
        vsub.U32 q2, q4, q0             // ....*.........................
        vmla.S32 q1, q6, r12            // .....*........................
        vldrw.U32 q3, [r0]              // ......*.......................
        vqrdmulh.S32 q4, q7, r5         // .......*......................
        vadd.U32 q6, q3, q1             // ........*.....................
        vmul.S32 q7, q7, r4             // .........*....................
        vmla.S32 q7, q4, r12            // ...........*..................
        vsub.U32 q5, q3, q1             // ............*.................
        vqrdmulh.S32 q3, q2, r7         // .............*................
        vsub.U32 q4, q6, q7             // ..............*...............
        vmul.S32 q2, q2, r6             // ...............*..............
        vadd.U32 q6, q6, q7             // ................*.............
        vstrw.U32 q4, [r0, #64]         // .................*............
        vmla.S32 q2, q3, r12            // ..................*...........
        vstrw.U32 q6, [r0], #16         // ...................*..........
        vsub.U32 q0, q5, q2             // ....................*.........
        vstrw.U32 q0, [r0, #176]        // .....................*........
        vadd.U32 q4, q5, q2             // ......................*.......
        vstrw.U32 q4, [r0, #112]        // .......................*......

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vsub.U32 q2, q4, q0           // ....*..........................
        // vqrdmulh.S32 q6, q2, r7       // .............*.................
        // vldrw.U32 q5, [r0, #128]      // *..............................
        // vqrdmulh.S32 q1, q5, r3       // .*.............................
        // vadd.U32 q0, q4, q0           // ..*............................
        // vmul.S32 q7, q5, r2           // ...*...........................
        // vmla.S32 q7, q1, r12          // .....*.........................
        // vldrw.U32 q3, [r0]            // ......*........................
        // vqrdmulh.S32 q1, q0, r5       // .......*.......................
        // vsub.U32 q5, q3, q7           // ............*..................
        // vmul.S32 q0, q0, r4           // .........*.....................
        // vadd.U32 q7, q3, q7           // ........*......................
        // vmla.S32 q0, q1, r12          // ...........*...................
        // vadd.U32 q1, q7, q0           // ................*..............
        // vmul.S32 q2, q2, r6           // ...............*...............
        // vsub.U32 q0, q7, q0           // ..............*................
        // vmla.S32 q2, q6, r12          // ..................*............
        // vstrw.U32 q0, [r0, #64]       // .................*.............
        // vsub.U32 q7, q5, q2           // ....................*..........
        // vadd.U32 q6, q5, q2           // ......................*........
        // vstrw.U32 q1, [r0], #16       // ...................*...........
        // vstrw.U32 q7, [r0, #176]      // .....................*.........
        // vstrw.U32 q6, [r0, #112]      // .......................*.......


        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*256)

        // Layers 5,6

        // 1 butterfly blocks per root config, 16 root configs
        // loop over root configs

        mov lr, #16
                                        // Instructions:    7
                                        // Expected cycles: 7
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     0.00s
                                        // User time:     0.00s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        ldrd r8, r7, [r11], #+24        // *.............................
        vldrw.U32 q5, [r0, #48]         // .*............................
        vqrdmulh.S32 q1, q5, r7         // ..*...........................
        vldrw.U32 q6, [r0, #16]         // ...*..........................
        vmul.S32 q7, q5, r8             // ....*.........................
        vldrw.U32 q5, [r0, #32]         // .....*........................
        vmla.S32 q7, q1, r12            // ......*.......................

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // ldrd r8, r7, [r11], #+24      // *..............................
        // vldrw.U32 q5, [r0, #32]       // .....*.........................
        // vldrw.U32 q4, [r0, #48]       // .*.............................
        // vqrdmulh.S32 q3, q4, r7       // ..*............................
        // vldrw.U32 q6, [r0, #16]       // ...*...........................
        // vmul.S32 q7, q4, r8           // ....*..........................
        // vmla.S32 q7, q3, r12          // ......*........................

        sub lr, lr, #1
.p2align 2
layer56_loop:
                                        // Instructions:    31
                                        // Expected cycles: 31
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     3.74s
                                        // User time:     3.74s
                                        //
                                        // ------ cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|-----
        vmul.S32 q0, q5, r8             // *..............................
        vadd.U32 q4, q6, q7             // .*.............................
        vqrdmulh.S32 q2, q5, r7         // ..*............................
        vldrw.U32 q3, [r0]              // ...*...........................
        vmla.S32 q0, q2, r12            // ....*..........................
        ldrd r3, r6, [r11, #-16]        // .....*.........................
        ldrd r8, r7, [r11], #+24        // ......e........................
        vmul.S32 q1, q4, r3             // .......*.......................
        vadd.U32 q2, q3, q0             // ........*......................
        vqrdmulh.S32 q5, q4, r6         // .........*.....................
        vsub.U32 q4, q6, q7             // ..........*....................
        ldrd r6, r4, [r11, #-32]        // ...........*...................
        vmla.S32 q1, q5, r12            // ............*..................
        vsub.U32 q7, q3, q0             // .............*.................
        vqrdmulh.S32 q6, q4, r4         // ..............*................
        vldrw.U32 q5, [r0, #96]         // ...............e...............
        vmul.S32 q0, q4, r6             // ................*..............
        vldrw.U32 q4, [r0, #112]        // .................e.............
        vmla.S32 q0, q6, r12            // ..................*............
        vadd.U32 q3, q2, q1             // ...................*...........
        vstrw.U32 q3, [r0], #64         // ....................*..........
        vsub.U32 q6, q7, q0             // .....................*.........
        vstrw.U32 q6, [r0, #-16]        // ......................*........
        vadd.U32 q0, q7, q0             // .......................*.......
        vqrdmulh.S32 q3, q4, r7         // ........................e......
        vldrw.U32 q6, [r0, #16]         // .........................e.....
        vmul.S32 q7, q4, r8             // ..........................e....
        vstrw.U32 q0, [r0, #-32]        // ...........................*...
        vmla.S32 q7, q3, r12            // ............................e..
        vsub.U32 q4, q2, q1             // .............................*.
        vstrw.U32 q4, [r0, #-48]        // ..............................*

                                                  // ------------------ cycle (expected) ------------------->
                                                  // 0                        25                       50
                                                  // |------------------------|------------------------|-----
        // ldrd r2, r3, [r11], #+24               // e........................'.....~........................
        // ldrd r4, r5, [r11, #(-16)]             // .........................'....*.........................
        // ldrd r6, r7, [r11, #(-8)]              // .....~...................'..........*...................
        // vldrw.u32 q0, [r0]                     // .........................'..*...........................
        // vldrw.u32 q1, [r0, #(4*1*4)]           // ...................e.....'........................~.....
        // vldrw.u32 q2, [r0, #(4*2*4)]           // .........e...............'..............~...............
        // vldrw.u32 q3, [r0, #(4*3*4)]           // ...........e.............'................~.............
        // vmul.s32       q4,  q2, r2             // .........................*..............................
        // vqrdmulh.s32   q2,  q2, r3             // .........................'.*............................
        // vmla.s32       q4,  q2, r12            // .........................'...*..........................
        // vsub.u32       q2,    q0, q4           // .......~.................'............*.................
        // vadd.u32       q0,    q0, q4           // ..~......................'.......*......................
        // vmul.s32       q4,  q3, r2             // ....................e....'.........................~....
        // vqrdmulh.s32   q3,  q3, r3             // ..................e......'.......................~......
        // vmla.s32       q4,  q3, r12            // ......................e..'...........................~..
        // vsub.u32       q3,    q1, q4           // ....~....................'.........*....................
        // vadd.u32       q1,    q1, q4           // .........................'*.............................
        // vmul.s32       q4,  q1, r4             // .~.......................'......*.......................
        // vqrdmulh.s32   q1,  q1, r5             // ...~.....................'........*.....................
        // vmla.s32       q4,  q1, r12            // ......~..................'...........*..................
        // vsub.u32       q1,    q0, q4           // .......................~.'............................*.
        // vadd.u32       q0,    q0, q4           // .............~...........'..................*...........
        // vmul.s32       q4,  q3, r6             // ..........~..............'...............*..............
        // vqrdmulh.s32   q3,  q3, r7             // ........~................'.............*................
        // vmla.s32       q4,  q3, r12            // ............~............'.................*............
        // vsub.u32       q3,    q2, q4           // ...............~.........'....................*.........
        // vadd.u32       q2,    q2, q4           // .................~.......'......................*.......
        // vstrw.u32 q0, [r0], #64                // ..............~..........'...................*..........
        // vstrw.u32 q1, [r0, #(4*1*4 - 64)]      // ........................~'.............................*
        // vstrw.u32 q2, [r0, #(4*2*4 - 64)]      // .....................~...'..........................*...
        // vstrw.u32 q3, [r0, #(4*3*4 - 64)]      // ................~........'.....................*........

        le lr, layer56_loop
                                        // Instructions:    24
                                        // Expected cycles: 24
                                        // Expected IPC:    1.00
                                        //
                                        // Wall time:     0.19s
                                        // User time:     0.19s
                                        //
                                        // ----- cycle (expected) ------>
                                        // 0                        25
                                        // |------------------------|----
        vqrdmulh.S32 q4, q5, r7         // *.............................
        vsub.U32 q3, q6, q7             // .*............................
        vmul.S32 q0, q5, r8             // ..*...........................
        ldrd r3, r6, [r11, #-16]        // ...*..........................
        vmla.S32 q0, q4, r12            // ....*.........................
        vadd.U32 q5, q6, q7             // .....*........................
        vqrdmulh.S32 q4, q5, r6         // ......*.......................
        vldrw.U32 q6, [r0]              // .......*......................
        vmul.S32 q7, q5, r3             // ........*.....................
        vadd.U32 q1, q6, q0             // .........*....................
        vmla.S32 q7, q4, r12            // ..........*...................
        ldrd r4, r7, [r11, #-8]         // ...........*..................
        vsub.U32 q4, q1, q7             // ............*.................
        vstrw.U32 q4, [r0, #16]         // .............*................
        vmul.S32 q5, q3, r4             // ..............*...............
        vsub.U32 q2, q6, q0             // ...............*..............
        vqrdmulh.S32 q6, q3, r7         // ................*.............
        vadd.U32 q4, q1, q7             // .................*............
        vmla.S32 q5, q6, r12            // ..................*...........
        vstrw.U32 q4, [r0], #64         // ...................*..........
        vadd.U32 q6, q2, q5             // ....................*.........
        vstrw.U32 q6, [r0, #-32]        // .....................*........
        vsub.U32 q4, q2, q5             // ......................*.......
        vstrw.U32 q4, [r0, #-16]        // .......................*......

                                         // ------ cycle (expected) ------>
                                         // 0                        25
                                         // |------------------------|-----
        // vmul.S32 q0, q5, r8           // ..*............................
        // vadd.U32 q4, q6, q7           // .....*.........................
        // vqrdmulh.S32 q2, q5, r7       // *..............................
        // vldrw.U32 q3, [r0]            // .......*.......................
        // vmla.S32 q0, q2, r12          // ....*..........................
        // ldrd r3, r6, [r11, #-16]      // ...*...........................
        // vmul.S32 q1, q4, r3           // ........*......................
        // vadd.U32 q2, q3, q0           // .........*.....................
        // vqrdmulh.S32 q5, q4, r6       // ......*........................
        // vsub.U32 q4, q6, q7           // .*.............................
        // ldrd r6, r4, [r11, #-8]       // ...........*...................
        // vmla.S32 q1, q5, r12          // ..........*....................
        // vsub.U32 q7, q3, q0           // ...............*...............
        // vqrdmulh.S32 q6, q4, r4       // ................*..............
        // vmul.S32 q0, q4, r6           // ..............*................
        // vmla.S32 q0, q6, r12          // ..................*............
        // vadd.U32 q3, q2, q1           // .................*.............
        // vstrw.U32 q3, [r0], #64       // ...................*...........
        // vsub.U32 q6, q7, q0           // ......................*........
        // vstrw.U32 q6, [r0, #-16]      // .......................*.......
        // vadd.U32 q0, q7, q0           // ....................*..........
        // vstrw.U32 q0, [r0, #-32]      // .....................*.........
        // vsub.U32 q4, q2, q1           // ............*..................
        // vstrw.U32 q4, [r0, #-48]      // .............*.................


        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr
