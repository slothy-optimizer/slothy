
///
/// Copyright (c) 2021 Arm Limited
/// Copyright (c) 2022 Hanno Becker
/// Copyright (c) 2023 Amin Abdulrahman, Matthias Kannwischer
/// SPDX-License-Identifier: MIT
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy
/// of this software and associated documentation files (the "Software"), to deal
/// in the Software without restriction, including without limitation the rights
/// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
/// copies of the Software, and to permit persons to whom the Software is
/// furnished to do so, subject to the following conditions:
///
/// The above copyright notice and this permission notice shall be included in all
/// copies or substantial portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
/// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
/// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
/// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
/// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
/// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
/// SOFTWARE.
///

.data
roots:
#include "ntt_kyber_1_23_45_67_twiddles.s"
.text

// Barrett multiplication
.macro mulmod dst, src, const, const_twisted
        vmul.s16       \dst,  \src, \const
        vqrdmulh.s16   \src,  \src, \const_twisted
        vmla.s16       \dst,  \src, modulus
.endm

.macro ct_butterfly a, b, root, root_twisted
        mulmod tmp, \b, \root, \root_twisted
        vsub.u16       \b,    \a, tmp
        vadd.u16       \a,    \a, tmp
.endm

.macro load_first_root root0, root0_twisted
        ldrd root0, root0_twisted, [root_ptr], #+8
.endm

.macro load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted
        ldrd root0, root0_twisted, [root_ptr], #+24
        ldrd root1, root1_twisted, [root_ptr, #(-16)]
        ldrd root2, root2_twisted, [root_ptr, #(-8)]
.endm

.align 4
roots_addr: .word roots
.syntax unified
.type ntt_kyber_1_23_45_67_no_trans_vld4_opt_m85, %function
.global ntt_kyber_1_23_45_67_no_trans_vld4_opt_m85
ntt_kyber_1_23_45_67_no_trans_vld4_opt_m85:

        push {r4-r11,lr}
        // Save MVE vector registers
        vpush {d8-d15}

        modulus  .req r12
        root_ptr .req r11

        .equ modulus_const, -3329
        movw modulus, #:lower16:modulus_const
        ldr  root_ptr, roots_addr

        in_low       .req r0
        in_high      .req r1

        add in_high, in_low, #(4*64)

        root0         .req r2
        root0_twisted .req r3
        root1         .req r4
        root1_twisted .req r5
        root2         .req r6
        root2_twisted .req r7

        data0 .req q0
        data1 .req q1
        data2 .req q2
        data3 .req q3

        tmp .req q4

        /* Layers 1 */

        load_first_root root0, root0_twisted

        mov lr, #16
        vldrw.u32 q4, [r1]              // *.
        vqrdmulh.s16 q2, q4, r3         // .*
        
        // original source code
        // vldrw.u32 q4, [r1]           // *. 
        // vqrdmulh.s16 q2, q4, r3      // .* 
        
        sub lr, lr, #1
.p2align 2
layer1_loop:
        vmul.s16 q6, q4, r2              // ..*......
        vldrw.u32 q7, [r0]               // *........
        vmla.s16 q6, q2, r12             // ....*....
        vldrw.u32 q4, [r1, #16]          // .e.......
        vsub.u16 q1, q7, q6              // .....*...
        vstrw.u32 q1, [r1] , #16         // ........*
        vadd.u16 q1, q7, q6              // ......*..
        vqrdmulh.s16 q2, q4, r3          // ...e.....
        vstrw.u32 q1, [r0] , #16         // .......*.
        
        // original source code
        // vldrw.u32 q0, [r0]                // ......|*....... 
        // vldrw.u32 q1, [r1]                // e.....|..e..... 
        // vmul.s16       q4,  q1, r2        // ......*........ 
        // vqrdmulh.s16   q1,  q1, r3        // ....e.|......e. 
        // vmla.s16       q4,  q1, r12       // ......|.*...... 
        // vsub.u16       q1,    q0, q4      // .*....|...*.... 
        // vadd.u16       q0,    q0, q4      // ...*..|.....*.. 
        // vstrw.u32 q0, [r0], #16           // .....*|.......* 
        // vstrw.u32 q1, [r1], #16           // ..*...|....*... 
        
        le lr, layer1_loop
        vmul.s16 q3, q4, r2              // *......
        vldrw.u32 q0, [r0]               // .*.....
        vmla.s16 q3, q2, r12             // ..*....
        // gap                           // .......
        vsub.u16 q6, q0, q3              // ...*...
        vstrw.u32 q6, [r1] , #16         // ....*..
        vadd.u16 q0, q0, q3              // .....*.
        vstrw.u32 q0, [r0] , #16         // ......*
        
        // original source code
        // vmul.s16 q6, q4, r2           // *...... 
        // vldrw.u32 q7, [r0]            // .*..... 
        // vmla.s16 q6, q2, r12          // ..*.... 
        // vsub.u16 q1, q7, q6           // ...*... 
        // vstrw.u32 q1, [r1] , #16      // ....*.. 
        // vadd.u16 q1, q7, q6           // .....*. 
        // vstrw.u32 q1, [r0] , #16      // ......* 
        
        .unreq in_high
        .unreq in_low

        in .req r0
        sub in, in, #(4*64)

        /* Layers 2,3 */

        count .req r1
        mov count, #2

out_start:
        load_next_roots root0, root0_twisted, root1, root1_twisted, root2, root2_twisted

        mov lr, #4
        vldrw.u32 q7, [r0, #128]          // *.
        vmul.s16 q6, q7, r2               // .*
        
        // original source code
        // vldrw.u32 q7, [r0, #128]      // *. 
        // vmul.s16 q6, q7, r2           // .* 
        
        sub lr, lr, #1
.p2align 2
layer23_loop:
        vqrdmulh.s16 q3, q7, r3           // .....*......................
        vldrw.u32 q1, [r0, #192]          // ...*........................
        vqrdmulh.s16 q2, q1, r3           // ..........*.................
        vldrw.u32 q7, [r0, #144]          // ..e.........................
        vmul.s16 q5, q1, r2               // .........*..................
        vldrw.u32 q1, [r0, #64]           // .*..........................
        vmla.s16 q5, q2, r12              // ...........*................
        vldrw.u32 q2, [r0]                // *...........................
        vmla.s16 q6, q3, r12              // ......*.....................
        vadd.u16 q3, q1, q5               // .............*..............
        vmul.s16 q4, q3, r4               // ..............*.............
        vsub.u16 q0, q2, q6               // .......*....................
        vqrdmulh.s16 q3, q3, r5           // ...............*............
        vadd.u16 q6, q2, q6               // ........*...................
        vmla.s16 q4, q3, r12              // ................*...........
        vsub.u16 q5, q1, q5               // ............*...............
        vqrdmulh.s16 q1, q5, r7           // ....................*.......
        vsub.u16 q2, q6, q4               // .................*..........
        vmul.s16 q5, q5, r6               // ...................*........
        vstrw.u32 q2, [r0, #64]           // .........................*..
        vmla.s16 q5, q1, r12              // .....................*......
        vadd.u16 q2, q6, q4               // ..................*.........
        vmul.s16 q6, q7, r2               // ....e.......................
        vstrw.u32 q2, [r0] , #16          // ........................*...
        vsub.u16 q2, q0, q5               // ......................*.....
        vstrw.u32 q2, [r0, #176]          // ...........................*
        vadd.u16 q5, q0, q5               // .......................*....
        vstrw.u32 q5, [r0, #112]          // ..........................*.
        
        // original source code
        // vldrw.u32 q0, [r0]                      // ....*....................|......*.................... 
        // vldrw.u32 q1, [r0, #(4*1*16)]           // ..*......................|....*...................... 
        // vldrw.u32 q2, [r0, #(4*2*16)]           // e........................|..e........................ 
        // vldrw.u32 q3, [r0, #(4*3*16)]           // .........................|*.......................... 
        // vmul.s16       q4,  q2, r2              // ...................e.....|.....................e..... 
        // vqrdmulh.s16   q2,  q2, r3              // .........................*........................... 
        // vmla.s16       q4,  q2, r12             // .....*...................|.......*................... 
        // vsub.u16       q2,    q0, q4            // ........*................|..........*................ 
        // vadd.u16       q0,    q0, q4            // ..........*..............|............*.............. 
        // vmul.s16       q4,  q3, r2              // .*.......................|...*....................... 
        // vqrdmulh.s16   q3,  q3, r3              // .........................|.*......................... 
        // vmla.s16       q4,  q3, r12             // ...*.....................|.....*..................... 
        // vsub.u16       q3,    q1, q4            // ............*............|..............*............ 
        // vadd.u16       q1,    q1, q4            // ......*..................|........*.................. 
        // vmul.s16       q4,  q1, r4              // .......*.................|.........*................. 
        // vqrdmulh.s16   q1,  q1, r5              // .........*...............|...........*............... 
        // vmla.s16       q4,  q1, r12             // ...........*.............|.............*............. 
        // vsub.u16       q1,    q0, q4            // ..............*..........|................*.......... 
        // vadd.u16       q0,    q0, q4            // ..................*......|....................*...... 
        // vmul.s16       q4,  q3, r6              // ...............*.........|.................*......... 
        // vqrdmulh.s16   q3,  q3, r7              // .............*...........|...............*........... 
        // vmla.s16       q4,  q3, r12             // .................*.......|...................*....... 
        // vsub.u16       q3,    q2, q4            // .....................*...|.......................*... 
        // vadd.u16       q2,    q2, q4            // .......................*.|.........................*. 
        // vstrw.u32 q0, [r0], #16                 // ....................*....|......................*.... 
        // vstrw.u32 q1, [r0, #(4*1*16 - 16)]      // ................*........|..................*........ 
        // vstrw.u32 q2, [r0, #(4*2*16 - 16)]      // ........................*|..........................* 
        // vstrw.u32 q3, [r0, #(4*3*16 - 16)]      // ......................*..|........................*.. 
        
        le lr, layer23_loop
        vqrdmulh.s16 q1, q7, r3           // *.........................
        vldrw.u32 q5, [r0, #192]          // .*........................
        vmul.s16 q3, q5, r2               // ...*......................
        vldrw.u32 q0, [r0]                // ......*...................
        vqrdmulh.s16 q4, q5, r3           // ..*.......................
        vldrw.u32 q2, [r0, #64]           // ....*.....................
        vmla.s16 q3, q4, r12              // .....*....................
        // gap                            // ..........................
        vmla.s16 q6, q1, r12              // .......*..................
        vsub.u16 q4, q2, q3               // ..............*...........
        vqrdmulh.s16 q5, q4, r7           // ...............*..........
        vadd.u16 q7, q2, q3               // ........*.................
        vmul.s16 q3, q4, r6               // .................*........
        vsub.u16 q2, q0, q6               // ..........*...............
        vmla.s16 q3, q5, r12              // ...................*......
        vadd.u16 q1, q0, q6               // ............*.............
        vqrdmulh.s16 q4, q7, r5           // ...........*..............
        vadd.u16 q5, q2, q3               // ........................*.
        vstrw.u32 q5, [r0, #128]          // .........................*
        vmul.s16 q5, q7, r4               // .........*................
        vsub.u16 q3, q2, q3               // ......................*...
        vmla.s16 q5, q4, r12              // .............*............
        vstrw.u32 q3, [r0, #192]          // .......................*..
        vsub.u16 q0, q1, q5               // ................*.........
        vstrw.u32 q0, [r0, #64]           // ..................*.......
        vadd.u16 q2, q1, q5               // ....................*.....
        vstrw.u32 q2, [r0] , #16          // .....................*....
        
        // original source code
        // vqrdmulh.s16 q3, q7, r3       // *......................... 
        // vldrw.u32 q1, [r0, #192]      // .*........................ 
        // vqrdmulh.s16 q2, q1, r3       // ....*..................... 
        // vmul.s16 q5, q1, r2           // ..*....................... 
        // vldrw.u32 q1, [r0, #64]       // .....*.................... 
        // vmla.s16 q5, q2, r12          // ......*................... 
        // vldrw.u32 q2, [r0]            // ...*...................... 
        // vmla.s16 q6, q3, r12          // .......*.................. 
        // vadd.u16 q3, q1, q5           // ..........*............... 
        // vmul.s16 q4, q3, r4           // ..................*....... 
        // vsub.u16 q0, q2, q6           // ............*............. 
        // vqrdmulh.s16 q3, q3, r5       // ...............*.......... 
        // vadd.u16 q6, q2, q6           // ..............*........... 
        // vmla.s16 q4, q3, r12          // ....................*..... 
        // vsub.u16 q5, q1, q5           // ........*................. 
        // vqrdmulh.s16 q1, q5, r7       // .........*................ 
        // vsub.u16 q2, q6, q4           // ......................*... 
        // vmul.s16 q5, q5, r6           // ...........*.............. 
        // vstrw.u32 q2, [r0, #64]       // .......................*.. 
        // vmla.s16 q5, q1, r12          // .............*............ 
        // vadd.u16 q2, q6, q4           // ........................*. 
        // vstrw.u32 q2, [r0] , #16      // .........................* 
        // vsub.u16 q2, q0, q5           // ...................*...... 
        // vstrw.u32 q2, [r0, #176]      // .....................*.... 
        // vadd.u16 q5, q0, q5           // ................*......... 
        // vstrw.u32 q5, [r0, #112]      // .................*........ 
        

        add in, in, #(4*64 - 4*16)
        subs count, count, #1
        bne out_start

        sub in, in, #(4*128)

        /* Layers 4,5 */

        mov lr, #8
.p2align 2
layer45_loop:
        ldrd r4, r5, [r11] , #24          // *..............................
        vldrw.u32 q5, [r0, #32]           // .....*.........................
        vqrdmulh.s16 q3, q5, r5           // ........*......................
        vldrw.u32 q7, [r0, #48]           // ......*........................
        vqrdmulh.s16 q0, q7, r5           // .............*.................
        ldrd r8, r6, [r11, #-8]           // ..*............................
        vmul.s16 q4, q7, r4               // ............*..................
        vldrw.u32 q1, [r0]                // ...*...........................
        vmla.s16 q4, q0, r12              // ..............*................
        vldrw.u32 q2, [r0, #16]           // ....*..........................
        vmul.s16 q7, q5, r4               // .......*.......................
        vsub.u16 q6, q2, q4               // ...............*...............
        vmul.s16 q0, q6, r8               // ......................*........
        ldrd r2, r1, [r11, #-16]          // .*.............................
        vmla.s16 q7, q3, r12              // .........*.....................
        vadd.u16 q5, q2, q4               // ................*..............
        vqrdmulh.s16 q4, q6, r6           // .......................*.......
        vadd.u16 q3, q1, q7               // ...........*...................
        vmla.s16 q0, q4, r12              // ........................*......
        vsub.u16 q7, q1, q7               // ..........*....................
        vqrdmulh.s16 q1, q5, r1           // ..................*............
        vadd.u16 q2, q7, q0               // ..........................*....
        vstrw.u32 q2, [r0, #32]           // .............................*.
        vmul.s16 q2, q5, r2               // .................*.............
        vsub.u16 q5, q7, q0               // .........................*.....
        vmla.s16 q2, q1, r12              // ...................*...........
        vstrw.u32 q5, [r0, #48]           // ..............................*
        vsub.u16 q5, q3, q2               // ....................*..........
        vstrw.u32 q5, [r0, #16]           // ............................*..
        vadd.u16 q6, q3, q2               // .....................*.........
        vstrw.u32 q6, [r0] , #64          // ...........................*...
        
        // original source code
        // ldrd r2, r3, [r11], #+24           // *.............................. 
        // ldrd r4, r5, [r11, #(-16)]         // .............*................. 
        // ldrd r6, r7, [r11, #(-8)]          // .....*......................... 
        // vldrw.u32 q0, [r0]                 // .......*....................... 
        // vldrw.u32 q1, [r0, #16]            // .........*..................... 
        // vldrw.u32 q2, [r0, #32]            // .*............................. 
        // vldrw.u32 q3, [r0, #48]            // ...*........................... 
        // vmul.s16       q4,  q2, r2         // ..........*.................... 
        // vqrdmulh.s16   q2,  q2, r3         // ..*............................ 
        // vmla.s16       q4,  q2, r12        // ..............*................ 
        // vsub.u16       q2,    q0, q4       // ...................*........... 
        // vadd.u16       q0,    q0, q4       // .................*............. 
        // vmul.s16       q4,  q3, r2         // ......*........................ 
        // vqrdmulh.s16   q3,  q3, r3         // ....*.......................... 
        // vmla.s16       q4,  q3, r12        // ........*...................... 
        // vsub.u16       q3,    q1, q4       // ...........*................... 
        // vadd.u16       q1,    q1, q4       // ...............*............... 
        // vmul.s16       q4,  q1, r4         // .......................*....... 
        // vqrdmulh.s16   q1,  q1, r5         // ....................*.......... 
        // vmla.s16       q4,  q1, r12        // .........................*..... 
        // vsub.u16       q1,    q0, q4       // ...........................*... 
        // vadd.u16       q0,    q0, q4       // .............................*. 
        // vmul.s16       q4,  q3, r6         // ............*.................. 
        // vqrdmulh.s16   q3,  q3, r7         // ................*.............. 
        // vmla.s16       q4,  q3, r12        // ..................*............ 
        // vsub.u16       q3,    q2, q4       // ........................*...... 
        // vadd.u16       q2,    q2, q4       // .....................*......... 
        // vstrw.u32 q0, [r0], #64            // ..............................* 
        // vstrw.u32 q1, [r0, #(-64+16)]      // ............................*.. 
        // vstrw.u32 q2, [r0, #(-64+32)]      // ......................*........ 
        // vstrw.u32 q3, [r0, #(-64+48)]      // ..........................*.... 
        
        le lr, layer45_loop

        sub in, in, #(4*128)

        /* Layers 6,7 */

        .unreq root0
        .unreq root0_twisted
        .unreq root1
        .unreq root1_twisted
        .unreq root2
        .unreq root2_twisted

        root0         .req q5
        root0_twisted .req q6
        root1         .req q5
        root1_twisted .req q6
        root2         .req q5
        root2_twisted .req q6

        mov lr, #8
        vld40.u32 {q1,q2,q3,q4}, [r0]          // *.....
        // gap                                 // ......
        vld41.u32 {q1,q2,q3,q4}, [r0]          // .*....
        // gap                                 // ......
        vld42.u32 {q1,q2,q3,q4}, [r0]          // ..*...
        // gap                                 // ......
        vld43.u32 {q1,q2,q3,q4}, [r0]!         // ...*..
        // gap                                 // ......
        vldrh.u16 q7, [r11] , #96              // ....*.
        // gap                                 // ......
        vmul.s16 q5, q3, q7                    // .....*
        
        // original source code
        // vld40.u32 {q1,q2,q3,q4}, [r0]       // *..... 
        // vld41.u32 {q1,q2,q3,q4}, [r0]       // .*.... 
        // vld42.u32 {q1,q2,q3,q4}, [r0]       // ..*... 
        // vld43.u32 {q1,q2,q3,q4}, [r0]!      // ...*.. 
        // vldrh.u16 q7, [r11] , #96           // ....*. 
        // vmul.s16 q5, q3, q7                 // .....* 
        
        sub lr, lr, #1
.p2align 2
layer67_loop:
        vmul.s16 q7, q4, q7                    // ...........*......................
        vldrh.u16 q0, [r11, #-80]              // .....*............................
        vqrdmulh.s16 q4, q4, q0                // ............*.....................
        vldrh.u16 q6, [r11, #-64]              // ................*.................
        vmla.s16 q7, q4, r12                   // .............*....................
        vldrh.u16 q4, [r11, #-48]              // .................*................
        vqrdmulh.s16 q3, q3, q0                // .......*..........................
        vadd.u16 q0, q2, q7                    // ...............*..................
        vmla.s16 q5, q3, r12                   // ........*.........................
        vsub.u16 q7, q2, q7                    // ..............*...................
        vqrdmulh.s16 q3, q0, q4                // ...................*..............
        vadd.u16 q4, q1, q5                    // ..........*.......................
        vmul.s16 q0, q0, q6                    // ..................*...............
        vsub.u16 q6, q1, q5                    // .........*........................
        vmla.s16 q0, q3, r12                   // ....................*.............
        vldrh.u16 q5, [r11, #-32]              // .......................*..........
        vsub.u16 q2, q4, q0                    // .....................*............
        vstrw.u32 q2, [r0, #-48]               // ...............................*..
        vadd.u16 q0, q4, q0                    // ......................*...........
        vld40.u32 {q1,q2,q3,q4}, [r0]          // e.................................
        vstrw.u32 q0, [r0, #-64]               // ..............................*...
        vmul.s16 q5, q7, q5                    // .........................*........
        vldrh.u16 q0, [r11, #-16]              // ........................*.........
        vqrdmulh.s16 q0, q7, q0                // ..........................*.......
        vld41.u32 {q1,q2,q3,q4}, [r0]          // .e................................
        vmla.s16 q5, q0, r12                   // ...........................*......
        vld42.u32 {q1,q2,q3,q4}, [r0]          // ..e...............................
        vsub.u16 q0, q6, q5                    // ............................*.....
        vld43.u32 {q1,q2,q3,q4}, [r0]!         // ...e..............................
        vadd.u16 q6, q6, q5                    // .............................*....
        vstrw.u32 q0, [r0, #-80]               // .................................*
        vldrh.u16 q7, [r11] , #96              // ....e.............................
        vmul.s16 q5, q3, q7                    // ......e...........................
        vstrw.u32 q6, [r0, #-96]               // ................................*.
        
        // original source code
        // vld40.u32 {q0, q1, q2, q3}, [r0]             // e..............|..................e.............. 
        // vld41.u32 {q0, q1, q2, q3}, [r0]             // .....e.........|.......................e......... 
        // vld42.u32 {q0, q1, q2, q3}, [r0]             // .......e.......|.........................e....... 
        // vld43.u32 {q0, q1, q2, q3}, [r0]!            // .........e.....|...........................e..... 
        // vldrh.u16 q5,         [r11], #+96            // ............e..|..............................e.. 
        // vldrh.u16 q6, [r11, #(+16-96)]               // ...............|*................................ 
        // vmul.s16       q4,  q2, q5                   // .............e.|...............................e. 
        // vqrdmulh.s16   q2,  q2, q6                   // ...............|.....*........................... 
        // vmla.s16       q4,  q2, r12                  // ...............|.......*......................... 
        // vsub.u16       q2,    q0, q4                 // ...............|............*.................... 
        // vadd.u16       q0,    q0, q4                 // ...............|..........*...................... 
        // vmul.s16       q4,  q3, q5                   // ...............*................................. 
        // vqrdmulh.s16   q3,  q3, q6                   // ...............|.*............................... 
        // vmla.s16       q4,  q3, r12                  // ...............|...*............................. 
        // vsub.u16       q3,    q1, q4                 // ...............|........*........................ 
        // vadd.u16       q1,    q1, q4                 // ...............|......*.......................... 
        // vldrh.u16 q5,         [r11, #(32 - 96)]      // ...............|..*.............................. 
        // vldrh.u16 q6, [r11, #(48 - 96)]              // ...............|....*............................ 
        // vmul.s16       q4,  q1, q5                   // ...............|...........*..................... 
        // vqrdmulh.s16   q1,  q1, q6                   // ...............|.........*....................... 
        // vmla.s16       q4,  q1, r12                  // ...............|.............*................... 
        // vsub.u16       q1,    q0, q4                 // ...............|...............*................. 
        // vadd.u16       q0,    q0, q4                 // ...............|.................*............... 
        // vldrh.u16 q5,         [r11, #(64-96)]        // ...............|..............*.................. 
        // vldrh.u16 q6, [r11, #(80-96)]                // ...*...........|.....................*........... 
        // vmul.s16       q4,  q3, q5                   // ..*............|....................*............ 
        // vqrdmulh.s16   q3,  q3, q6                   // ....*..........|......................*.......... 
        // vmla.s16       q4,  q3, r12                  // ......*........|........................*........ 
        // vsub.u16       q3,    q2, q4                 // ........*......|..........................*...... 
        // vadd.u16       q2,    q2, q4                 // ..........*....|............................*.... 
        // vstrw.32 q0, [r0, #( 0 - 64)]                // .*.............|...................*............. 
        // vstrw.32 q1, [r0, #(16 - 64)]                // ...............|................*................ 
        // vstrw.32 q2, [r0, #(32 - 64)]                // ..............*|................................* 
        // vstrw.32 q3, [r0, #(48 - 64)]                // ...........*...|.............................*... 
        
        le lr, layer67_loop
        vmul.s16 q6, q4, q7                // *...........................
        vldrh.u16 q7, [r11, #-80]          // .*..........................
        vqrdmulh.s16 q4, q4, q7            // ..*.........................
        vldrh.u16 q0, [r11, #-48]          // .....*......................
        vmla.s16 q6, q4, r12               // ....*.......................
        vldrh.u16 q4, [r11, #-64]          // ...*........................
        vqrdmulh.s16 q7, q3, q7            // ......*.....................
        vsub.u16 q3, q2, q6                // .........*..................
        vmla.s16 q5, q7, r12               // ........*...................
        vadd.u16 q2, q2, q6                // .......*....................
        vmul.s16 q4, q2, q4                // ............*...............
        vadd.u16 q7, q1, q5                // ...........*................
        vqrdmulh.s16 q2, q2, q0            // ..........*.................
        vsub.u16 q6, q1, q5                // .............*..............
        vmla.s16 q4, q2, r12               // ..............*.............
        vldrh.u16 q2, [r11, #-32]          // ...............*............
        vadd.u16 q1, q7, q4                // ..................*.........
        vstrw.u32 q1, [r0, #-64]           // ...................*........
        vmul.s16 q2, q3, q2                // ....................*.......
        vldrh.u16 q1, [r11, #-16]          // .....................*......
        vqrdmulh.s16 q1, q3, q1            // ......................*.....
        vsub.u16 q4, q7, q4                // ................*...........
        vmla.s16 q2, q1, r12               // .......................*....
        vstrw.u32 q4, [r0, #-48]           // .................*..........
        vsub.u16 q1, q6, q2                // ........................*...
        vstrw.u32 q1, [r0, #-16]           // ..........................*.
        vadd.u16 q1, q6, q2                // .........................*..
        vstrw.u32 q1, [r0, #-32]           // ...........................*
        
        // original source code
        // vmul.s16 q7, q4, q7            // *........................... 
        // vldrh.u16 q0, [r11, #-80]      // .*.......................... 
        // vqrdmulh.s16 q4, q4, q0        // ..*......................... 
        // vldrh.u16 q6, [r11, #-64]      // .....*...................... 
        // vmla.s16 q7, q4, r12           // ....*....................... 
        // vldrh.u16 q4, [r11, #-48]      // ...*........................ 
        // vqrdmulh.s16 q3, q3, q0        // ......*..................... 
        // vadd.u16 q0, q2, q7            // .........*.................. 
        // vmla.s16 q5, q3, r12           // ........*................... 
        // vsub.u16 q7, q2, q7            // .......*.................... 
        // vqrdmulh.s16 q3, q0, q4        // ............*............... 
        // vadd.u16 q4, q1, q5            // ...........*................ 
        // vmul.s16 q0, q0, q6            // ..........*................. 
        // vsub.u16 q6, q1, q5            // .............*.............. 
        // vmla.s16 q0, q3, r12           // ..............*............. 
        // vldrh.u16 q5, [r11, #-32]      // ...............*............ 
        // vsub.u16 q2, q4, q0            // .....................*...... 
        // vstrw.u32 q2, [r0, #-48]       // .......................*.... 
        // vadd.u16 q0, q4, q0            // ................*........... 
        // vstrw.u32 q0, [r0, #-64]       // .................*.......... 
        // vmul.s16 q5, q7, q5            // ..................*......... 
        // vldrh.u16 q0, [r11, #-16]      // ...................*........ 
        // vqrdmulh.s16 q0, q7, q0        // ....................*....... 
        // vmla.s16 q5, q0, r12           // ......................*..... 
        // vsub.u16 q0, q6, q5            // ........................*... 
        // vadd.u16 q6, q6, q5            // ..........................*. 
        // vstrw.u32 q0, [r0, #-16]       // .........................*.. 
        // vstrw.u32 q6, [r0, #-32]       // ...........................* 
        

        // Restore MVE vector registers
        vpop {d8-d15}
        // Restore GPRs
        pop {r4-r11,lr}
        bx lr